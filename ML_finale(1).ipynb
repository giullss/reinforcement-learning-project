{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uu4XLQ6d9pS7",
        "WclS1IW-cpgc",
        "7EA0YwhCGQcD",
        "BlNvlMgzu6FQ",
        "DNCT-Pvdvvss"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PAC-MAN, PROGETTO ML :)**"
      ],
      "metadata": {
        "id": "bJSWD3Obb8pR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***INTRODUZIONE DEL PROGETTO:***\n"
      ],
      "metadata": {
        "id": "xQbiMbw9cFMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***FASE 1:*** configurazione e analisi dell'ambiente di lavoro"
      ],
      "metadata": {
        "id": "uu4XLQ6d9pS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in questa prima parte andremo a configurare correttamente l'ambiente di gioco scelto.\n",
        "<br> è stato scelto come ambiente quello legato al Berkeley Pacman Project che però non è disponibile come pacchetto PyPI installabile direttamente; --> si deve cercare una open source scritta in python 3 (o tradotta da python 2) su github per poi clonarla e configurarla correttamente su questo colab.\n",
        "l'ambiente di lavoro è preso dal seguente link di github:\n",
        "<br> https://github.com/jeff-hykin/berkeley_pacman.git\n",
        "\n",
        "- ***perché dobbiamo clonare il pacman project?***\n",
        "<br> l'ambiente di gioco di pacman non è incluso in python né in colab --> non si può fare semplicemente import pacman come con altre librerie. il Berkeley Pacman Project è un ambiente didattico sviluppato dalla university of caliornia per progetti dove si legano i concetti di ML e reinforcement learning.\n",
        "<br> questo ambiente è comodo perché ci fornisce già\n",
        "* il gioco implementato in python\n",
        "* struttura che si può usare per definire gli agenti intelligenti\n",
        "* layout dei labirinti\n",
        "* funzioni di scoring e reward etc.\n",
        "* la struttura dei fantasmi\n",
        "\n",
        "il link sopra messo porta alla repo di github dove sono contenute delle cartelle con i vari codici o documentazioni necessarie"
      ],
      "metadata": {
        "id": "4eIbivEACVko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "di seguito andiamo a clonare la repo e fare un po' di considerazioni sull'ambiente di lavoro"
      ],
      "metadata": {
        "id": "adwiWLTx_ioC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cloniamo la repo del progetto di pacman da github e scarichiamolo\n",
        "\n",
        "!git clone https://github.com/jeff-hykin/berkeley_pacman.git\n",
        "\n",
        "# entriamo nella cartella del progetto\n",
        "\n",
        "%cd berkeley_pacman"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNwB4ThhYJ8t",
        "outputId": "af7840b9-e57a-4113-bca5-c63ed6c62ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'berkeley_pacman'...\n",
            "remote: Enumerating objects: 3567, done.\u001b[K\n",
            "remote: Counting objects: 100% (154/154), done.\u001b[K\n",
            "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
            "remote: Total 3567 (delta 77), reused 90 (delta 37), pack-reused 3413 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3567/3567), 1.55 MiB | 20.03 MiB/s, done.\n",
            "Resolving deltas: 100% (2288/2288), done.\n",
            "/content/berkeley_pacman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vediamo dove siamo\n",
        "!pwd\n",
        "\n",
        "# spostiamoci nella cartella main\n",
        "%cd main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArCVWx1XYMfG",
        "outputId": "64c20f32-12d4-4d49-87ba-957cbadb45e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/berkeley_pacman\n",
            "/content/berkeley_pacman/main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# librerie necessarie\n",
        "import pandas as pd\n",
        "from layout import Layout, get_layout\n",
        "from util import manhattan_distance\n",
        "\n",
        "from game import Agent\n",
        "import random\n",
        "import util\n",
        "\n",
        "\n",
        "from pacman import ClassicGameRules\n",
        "from ghost_agents import DirectionalGhost, RandomGhost"
      ],
      "metadata": {
        "id": "uvm9ddWjYQ3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***FASE 2:*** analisi dell'ambiente di lavoro"
      ],
      "metadata": {
        "id": "WclS1IW-cpgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "qui di seguito una piccola analisi specifica sul labirinto small_classic"
      ],
      "metadata": {
        "id": "cX3NX-G0QtLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **!cat** --> comando shell che permette di stampare il contenuto del file di testo del layout\n",
        "* **get_layout()** --> restituisce un oggetto Layout del progetto di berkeley, con strutture interne che poi sono chiamate dopo nei vari print"
      ],
      "metadata": {
        "id": "Z4zro2AfJdi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"---- visualizzazione del labirinto SMALL ----\")\n",
        "!cat layouts/small_classic.lay\n",
        "print()\n",
        "print(\"-\"*15)\n",
        "# Carichiamo il layout e stampa info\n",
        "\n",
        "layout = get_layout(\"small_classic\")\n",
        "\n",
        "print(f\"Dimensioni: {layout.width} x {layout.height}\")\n",
        "print(f\"Num. food: {layout.food.count()}\")\n",
        "print(f\"Num. capsule: {len(layout.capsules)}\")\n",
        "print(f\"Pacman start: {layout.agent_positions[0]}\")\n",
        "print(f\"Ghost starts: {[pos for pos in layout.agent_positions[1:]]}\")\n",
        "\n",
        "# Calcoliamo \"densità\" del labirinto\n",
        "total_cells = layout.width * layout.height\n",
        "wall_cells = sum(1 for x in range(layout.width)\n",
        "                  for y in range(layout.height)\n",
        "                  if layout.walls[x][y])\n",
        "open_cells = total_cells - wall_cells\n",
        "\n",
        "print(f\"Celle totali: {total_cells}\")\n",
        "print(f\"Celle muro: {wall_cells}\")\n",
        "print(f\"Celle libere: {open_cells}\")\n",
        "print(f\"Wall occupancy (%): {wall_cells/total_cells:.1%}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSGK8x8QEK9K",
        "outputId": "3e53ea94-f308-4ad0-86a6-bf9494698af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- visualizzazione del labirinto SMALL ----\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "%......%G  G%......%\n",
            "%.%%...%%  %%...%%.%\n",
            "%.%o.%........%.o%.%\n",
            "%.%%.%.%%%%%%.%.%%.%\n",
            "%........P.........%\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "\n",
            "---------------\n",
            "Dimensioni: 20 x 7\n",
            "Num. food: 55\n",
            "Num. capsule: 2\n",
            "Pacman start: (True, (9, 1))\n",
            "Ghost starts: [(False, (8, 5)), (False, (11, 5))]\n",
            "Celle totali: 140\n",
            "Celle muro: 76\n",
            "Celle libere: 64\n",
            "Wall occupancy (%): 54.3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "di seguito un analisi più completa e comparativa sui 3 labirinti classici, che useremo nel training/test\n",
        "* small_classic\n",
        "* medium_classic\n",
        "* original_classic"
      ],
      "metadata": {
        "id": "1Xa31XXiRIPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **df[...].to_string()** --> estrazione di righe o colonne con un metodo di stampa (opportuno del DataFrame) in modo che sia ben formattata e leggibile sulla console (senza troncamenti)\n",
        "* **.loc[row_label, column_label], .iloc[]** --> metodo di accesso label based ai dati del DataFrame, si utilizzano nel primo caso le etichette (nomi), il secondo invece usa i numeri interi"
      ],
      "metadata": {
        "id": "oirwW-0uPrxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# restituisce un dizionario di statistiche strutturali e spaziali\n",
        "# in forma di funzione almeno si può chiamare su più layout\n",
        "def analyze_layout(layout_name):\n",
        "\n",
        "    layout = get_layout(layout_name)\n",
        "\n",
        "    # Dimensioni base\n",
        "    total_cells = layout.width * layout.height\n",
        "    wall_cells = sum(1 for x in range(layout.width)\n",
        "                      for y in range(layout.height)\n",
        "                      if layout.walls[x][y])\n",
        "    open_cells = total_cells - wall_cells\n",
        "\n",
        "    # Posizioni\n",
        "    pacman_pos = layout.agent_positions[0][1]\n",
        "    ghost_positions = [pos[1] for pos in layout.agent_positions[1:]]\n",
        "    food_positions = layout.food.as_list()\n",
        "\n",
        "    # Distanze fantasmi\n",
        "    ghost_distances = [manhattan_distance(pacman_pos, g) for g in ghost_positions]\n",
        "\n",
        "    # Distanze cibo\n",
        "    food_distances = [manhattan_distance(pacman_pos, f) for f in food_positions] if food_positions else [0]\n",
        "\n",
        "    return {\n",
        "        \"Layout\": layout_name,\n",
        "        \"Width\": layout.width,\n",
        "        \"Height\": layout.height,\n",
        "        \"Celle_Totali\": total_cells,\n",
        "        \"Celle_Libere\": open_cells,\n",
        "        \"Celle_Muro\": wall_cells,\n",
        "        \"Wall_occupancy_%\": round(wall_cells / total_cells * 100, 1),\n",
        "        \"Food\": layout.food.count(),\n",
        "        \"Capsule\": len(layout.capsules),\n",
        "        \"n_Fantasmi\": len(ghost_positions),\n",
        "        \"Cibo_%\": round(layout.food.count() / open_cells * 100, 1) if open_cells > 0 else 0,\n",
        "        \"Ghost_Dist_Min\": min(ghost_distances),\n",
        "        \"Ghost_Dist_Max\": max(ghost_distances),\n",
        "        \"Ghost_Dist_Avg\": round(sum(ghost_distances) / len(ghost_distances), 1),\n",
        "        \"Food_Dist_Min\": min(food_distances),\n",
        "        \"Food_Dist_Max\": max(food_distances),\n",
        "        \"Food_Dist_Avg\": round(sum(food_distances) / len(food_distances), 1)\n",
        "    }\n",
        "\n",
        "# data una lista di nomi e si costruisce un DataFrame\n",
        "def create_layout_comparison(layout_names):\n",
        "    data = [analyze_layout(name) for name in layout_names]\n",
        "    df = pd.DataFrame(data)\n",
        "    df.set_index(\"Layout\", inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def display_analysis(layout_names):\n",
        "    # DataFrame principale\n",
        "    df = create_layout_comparison(layout_names)\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"ANALISI COMPARATIVA LAYOUT\")\n",
        "    print(\"=\"*80)\n",
        "    print()\n",
        "\n",
        "    # Tabella 1: Dimensioni e struttura\n",
        "    print(\"DIMENSIONI E STRUTTURA\")\n",
        "    print(\"-\"*80)\n",
        "    cols_dim = [\"Width\", \"Height\", \"Celle_Totali\", \"Celle_Libere\", \"Celle_Muro\", \"Wall_occupancy_%\"]\n",
        "    print(df[cols_dim].to_string())\n",
        "    print()\n",
        "\n",
        "    # Tabella 2: Risorse\n",
        "    print(\"CIBO E AGENTI\")\n",
        "    print(\"-\"*80)\n",
        "    cols_res = [\"Food\", \"Capsule\", \"n_Fantasmi\", \"Cibo_%\"]\n",
        "    print(df[cols_res].to_string())\n",
        "    print()\n",
        "\n",
        "    # Tabella 3: Distanze\n",
        "    print(\"DISTANZE INIZIALI\")\n",
        "    print(\"-\"*80)\n",
        "    cols_dist = [\"Ghost_Dist_Min\", \"Ghost_Dist_Avg\", \"Ghost_Dist_Max\",\n",
        "                 \"Food_Dist_Min\", \"Food_Dist_Avg\", \"Food_Dist_Max\"]\n",
        "    print(df[cols_dist].to_string())\n",
        "    print()\n",
        "\n",
        "    # Statistiche riassuntive\n",
        "    print(\"STATISTICHE\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Confronto dimensioni\n",
        "    print(f\"\\nRapporto dimensioni:\")\n",
        "    base_size = df[\"Celle_Totali\"].iloc[0]\n",
        "    for layout in df.index:\n",
        "        ratio = df.loc[layout, \"Celle_Totali\"] / base_size\n",
        "        print(f\"  {layout}: {ratio:.1f}x rispetto a {df.index[0]}\")\n",
        "\n",
        "    # Difficoltà stimata\n",
        "    print(f\"\\nDifficoltà stimata (basata su dimensione e spawn):\")\n",
        "    df_sorted = df.sort_values(\"Celle_Totali\")\n",
        "    for i, layout in enumerate(df_sorted.index, 1):\n",
        "        size = \"piccolo\" if df.loc[layout, \"Celle_Totali\"] < 250 else \\\n",
        "               \"medio\" if df.loc[layout, \"Celle_Totali\"] < 500 else \"grande\"\n",
        "        spawn = \"sicuro\" if df.loc[layout, \"Ghost_Dist_Min\"] > 5 else \"pericoloso\"\n",
        "        print(f\"  {i}. {layout}: {size}, spawn {spawn}\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "cqRcXZ87O8hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layout da analizzare\n",
        "layouts = [\"small_classic\", \"medium_classic\", \"original_classic\"]\n",
        "\n",
        "# Analisi completa\n",
        "df = display_analysis(layouts)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQwhK5skPZHd",
        "outputId": "06e3f778-1df3-4b4c-89d0-1bf3fa8afe32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ANALISI COMPARATIVA LAYOUT\n",
            "================================================================================\n",
            "\n",
            "DIMENSIONI E STRUTTURA\n",
            "--------------------------------------------------------------------------------\n",
            "                  Width  Height  Celle_Totali  Celle_Libere  Celle_Muro  Wall_occupancy_%\n",
            "Layout                                                                                   \n",
            "small_classic        20       7           140            64          76              54.3\n",
            "medium_classic       20      11           220           106         114              51.8\n",
            "original_classic     28      27           756           294         462              61.1\n",
            "\n",
            "CIBO E AGENTI\n",
            "--------------------------------------------------------------------------------\n",
            "                  Food  Capsule  n_Fantasmi  Cibo_%\n",
            "Layout                                             \n",
            "small_classic       55        2           2    85.9\n",
            "medium_classic      97        2           2    91.5\n",
            "original_classic   229        4           4    77.9\n",
            "\n",
            "DISTANZE INIZIALI\n",
            "--------------------------------------------------------------------------------\n",
            "                  Ghost_Dist_Min  Ghost_Dist_Avg  Ghost_Dist_Max  Food_Dist_Min  Food_Dist_Avg  Food_Dist_Max\n",
            "Layout                                                                                                       \n",
            "small_classic                  5             5.5               6              1            6.9             13\n",
            "medium_classic                 5             5.5               6              1            8.9             17\n",
            "original_classic              12            14.0              16              1           18.6             37\n",
            "\n",
            "STATISTICHE\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rapporto dimensioni:\n",
            "  small_classic: 1.0x rispetto a small_classic\n",
            "  medium_classic: 1.6x rispetto a small_classic\n",
            "  original_classic: 5.4x rispetto a small_classic\n",
            "\n",
            "Difficoltà stimata (basata su dimensione e spawn):\n",
            "  1. small_classic: piccolo, spawn pericoloso\n",
            "  2. medium_classic: piccolo, spawn pericoloso\n",
            "  3. original_classic: grande, spawn sicuro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***FASE 3:*** progettazione dell'agente Q-learning tabellare"
      ],
      "metadata": {
        "id": "7EA0YwhCGQcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in questa fase viene definito il primo modello di agente che imparerà a giocare a pacman.\n",
        "<br> si baserà sul Q-learning e quindi sul reinforcement learning\n",
        "\n",
        "- **cosa è il reinforcement learning?**\n",
        "<br> è un ramo dell'apprendimento automatico in cui un agente impara a compiere azioni in un ambiente per massimizzare una ricompensa. tutto viene formalizzato tramite una catena di markov con le seguenti componenti:\n",
        "* S = insieme degli stati\n",
        "* A = insieme delle azioni possibili\n",
        "* R(s,a) = ricompensa per aver fatto l'azione a in s\n",
        "* gamma = discount factor: quanto varrà in futuro\n",
        "\n",
        "in questo caso abbiamo\n",
        "* pacman come agente\n",
        "* ogni configurazione (pos, cibo, fantasmi, etc) rappresenta uno stato\n",
        "* le azioni (nord, sud, est, ovest)\n",
        "* la ricompensa positiva è quella di mangiare il cibo e quella negativa è di morire o perdere tempo\n",
        "\n"
      ],
      "metadata": {
        "id": "b_x1-976GbH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **cosa è il Q-learning?**\n",
        "<br> è un algoritmo del RL off-policy che insegna ad un agente a comportarsi in un ambiente andando a massimizzare una ricompensa comulativa. è inoltre un algoritmo model free --> la dinamica dell'ambiente non viene resa nota all'agente a priori.\n",
        "<br> il Q-learning in particolare si basa sull'equazione di bellman che è quella che andrà massimizzata\n",
        "\n",
        "$$\n",
        "Q(s,a) \\leftarrow (1 - \\alpha)\\, Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') \\right]\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "MKP8Rw2ORcg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "bene ora più o meno siamo pronti per iniziare a definire la classe del nostro agente e successivamente definiremo anche la funzione di training.\n",
        "\n",
        "nel repo di github non sono presenti agenti costruiti basati sul Q-learning o RL quindi dovremo implementare il tutto da 0.\n",
        "\n"
      ],
      "metadata": {
        "id": "W7irrFtGSJq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "classe agente tabellare:\n",
        "<br> vengono usate funzioni derivate dalla classe Agent definita nel rispettivo file della repo\n",
        "\n",
        "\n",
        "* **alpha: learning rate** = quanto velocemente l'agente aggiorna i Q-values dopo aver osservato una transizione. più alto = più \"apprende\"\n",
        "* **gamma: discount factor** = quanto valore assegna alle ricompense future rispetto a quelle immediate (vicino a 0: reward immediato)\n",
        "* **epsilon: esplorazione** = probabilità con cui l'agente fa una mossa casuale (ε-greedy). Serve per evitare di fissarsi troppo presto su una policy non ottimale.\n",
        "* **q_values** = dizionario che memorizza Q(s,a) per ogni coppia stato-azione osservata. Usa Counter per gestire valori di default a 0.\n",
        "\n",
        "\n",
        "la tupla che ritorna la funzione _key() è importante perché è una tupla che restituisce:\n",
        "* in quali direzioni per avvicinarsi al cibo\n",
        "* in quali direzioni ci sono i pericoli dei fantasmi\n",
        "* quanto si è lontani dal fantasma più vicino\n",
        "\n",
        "è una rappresentazione che riduce lo stato continuo ad uno stato discreto in modo da poterlo utilizzare dopo per apprendere i Q-values\n",
        "\n",
        "* **util.flip_coin()** --> funzione della libreria util che ritorna True con probabilità epsilon e False con 1-epsilon\n"
      ],
      "metadata": {
        "id": "cLjoy1z0Zpci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---- IMPLEMENTAZIONE AGENTE Q-LEARNING TABELLARE ----\n",
        "\n",
        "class QLearningAgent1(Agent):\n",
        "    def __init__(self, alpha=0.5, gamma=0.99, epsilon=0.1, verbose= False):\n",
        "        self.q_values = util.Counter()\n",
        "        self.alpha = float(alpha)\n",
        "        self.gamma = float(gamma)\n",
        "        self.epsilon = float(epsilon)\n",
        "        self.episode_count = 0\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def _key(self, state):\n",
        "        # posizione pacman, cibo e fantasmi\n",
        "        pacman_pos = state.get_pacman_position()\n",
        "        food = state.get_food().as_list()\n",
        "        ghosts = state.get_ghost_positions()\n",
        "        legal = state.get_legal_actions(0)\n",
        "\n",
        "        # Trova cibo più vicino\n",
        "        closest_food_dir = (0, 0, 0, 0)  # N, E, S, W\n",
        "        if food:\n",
        "            target_food = min(food, key=lambda f: manhattan_distance(pacman_pos, f))\n",
        "            closest_food_dir = self._get_direction_vector(pacman_pos, target_food)\n",
        "\n",
        "        # fantasmi\n",
        "        ghost_danger = [0, 0, 0, 0]  # pericolo in ogni direzione\n",
        "        closest_ghost_dist = 999\n",
        "\n",
        "        for ghost_pos in ghosts:\n",
        "            dist = manhattan_distance(pacman_pos, ghost_pos)\n",
        "            closest_ghost_dist = min(closest_ghost_dist, dist)\n",
        "\n",
        "            # Se il fantasma è vicino (entro 3 celle), marca le direzioni pericolose\n",
        "            if dist <= 3:\n",
        "                directions = ['North', 'East', 'South', 'West']\n",
        "                for i, dir in enumerate(directions):\n",
        "                    if dir not in legal:\n",
        "                        continue\n",
        "                    successor = state.generate_successor(0, dir) # simuliamo la mossa\n",
        "                    new_pos = successor.get_pacman_position()\n",
        "                    new_dist = manhattan_distance(new_pos, ghost_pos)\n",
        "\n",
        "                    # se con la mossa ci avviciniamo al fantasma, aumenta il pericolo\n",
        "                    if new_dist < dist:\n",
        "                        ghost_danger[i] = 1\n",
        "\n",
        "        # discretizzazione della distanza dal fantasma più vicino\n",
        "        ghost_dist_bucket = 0\n",
        "        if closest_ghost_dist <= 1:\n",
        "            ghost_dist_bucket = 0  # pericolo immediato\n",
        "        elif closest_ghost_dist <= 3:\n",
        "            ghost_dist_bucket = 1  # vicino\n",
        "        elif closest_ghost_dist <= 5:\n",
        "            ghost_dist_bucket = 2  # medio\n",
        "        else:\n",
        "            ghost_dist_bucket = 3  # lontano/sicuro\n",
        "\n",
        "        return (closest_food_dir, tuple(ghost_danger), ghost_dist_bucket)\n",
        "\n",
        "# Ritorna (N, E, S, W) dove 1 indica che andare in quella direzione avvicina al target\n",
        "    def _get_direction_vector(self, from_pos, to_pos):\n",
        "        dx = to_pos[0] - from_pos[0]\n",
        "        dy = to_pos[1] - from_pos[1]\n",
        "\n",
        "        north = 1 if dy > 0 else 0\n",
        "        south = 1 if dy < 0 else 0\n",
        "        east = 1 if dx > 0 else 0\n",
        "        west = 1 if dx < 0 else 0\n",
        "\n",
        "        return (north, east, south, west)\n",
        "# Restituisce Q(state, action)\n",
        "    def getQValue(self, state, action):\n",
        "        return self.q_values[(self._key(state), action)]\n",
        "# Restituisce max_a Q(state, a)\n",
        "    def getValue(self, state):\n",
        "        legal_actions = state.get_legal_actions(0)\n",
        "        if not legal_actions:\n",
        "            return 0.0\n",
        "        return max([self.getQValue(state, action) for action in legal_actions])\n",
        "# Restituisce la migliore azione secondo la policy corrente\n",
        "# dato uno stato restituisce l'azione con il valore Q più alto\n",
        "    def getPolicy(self, state):\n",
        "        legal = state.get_legal_actions(0) # azioni legali\n",
        "        if not legal:\n",
        "            return None\n",
        "\n",
        "        q_vals = [(a, self.getQValue(state, a)) for a in legal] # valore Q(s,a)\n",
        "        best = max(v for _, v in q_vals)\n",
        "        best_actions = [a for a, v in q_vals if v == best]\n",
        "        return random.choice(best_actions)\n",
        "# sceglie effettivamente l'azione da fare nel gioco\n",
        "# implementata con la greedy policy però in modo da evitare mosse suicide\n",
        "    def get_action(self, state):\n",
        "        legal = state.get_legal_actions(0)\n",
        "        if not legal:\n",
        "            return None\n",
        "\n",
        "        # euristica di sopravvivenza: esclude mosse suicide\n",
        "        safe_actions = self._get_safe_actions(state, legal)\n",
        "\n",
        "        # Se non ci sono mosse sicure, usiamo tutte le legali (siamo disperati qui)\n",
        "        if not safe_actions:\n",
        "            safe_actions = legal\n",
        "\n",
        "        # Esplorazione solo tra mosse sicure\n",
        "        if util.flip_coin(self.epsilon):\n",
        "            return random.choice(safe_actions)\n",
        "\n",
        "        # Exploitation: scegli la migliore tra le mosse sicure\n",
        "        q_vals = [(a, self.getQValue(state, a)) for a in safe_actions]\n",
        "        best = max(v for _, v in q_vals)\n",
        "        best_actions = [a for a, v in q_vals if v == best]\n",
        "        return random.choice(best_actions)\n",
        "# Filtra le azioni, euristica difensiva\n",
        "    def _get_safe_actions(self, state, legal_actions):\n",
        "        pacman_pos = state.get_pacman_position()\n",
        "        ghosts = state.get_ghost_positions()\n",
        "        safe = []\n",
        "\n",
        "        for action in legal_actions:\n",
        "            successor = state.generate_successor(0, action)\n",
        "            new_pos = successor.get_pacman_position()\n",
        "\n",
        "            # Controlliamo se la nuova posizione è troppo vicina a un fantasma\n",
        "            is_safe = True\n",
        "            for ghost_pos in ghosts:\n",
        "                if manhattan_distance(new_pos, ghost_pos) <= 1:\n",
        "                    is_safe = False\n",
        "                    break\n",
        "\n",
        "            if is_safe:\n",
        "                safe.append(action)\n",
        "\n",
        "        return safe\n",
        "# Aggiorna Q(state, action) usando l'equazione di Bellman standard\n",
        "# verrà chiamata ogni volta che avviene una transizione\n",
        "    def update(self, state, action, nextState, reward):\n",
        "        # Bellman update standard - niente reward shaping qui\n",
        "        next_legal = nextState.get_legal_actions(0)\n",
        "        next_best = 0.0 if not next_legal else max(\n",
        "            self.getQValue(nextState, a) for a in next_legal\n",
        "        )\n",
        "        target = reward + self.gamma * next_best\n",
        "\n",
        "        old = self.getQValue(state, action)\n",
        "        self.q_values[(self._key(state), action)] = old + self.alpha * (target - old)\n",
        "# funzione di interfaccia tra il framework e l’agente\n",
        "    def observeTransition(self, state, action, nextState, deltaReward):\n",
        "        self.update(state, action, nextState, deltaReward)\n",
        "\n",
        "# Chiamata all'inizio di ogni episodio\n",
        "    def startEpisode(self):\n",
        "        self.episode_count += 1\n",
        "        if self.verbose and self.episode_count % 10 == 0:\n",
        "            print(f\"\\n=== EPISODE {self.episode_count} ===\")\n",
        "# Chiamata alla fine di ogni episodio\n",
        "    def stopEpisode(self):\n",
        "        if self.verbose and self.episode_count % 10 == 0:\n",
        "            print(f\"Episode {self.episode_count} ended - Q-values learned: {len(self.q_values)}\")\n",
        "# Aggiorna epsilon durante il training\n",
        "    def set_epsilon(self, eps):\n",
        "        self.epsilon = float(eps)\n",
        "# Aggiorna learning rate durante il training\n",
        "    def set_alpha(self, alpha):\n",
        "        self.alpha = float(alpha)"
      ],
      "metadata": {
        "id": "1U5hBnjbWWy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "di seguito vengono definite le funzioni di training e di test"
      ],
      "metadata": {
        "id": "I_FKr52AYHeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- FUNZIONE DI TRAINING ----\n",
        "\n",
        "def train_with_progress1(\n",
        "    agent,\n",
        "    num_episodes=100,\n",
        "    layout_name=\"small_classic\",\n",
        "    n_ghosts=1,\n",
        "    ghost_cls=DirectionalGhost,\n",
        "    epsilon_schedule=None,\n",
        "    seed=None,\n",
        "    print_every=10,\n",
        "    max_steps=1000,\n",
        "    use_powerup_rewards=False\n",
        "):\n",
        "\n",
        "# # impostiamo il seed per la riproducibilità, considerando le scelte casuali\n",
        "    if seed is not None:\n",
        "        import random\n",
        "        random.seed(seed)\n",
        "# carichiamo il layout e le regole di gioco\n",
        "    layout = get_layout(layout_name)\n",
        "    rules = ClassicGameRules(timeout=0)\n",
        "# inizializziamo\n",
        "    wins = 0\n",
        "    deaths = 0\n",
        "    results = []\n",
        "    recent_wins = []\n",
        "    recent_scores = []\n",
        "    recent_steps = []\n",
        "\n",
        "# ogni iterazione indica un episodio completo\n",
        "    for ep in range(1, num_episodes + 1):\n",
        "        agent.startEpisode()\n",
        "\n",
        "        # Epsilon decay automatico se non viene specificato\n",
        "        if epsilon_schedule is not None:\n",
        "            agent.set_epsilon(epsilon_schedule(ep))\n",
        "        elif ep > 1:  # Decay lineare automatico\n",
        "            agent.set_epsilon(max(0.05, agent.epsilon * 0.995))\n",
        "\n",
        "        ghosts = [ghost_cls(i+1) for i in range(n_ghosts)]\n",
        "        game = rules.new_game(layout, agent, ghosts, display=None, quiet=True, catch_exceptions=False)\n",
        "        state = game.state\n",
        "        steps = 0\n",
        "# fino a quando una delle condizioni non interrompe l'episodio\n",
        "        while not (state.is_win() or state.is_lose()) and steps < max_steps:\n",
        "            steps += 1\n",
        "\n",
        "            # Turno Pacman\n",
        "            old_score = state.get_score()\n",
        "            old_food_count = state.get_food().count()\n",
        "\n",
        "            action = agent.get_action(state)\n",
        "            next_state = state.generate_successor(0, action)\n",
        "\n",
        "            # reward shaping\n",
        "            new_score = next_state.get_score()\n",
        "            new_food_count = next_state.get_food().count()\n",
        "\n",
        "            reward = new_score - old_score  # Base reward\n",
        "\n",
        "            if not next_state.is_lose():\n",
        "                reward += 2.0  # sopravvivenza\n",
        "\n",
        "            if next_state.is_lose():\n",
        "                reward -= 300  # morte\n",
        "\n",
        "            if next_state.is_win():\n",
        "                reward += 500  # vittoria\n",
        "\n",
        "            if old_food_count == new_food_count:\n",
        "                reward -= 0.5  # efficienza\n",
        "\n",
        "\n",
        "            if use_powerup_rewards: # solo se sono attivi\n",
        "                # BONUS: se mangia power-up\n",
        "                if len(state.get_capsules()) > len(next_state.get_capsules()):\n",
        "                    reward += 50\n",
        "\n",
        "                # BONUS: se mangia fantasma spaventato\n",
        "                if any(gs.scared_timer > 0 for gs in next_state.get_ghost_states()):\n",
        "                    # Controlliamo se score aumenta molto (200 punti = fantasma mangiato)\n",
        "                    if new_score - old_score >= 200:\n",
        "                        reward += 100\n",
        "# chiamiamo observeTransition() per aggiornare i Q-values (caso tabellare)\n",
        "            agent.observeTransition(state, action, next_state, reward)\n",
        "            state = next_state\n",
        "\n",
        "            if state.is_win() or state.is_lose():\n",
        "                break\n",
        "\n",
        "            # Turno fantasmi\n",
        "            for g_idx, ghost in enumerate(ghosts, start=1):\n",
        "                g_action = ghost.get_action(state)\n",
        "                state = state.generate_successor(g_idx, g_action)\n",
        "                if state.is_win() or state.is_lose():\n",
        "                    break\n",
        "\n",
        "        # Statistiche finali episodio\n",
        "        final_score = state.get_score()\n",
        "        win = state.is_win()\n",
        "        lose = state.is_lose()\n",
        "\n",
        "        wins += 1 if win else 0\n",
        "        deaths += 1 if lose else 0\n",
        "        win_rate = wins / ep\n",
        "        death_rate = deaths / ep\n",
        "        # ci serve hasattr per poter riutilizzare la func anche con il modello neurale\n",
        "        q_values = len(agent.q_values) if hasattr(agent, \"q_values\") else None\n",
        "        results.append({\n",
        "            \"episode\": ep,\n",
        "            \"score\": final_score,\n",
        "            \"win\": win,\n",
        "            \"lose\": lose,\n",
        "            \"steps\": steps,\n",
        "            \"epsilon\": agent.epsilon,\n",
        "            \"q_values_count\": q_values\n",
        "        })\n",
        "\n",
        "        recent_wins.append(1 if win else 0)\n",
        "        recent_scores.append(final_score)\n",
        "        recent_steps.append(steps)\n",
        "\n",
        "        # statistiche locali\n",
        "        if len(recent_wins) > print_every:\n",
        "            recent_wins.pop(0)\n",
        "            recent_scores.pop(0)\n",
        "            recent_steps.pop(0)\n",
        "\n",
        "        agent.stopEpisode()\n",
        "\n",
        "        # Stampa statistiche locali\n",
        "        if ep % print_every == 0:\n",
        "            local_win_rate = sum(recent_wins) / len(recent_wins)\n",
        "            avg_score = sum(recent_scores) / len(recent_scores)\n",
        "            avg_steps = sum(recent_steps) / len(recent_steps)\n",
        "\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Episode {ep}/{num_episodes}\")\n",
        "            print(f\"  Win rate (ultimi {print_every}): {local_win_rate:.2%}\")\n",
        "            print(f\"  Win rate (globale): {win_rate:.2%}\")\n",
        "            print(f\"  Avg score: {avg_score:.1f} | Avg steps: {avg_steps:.1f}\")\n",
        "            print(f\"  Epsilon: {agent.epsilon:.3f}\")\n",
        "            print(f\"  Q-values appresi: {q_values}\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "    # statistiche finali\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TRAINING COMPLETATO\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Episodi totali: {num_episodes}\")\n",
        "    print(f\"Vittorie: {wins} ({win_rate:.2%})\")\n",
        "    print(f\"Sconfitte: {deaths} ({death_rate:.2%})\")\n",
        "    print(f\"Q-values finali: {q_values}\")\n",
        "    print(f\"Epsilon finale: {agent.epsilon:.3f}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "67FIU5CFYPTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- FUNZIONE DI UTILITÀ PER TESTARE L'AGENTE ADDESTRATO ----\n",
        "\n",
        "def test_agent(agent, num_episodes=10, layout_name=\"small_classic\", n_ghosts=1, ghost_cls=RandomGhost):\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.set_epsilon(0.0)  # nessuna exploration durante il test\n",
        "\n",
        "    layout = get_layout(layout_name)\n",
        "    rules = ClassicGameRules(timeout=0)\n",
        "\n",
        "    wins = 0\n",
        "    total_score = 0\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TESTING AGENT (epsilon=0, greedy policy)\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    for ep in range(1, num_episodes + 1):\n",
        "        ghosts = [ghost_cls(i+1) for i in range(n_ghosts)]\n",
        "        game = rules.new_game(layout, agent, ghosts, display=None, quiet=True, catch_exceptions=False)\n",
        "        state = game.state\n",
        "        steps = 0\n",
        "\n",
        "        while not (state.is_win() or state.is_lose()) and steps < 1000:\n",
        "            steps += 1\n",
        "            # turno pacman\n",
        "            action = agent.get_action(state)\n",
        "            state = state.generate_successor(0, action)\n",
        "\n",
        "            if state.is_win() or state.is_lose():\n",
        "                break\n",
        "            # turno fantasmi\n",
        "            for g_idx, ghost in enumerate(ghosts, start=1):\n",
        "                g_action = ghost.get_action(state)\n",
        "                state = state.generate_successor(g_idx, g_action)\n",
        "                if state.is_win() or state.is_lose():\n",
        "                    break\n",
        "\n",
        "        win = state.is_win()\n",
        "        score = state.get_score()\n",
        "        wins += 1 if win else 0\n",
        "        total_score += score\n",
        "        #statistica del singolo episodio\n",
        "        result = \"WIN\" if win else \"LOSE\"\n",
        "        print(f\"Test {ep}/{num_episodes}: {result} | Score: {score} | Steps: {steps}\")\n",
        "    # statistiche totali\n",
        "    win_rate = wins / num_episodes\n",
        "    avg_score = total_score / num_episodes\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TEST RESULTS\")\n",
        "    print(f\"  Win rate: {win_rate:.2%} ({wins}/{num_episodes})\")\n",
        "    print(f\"  Avg score: {avg_score:.1f}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    agent.set_epsilon(original_epsilon)  # Ripristina epsilon originale\n",
        "    return win_rate, avg_score"
      ],
      "metadata": {
        "id": "hJpt5o2XalF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creiamo l'agente\n",
        "agent = QLearningAgent1(alpha=0.3, gamma=0.9, epsilon=0.3, verbose=False)\n",
        "\n",
        "# training\n",
        "results = train_with_progress1(\n",
        "    agent,\n",
        "    num_episodes=300,\n",
        "    n_ghosts=1,\n",
        "    ghost_cls=RandomGhost,\n",
        "    print_every=50\n",
        ")"
      ],
      "metadata": {
        "id": "-up_mMvYY6Sx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fadb6fe-ab4d-4538-8a76-f2d14a5a187b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Episode 50/300\n",
            "  Win rate (ultimi 50): 72.00%\n",
            "  Win rate (globale): 72.00%\n",
            "  Avg score: 435.1 | Avg steps: 284.7\n",
            "  Epsilon: 0.235\n",
            "  Q-values appresi: 536\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 100/300\n",
            "  Win rate (ultimi 50): 86.00%\n",
            "  Win rate (globale): 79.00%\n",
            "  Avg score: 613.8 | Avg steps: 296.2\n",
            "  Epsilon: 0.183\n",
            "  Q-values appresi: 546\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 150/300\n",
            "  Win rate (ultimi 50): 92.00%\n",
            "  Win rate (globale): 83.33%\n",
            "  Avg score: 726.7 | Avg steps: 239.9\n",
            "  Epsilon: 0.142\n",
            "  Q-values appresi: 548\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 200/300\n",
            "  Win rate (ultimi 50): 92.00%\n",
            "  Win rate (globale): 85.50%\n",
            "  Avg score: 759.1 | Avg steps: 202.1\n",
            "  Epsilon: 0.111\n",
            "  Q-values appresi: 552\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 250/300\n",
            "  Win rate (ultimi 50): 88.00%\n",
            "  Win rate (globale): 86.00%\n",
            "  Avg score: 675.3 | Avg steps: 259.3\n",
            "  Epsilon: 0.086\n",
            "  Q-values appresi: 552\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 300/300\n",
            "  Win rate (ultimi 50): 94.00%\n",
            "  Win rate (globale): 87.33%\n",
            "  Avg score: 840.5 | Avg steps: 134.5\n",
            "  Epsilon: 0.067\n",
            "  Q-values appresi: 552\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETATO\n",
            "============================================================\n",
            "Episodi totali: 300\n",
            "Vittorie: 262 (87.33%)\n",
            "Sconfitte: 35 (11.67%)\n",
            "Q-values finali: 552\n",
            "Epsilon finale: 0.067\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTA:** se volessimo considerare dei risultati con meno varianza si potrebbe condurre più chiamate dei test e fare poi una media tra i win rate ottenuti"
      ],
      "metadata": {
        "id": "SfkmA8qccakp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test con 1 fantasma random ghost in small_classic\n",
        "test_agent(agent, num_episodes=30,\n",
        "           n_ghosts=1, ghost_cls=RandomGhost,\n",
        "           layout_name=\"small_classic\")"
      ],
      "metadata": {
        "id": "eNGkFCeYY94w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c4cc8d-c1fa-47b6-819e-4898dfae1b30",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING AGENT (epsilon=0, greedy policy)\n",
            "============================================================\n",
            "\n",
            "Test 1/30: LOSE | Score: -510.0 | Steps: 1000\n",
            "Test 2/30: WIN | Score: 917.0 | Steps: 133\n",
            "Test 3/30: LOSE | Score: -375.0 | Steps: 165\n",
            "Test 4/30: WIN | Score: 931.0 | Steps: 119\n",
            "Test 5/30: WIN | Score: 211.0 | Steps: 839\n",
            "Test 6/30: WIN | Score: 926.0 | Steps: 124\n",
            "Test 7/30: WIN | Score: 720.0 | Steps: 330\n",
            "Test 8/30: WIN | Score: 915.0 | Steps: 135\n",
            "Test 9/30: WIN | Score: 947.0 | Steps: 103\n",
            "Test 10/30: LOSE | Score: -443.0 | Steps: 183\n",
            "Test 11/30: WIN | Score: 305.0 | Steps: 745\n",
            "Test 12/30: WIN | Score: 606.0 | Steps: 444\n",
            "Test 13/30: WIN | Score: 683.0 | Steps: 367\n",
            "Test 14/30: WIN | Score: 712.0 | Steps: 338\n",
            "Test 15/30: WIN | Score: 931.0 | Steps: 119\n",
            "Test 16/30: WIN | Score: 880.0 | Steps: 170\n",
            "Test 17/30: WIN | Score: 966.0 | Steps: 84\n",
            "Test 18/30: WIN | Score: 524.0 | Steps: 526\n",
            "Test 19/30: WIN | Score: 933.0 | Steps: 117\n",
            "Test 20/30: WIN | Score: 869.0 | Steps: 181\n",
            "Test 21/30: LOSE | Score: -591.0 | Steps: 331\n",
            "Test 22/30: WIN | Score: 970.0 | Steps: 80\n",
            "Test 23/30: WIN | Score: 920.0 | Steps: 130\n",
            "Test 24/30: WIN | Score: 901.0 | Steps: 149\n",
            "Test 25/30: WIN | Score: 235.0 | Steps: 815\n",
            "Test 26/30: WIN | Score: 909.0 | Steps: 141\n",
            "Test 27/30: WIN | Score: 940.0 | Steps: 110\n",
            "Test 28/30: WIN | Score: 771.0 | Steps: 279\n",
            "Test 29/30: WIN | Score: 968.0 | Steps: 82\n",
            "Test 30/30: WIN | Score: 929.0 | Steps: 121\n",
            "\n",
            "============================================================\n",
            "TEST RESULTS\n",
            "  Win rate: 86.67% (26/30)\n",
            "  Avg score: 620.0\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8666666666666667, 620.0)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "di seguito vedremo:\n",
        "* caso con un 1 directional ghost su small_classic\n",
        "* caso con un 1 random ghost ma su un altro labirinto (es. medium_classic) per vedere come variano le statistiche\n",
        "\n"
      ],
      "metadata": {
        "id": "wTIkKZd_e1SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creiamo un nuovo agente\n",
        "agent = QLearningAgent1(alpha=0.3, gamma=0.9, epsilon=0.3, verbose=False)\n",
        "print(\"agente caricato con successo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y_LUTAffN0c",
        "outputId": "a07e0127-3c0c-4ffa-f0f1-da57a7e74185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "agente caricato con successo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FASE 1: Pre-training con RandomGhost\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FASE 1: Pre-training con RandomGhost\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results_R = train_with_progress1(agent,\n",
        "                     num_episodes=300,\n",
        "                     n_ghosts=1,\n",
        "                     ghost_cls=RandomGhost,\n",
        "                     layout_name=\"small_classic\",\n",
        "                     print_every=50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZMs409jfYvG",
        "outputId": "a3488971-525d-4112-8ef9-d44678c461bd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "FASE 1: Pre-training con RandomGhost\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "Episode 50/300\n",
            "  Win rate (ultimi 50): 70.00%\n",
            "  Win rate (globale): 70.00%\n",
            "  Avg score: 390.3 | Avg steps: 356.3\n",
            "  Epsilon: 0.235\n",
            "  Q-values appresi: 533\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 100/300\n",
            "  Win rate (ultimi 50): 90.00%\n",
            "  Win rate (globale): 80.00%\n",
            "  Avg score: 619.2 | Avg steps: 331.6\n",
            "  Epsilon: 0.183\n",
            "  Q-values appresi: 541\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 150/300\n",
            "  Win rate (ultimi 50): 82.00%\n",
            "  Win rate (globale): 80.67%\n",
            "  Avg score: 610.7 | Avg steps: 254.7\n",
            "  Epsilon: 0.142\n",
            "  Q-values appresi: 543\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 200/300\n",
            "  Win rate (ultimi 50): 90.00%\n",
            "  Win rate (globale): 83.00%\n",
            "  Avg score: 707.2 | Avg steps: 235.2\n",
            "  Epsilon: 0.111\n",
            "  Q-values appresi: 554\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 250/300\n",
            "  Win rate (ultimi 50): 80.00%\n",
            "  Win rate (globale): 82.40%\n",
            "  Avg score: 642.5 | Avg steps: 183.1\n",
            "  Epsilon: 0.086\n",
            "  Q-values appresi: 554\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 300/300\n",
            "  Win rate (ultimi 50): 94.00%\n",
            "  Win rate (globale): 84.33%\n",
            "  Avg score: 840.8 | Avg steps: 146.4\n",
            "  Epsilon: 0.067\n",
            "  Q-values appresi: 554\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETATO\n",
            "============================================================\n",
            "Episodi totali: 300\n",
            "Vittorie: 253 (84.33%)\n",
            "Sconfitte: 42 (14.00%)\n",
            "Q-values finali: 554\n",
            "Epsilon finale: 0.067\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FASE 2: 1 DirectionalGhost\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FASE 2: 1 DirectionalGhost\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results_D= train_with_progress1(agent,\n",
        "                     num_episodes=500,\n",
        "                     n_ghosts=1,\n",
        "                     ghost_cls=DirectionalGhost,\n",
        "                     layout_name=\"small_classic\",\n",
        "                     print_every=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmtHi-RIfdCr",
        "outputId": "5df71b2e-2f49-40e4-cf55-84d4074d10a6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "FASE 2: 1 DirectionalGhost\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "Episode 50/500\n",
            "  Win rate (ultimi 50): 74.00%\n",
            "  Win rate (globale): 74.00%\n",
            "  Avg score: 617.1 | Avg steps: 129.3\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 558\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 100/500\n",
            "  Win rate (ultimi 50): 76.00%\n",
            "  Win rate (globale): 75.00%\n",
            "  Avg score: 653.6 | Avg steps: 121.8\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 558\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 150/500\n",
            "  Win rate (ultimi 50): 72.00%\n",
            "  Win rate (globale): 74.00%\n",
            "  Avg score: 575.3 | Avg steps: 142.7\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 558\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 200/500\n",
            "  Win rate (ultimi 50): 76.00%\n",
            "  Win rate (globale): 74.50%\n",
            "  Avg score: 618.0 | Avg steps: 142.4\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 558\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 250/500\n",
            "  Win rate (ultimi 50): 82.00%\n",
            "  Win rate (globale): 76.00%\n",
            "  Avg score: 701.2 | Avg steps: 134.6\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 558\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 300/500\n",
            "  Win rate (ultimi 50): 78.00%\n",
            "  Win rate (globale): 76.33%\n",
            "  Avg score: 672.6 | Avg steps: 120.8\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 558\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 350/500\n",
            "  Win rate (ultimi 50): 90.00%\n",
            "  Win rate (globale): 78.29%\n",
            "  Avg score: 783.3 | Avg steps: 146.3\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 558\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 400/500\n",
            "  Win rate (ultimi 50): 62.00%\n",
            "  Win rate (globale): 76.25%\n",
            "  Avg score: 469.3 | Avg steps: 161.7\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 558\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 450/500\n",
            "  Win rate (ultimi 50): 70.00%\n",
            "  Win rate (globale): 75.56%\n",
            "  Avg score: 564.2 | Avg steps: 158.4\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 558\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 500/500\n",
            "  Win rate (ultimi 50): 84.00%\n",
            "  Win rate (globale): 76.40%\n",
            "  Avg score: 727.3 | Avg steps: 124.3\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 558\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETATO\n",
            "============================================================\n",
            "Episodi totali: 500\n",
            "Vittorie: 382 (76.40%)\n",
            "Sconfitte: 118 (23.60%)\n",
            "Q-values finali: 558\n",
            "Epsilon finale: 0.050\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TEST con directional ghost\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST: 1 DirectionalGhost\")\n",
        "print(\"=\"*70)\n",
        "test1_wr, test1_score = test_agent(agent,\n",
        "                                    num_episodes=50,\n",
        "                                    n_ghosts=1,\n",
        "                                    ghost_cls=DirectionalGhost,\n",
        "                                    layout_name=\"small_classic\")\n",
        "\n",
        "\n",
        "# statistiche finali\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"statistiche finali\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Q-values appresi: {len(agent.q_values)}\")\n",
        "print(f\"Epsilon finale: {agent.epsilon:.3f}\")\n",
        "print(f\"\\nRisultati Test:\")\n",
        "print(f\"  1 DirectionalGhost: {test1_wr:.1%} win rate, {test1_score:.1f} avg score\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqWaScSUe9Jh",
        "outputId": "3b447e35-e6a3-4545-bd12-f8c39165d0ed",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST: 1 DirectionalGhost\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "TESTING AGENT (epsilon=0, greedy policy)\n",
            "============================================================\n",
            "\n",
            "Test 1/50: WIN | Score: 961.0 | Steps: 89\n",
            "Test 2/50: WIN | Score: 901.0 | Steps: 149\n",
            "Test 3/50: WIN | Score: 954.0 | Steps: 96\n",
            "Test 4/50: WIN | Score: 950.0 | Steps: 100\n",
            "Test 5/50: WIN | Score: 973.0 | Steps: 77\n",
            "Test 6/50: WIN | Score: 942.0 | Steps: 108\n",
            "Test 7/50: WIN | Score: 932.0 | Steps: 118\n",
            "Test 8/50: WIN | Score: 925.0 | Steps: 125\n",
            "Test 9/50: WIN | Score: 737.0 | Steps: 313\n",
            "Test 10/50: WIN | Score: 870.0 | Steps: 180\n",
            "Test 11/50: WIN | Score: 942.0 | Steps: 108\n",
            "Test 12/50: WIN | Score: 871.0 | Steps: 179\n",
            "Test 13/50: WIN | Score: 944.0 | Steps: 106\n",
            "Test 14/50: WIN | Score: 959.0 | Steps: 91\n",
            "Test 15/50: WIN | Score: 924.0 | Steps: 126\n",
            "Test 16/50: WIN | Score: 937.0 | Steps: 113\n",
            "Test 17/50: LOSE | Score: -81.0 | Steps: 111\n",
            "Test 18/50: WIN | Score: 915.0 | Steps: 135\n",
            "Test 19/50: WIN | Score: 973.0 | Steps: 77\n",
            "Test 20/50: WIN | Score: 906.0 | Steps: 144\n",
            "Test 21/50: WIN | Score: 976.0 | Steps: 74\n",
            "Test 22/50: WIN | Score: 955.0 | Steps: 95\n",
            "Test 23/50: WIN | Score: 967.0 | Steps: 83\n",
            "Test 24/50: LOSE | Score: -490.0 | Steps: 1000\n",
            "Test 25/50: WIN | Score: 934.0 | Steps: 116\n",
            "Test 26/50: WIN | Score: 922.0 | Steps: 128\n",
            "Test 27/50: WIN | Score: 963.0 | Steps: 87\n",
            "Test 28/50: WIN | Score: 899.0 | Steps: 151\n",
            "Test 29/50: WIN | Score: 853.0 | Steps: 197\n",
            "Test 30/50: WIN | Score: 924.0 | Steps: 126\n",
            "Test 31/50: LOSE | Score: -277.0 | Steps: 67\n",
            "Test 32/50: WIN | Score: 867.0 | Steps: 183\n",
            "Test 33/50: WIN | Score: 882.0 | Steps: 168\n",
            "Test 34/50: WIN | Score: 940.0 | Steps: 110\n",
            "Test 35/50: LOSE | Score: -127.0 | Steps: 147\n",
            "Test 36/50: WIN | Score: 982.0 | Steps: 68\n",
            "Test 37/50: WIN | Score: 927.0 | Steps: 123\n",
            "Test 38/50: WIN | Score: 916.0 | Steps: 134\n",
            "Test 39/50: WIN | Score: 973.0 | Steps: 77\n",
            "Test 40/50: WIN | Score: 935.0 | Steps: 115\n",
            "Test 41/50: WIN | Score: 964.0 | Steps: 86\n",
            "Test 42/50: WIN | Score: 908.0 | Steps: 142\n",
            "Test 43/50: WIN | Score: 953.0 | Steps: 97\n",
            "Test 44/50: WIN | Score: 905.0 | Steps: 145\n",
            "Test 45/50: LOSE | Score: -500.0 | Steps: 1000\n",
            "Test 46/50: LOSE | Score: -363.0 | Steps: 83\n",
            "Test 47/50: WIN | Score: 818.0 | Steps: 232\n",
            "Test 48/50: WIN | Score: 955.0 | Steps: 95\n",
            "Test 49/50: WIN | Score: 936.0 | Steps: 114\n",
            "Test 50/50: WIN | Score: 919.0 | Steps: 131\n",
            "\n",
            "============================================================\n",
            "TEST RESULTS\n",
            "  Win rate: 88.00% (44/50)\n",
            "  Avg score: 777.0\n",
            "============================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "statistiche finali\n",
            "======================================================================\n",
            "Q-values appresi: 558\n",
            "Epsilon finale: 0.050\n",
            "\n",
            "Risultati Test:\n",
            "  1 DirectionalGhost: 88.0% win rate, 777.0 avg score\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**NOTA:** solo per indagine statistica e per vericare la stabilità del modello testiamo lo stesso agente già addestrato su più seed diversi"
      ],
      "metadata": {
        "id": "PeZoV7SbiAz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testiamo lo stesso agente con configurazioni iniziali diverse\n",
        "import random\n",
        "\n",
        "for seed in [42, 123, 456]:\n",
        "    random.seed(seed)  # Cambia spawn iniziali\n",
        "    wr, _ = test_agent(agent, num_episodes=50,\n",
        "                       ghost_cls=DirectionalGhost)\n",
        "    print(f\"Seed {seed}: {wr:.1%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hztGJQjN59s",
        "outputId": "c0406b6c-3aea-4453-ca49-8f0d52a87a3f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING AGENT (epsilon=0, greedy policy)\n",
            "============================================================\n",
            "\n",
            "Test 1/50: WIN | Score: 946.0 | Steps: 104\n",
            "Test 2/50: WIN | Score: 956.0 | Steps: 94\n",
            "Test 3/50: WIN | Score: 957.0 | Steps: 93\n",
            "Test 4/50: LOSE | Score: -500.0 | Steps: 1000\n",
            "Test 5/50: WIN | Score: 916.0 | Steps: 134\n",
            "Test 6/50: LOSE | Score: -199.0 | Steps: 59\n",
            "Test 7/50: WIN | Score: 968.0 | Steps: 82\n",
            "Test 8/50: WIN | Score: 949.0 | Steps: 101\n",
            "Test 9/50: WIN | Score: 983.0 | Steps: 67\n",
            "Test 10/50: WIN | Score: 865.0 | Steps: 185\n",
            "Test 11/50: WIN | Score: 921.0 | Steps: 129\n",
            "Test 12/50: WIN | Score: 930.0 | Steps: 120\n",
            "Test 13/50: WIN | Score: 916.0 | Steps: 134\n",
            "Test 14/50: WIN | Score: 911.0 | Steps: 139\n",
            "Test 15/50: WIN | Score: 884.0 | Steps: 166\n",
            "Test 16/50: WIN | Score: 942.0 | Steps: 108\n",
            "Test 17/50: WIN | Score: 927.0 | Steps: 123\n",
            "Test 18/50: WIN | Score: 924.0 | Steps: 126\n",
            "Test 19/50: WIN | Score: 894.0 | Steps: 156\n",
            "Test 20/50: WIN | Score: 924.0 | Steps: 126\n",
            "Test 21/50: WIN | Score: 937.0 | Steps: 113\n",
            "Test 22/50: WIN | Score: 936.0 | Steps: 114\n",
            "Test 23/50: WIN | Score: 976.0 | Steps: 74\n",
            "Test 24/50: WIN | Score: 908.0 | Steps: 142\n",
            "Test 25/50: WIN | Score: 908.0 | Steps: 142\n",
            "Test 26/50: WIN | Score: 956.0 | Steps: 94\n",
            "Test 27/50: WIN | Score: 962.0 | Steps: 88\n",
            "Test 28/50: WIN | Score: 946.0 | Steps: 104\n",
            "Test 29/50: WIN | Score: 952.0 | Steps: 98\n",
            "Test 30/50: WIN | Score: 927.0 | Steps: 123\n",
            "Test 31/50: WIN | Score: 947.0 | Steps: 103\n",
            "Test 32/50: LOSE | Score: -366.0 | Steps: 116\n",
            "Test 33/50: WIN | Score: 951.0 | Steps: 99\n",
            "Test 34/50: WIN | Score: 950.0 | Steps: 100\n",
            "Test 35/50: WIN | Score: 951.0 | Steps: 99\n",
            "Test 36/50: WIN | Score: 855.0 | Steps: 195\n",
            "Test 37/50: WIN | Score: 943.0 | Steps: 107\n",
            "Test 38/50: WIN | Score: 963.0 | Steps: 87\n",
            "Test 39/50: LOSE | Score: -283.0 | Steps: 73\n",
            "Test 40/50: WIN | Score: 968.0 | Steps: 82\n",
            "Test 41/50: WIN | Score: 981.0 | Steps: 69\n",
            "Test 42/50: WIN | Score: 909.0 | Steps: 141\n",
            "Test 43/50: WIN | Score: 969.0 | Steps: 81\n",
            "Test 44/50: LOSE | Score: -251.0 | Steps: 41\n",
            "Test 45/50: WIN | Score: 910.0 | Steps: 140\n",
            "Test 46/50: WIN | Score: 938.0 | Steps: 112\n",
            "Test 47/50: WIN | Score: 962.0 | Steps: 88\n",
            "Test 48/50: WIN | Score: 960.0 | Steps: 90\n",
            "Test 49/50: LOSE | Score: -395.0 | Steps: 115\n",
            "Test 50/50: WIN | Score: 946.0 | Steps: 104\n",
            "\n",
            "============================================================\n",
            "TEST RESULTS\n",
            "  Win rate: 88.00% (44/50)\n",
            "  Avg score: 784.6\n",
            "============================================================\n",
            "\n",
            "Seed 42: 88.0%\n",
            "\n",
            "============================================================\n",
            "TESTING AGENT (epsilon=0, greedy policy)\n",
            "============================================================\n",
            "\n",
            "Test 1/50: LOSE | Score: -133.0 | Steps: 173\n",
            "Test 2/50: WIN | Score: 946.0 | Steps: 104\n",
            "Test 3/50: LOSE | Score: -500.0 | Steps: 1000\n",
            "Test 4/50: WIN | Score: 961.0 | Steps: 89\n",
            "Test 5/50: WIN | Score: 926.0 | Steps: 124\n",
            "Test 6/50: WIN | Score: 926.0 | Steps: 124\n",
            "Test 7/50: WIN | Score: 926.0 | Steps: 124\n",
            "Test 8/50: WIN | Score: 944.0 | Steps: 106\n",
            "Test 9/50: WIN | Score: 980.0 | Steps: 70\n",
            "Test 10/50: WIN | Score: 922.0 | Steps: 128\n",
            "Test 11/50: WIN | Score: 894.0 | Steps: 156\n",
            "Test 12/50: WIN | Score: 890.0 | Steps: 160\n",
            "Test 13/50: WIN | Score: 977.0 | Steps: 73\n",
            "Test 14/50: WIN | Score: 940.0 | Steps: 110\n",
            "Test 15/50: WIN | Score: 942.0 | Steps: 108\n",
            "Test 16/50: WIN | Score: 962.0 | Steps: 88\n",
            "Test 17/50: LOSE | Score: -490.0 | Steps: 1000\n",
            "Test 18/50: WIN | Score: 935.0 | Steps: 115\n",
            "Test 19/50: WIN | Score: 932.0 | Steps: 118\n",
            "Test 20/50: WIN | Score: 924.0 | Steps: 126\n",
            "Test 21/50: WIN | Score: 905.0 | Steps: 145\n",
            "Test 22/50: WIN | Score: 924.0 | Steps: 126\n",
            "Test 23/50: WIN | Score: 938.0 | Steps: 112\n",
            "Test 24/50: WIN | Score: 940.0 | Steps: 110\n",
            "Test 25/50: WIN | Score: 937.0 | Steps: 113\n",
            "Test 26/50: WIN | Score: 963.0 | Steps: 87\n",
            "Test 27/50: WIN | Score: 896.0 | Steps: 154\n",
            "Test 28/50: WIN | Score: 931.0 | Steps: 119\n",
            "Test 29/50: WIN | Score: 906.0 | Steps: 144\n",
            "Test 30/50: WIN | Score: 906.0 | Steps: 144\n",
            "Test 31/50: WIN | Score: 968.0 | Steps: 82\n",
            "Test 32/50: WIN | Score: 853.0 | Steps: 197\n",
            "Test 33/50: WIN | Score: 874.0 | Steps: 176\n",
            "Test 34/50: WIN | Score: 942.0 | Steps: 108\n",
            "Test 35/50: WIN | Score: 952.0 | Steps: 98\n",
            "Test 36/50: WIN | Score: 918.0 | Steps: 132\n",
            "Test 37/50: WIN | Score: 982.0 | Steps: 68\n",
            "Test 38/50: WIN | Score: 976.0 | Steps: 74\n",
            "Test 39/50: WIN | Score: 955.0 | Steps: 95\n",
            "Test 40/50: WIN | Score: 989.0 | Steps: 61\n",
            "Test 41/50: WIN | Score: 967.0 | Steps: 83\n",
            "Test 42/50: WIN | Score: 915.0 | Steps: 135\n",
            "Test 43/50: WIN | Score: 966.0 | Steps: 84\n",
            "Test 44/50: WIN | Score: 957.0 | Steps: 93\n",
            "Test 45/50: WIN | Score: 933.0 | Steps: 117\n",
            "Test 46/50: WIN | Score: 969.0 | Steps: 81\n",
            "Test 47/50: WIN | Score: 943.0 | Steps: 107\n",
            "Test 48/50: WIN | Score: 933.0 | Steps: 117\n",
            "Test 49/50: WIN | Score: 989.0 | Steps: 61\n",
            "Test 50/50: WIN | Score: 980.0 | Steps: 70\n",
            "\n",
            "============================================================\n",
            "TEST RESULTS\n",
            "  Win rate: 94.00% (47/50)\n",
            "  Avg score: 860.2\n",
            "============================================================\n",
            "\n",
            "Seed 123: 94.0%\n",
            "\n",
            "============================================================\n",
            "TESTING AGENT (epsilon=0, greedy policy)\n",
            "============================================================\n",
            "\n",
            "Test 1/50: WIN | Score: 974.0 | Steps: 76\n",
            "Test 2/50: WIN | Score: 927.0 | Steps: 123\n",
            "Test 3/50: WIN | Score: 957.0 | Steps: 93\n",
            "Test 4/50: WIN | Score: 911.0 | Steps: 139\n",
            "Test 5/50: WIN | Score: 928.0 | Steps: 122\n",
            "Test 6/50: WIN | Score: 964.0 | Steps: 86\n",
            "Test 7/50: WIN | Score: 937.0 | Steps: 113\n",
            "Test 8/50: WIN | Score: 912.0 | Steps: 138\n",
            "Test 9/50: WIN | Score: 936.0 | Steps: 114\n",
            "Test 10/50: WIN | Score: 920.0 | Steps: 130\n",
            "Test 11/50: WIN | Score: 887.0 | Steps: 163\n",
            "Test 12/50: WIN | Score: 954.0 | Steps: 96\n",
            "Test 13/50: WIN | Score: 945.0 | Steps: 105\n",
            "Test 14/50: WIN | Score: 965.0 | Steps: 85\n",
            "Test 15/50: LOSE | Score: -205.0 | Steps: 65\n",
            "Test 16/50: WIN | Score: 946.0 | Steps: 104\n",
            "Test 17/50: LOSE | Score: -209.0 | Steps: 69\n",
            "Test 18/50: WIN | Score: 932.0 | Steps: 118\n",
            "Test 19/50: WIN | Score: 915.0 | Steps: 135\n",
            "Test 20/50: WIN | Score: 924.0 | Steps: 126\n",
            "Test 21/50: WIN | Score: 943.0 | Steps: 107\n",
            "Test 22/50: WIN | Score: 972.0 | Steps: 78\n",
            "Test 23/50: WIN | Score: 954.0 | Steps: 96\n",
            "Test 24/50: LOSE | Score: -253.0 | Steps: 43\n",
            "Test 25/50: WIN | Score: 974.0 | Steps: 76\n",
            "Test 26/50: WIN | Score: 891.0 | Steps: 159\n",
            "Test 27/50: WIN | Score: 897.0 | Steps: 153\n",
            "Test 28/50: WIN | Score: 862.0 | Steps: 188\n",
            "Test 29/50: WIN | Score: 959.0 | Steps: 91\n",
            "Test 30/50: WIN | Score: 977.0 | Steps: 73\n",
            "Test 31/50: WIN | Score: 904.0 | Steps: 146\n",
            "Test 32/50: WIN | Score: 920.0 | Steps: 130\n",
            "Test 33/50: WIN | Score: 907.0 | Steps: 143\n",
            "Test 34/50: WIN | Score: 952.0 | Steps: 98\n",
            "Test 35/50: WIN | Score: 948.0 | Steps: 102\n",
            "Test 36/50: WIN | Score: 931.0 | Steps: 119\n",
            "Test 37/50: WIN | Score: 924.0 | Steps: 126\n",
            "Test 38/50: WIN | Score: 926.0 | Steps: 124\n",
            "Test 39/50: WIN | Score: 907.0 | Steps: 143\n",
            "Test 40/50: WIN | Score: 859.0 | Steps: 191\n",
            "Test 41/50: WIN | Score: 919.0 | Steps: 131\n",
            "Test 42/50: WIN | Score: 932.0 | Steps: 118\n",
            "Test 43/50: WIN | Score: 958.0 | Steps: 92\n",
            "Test 44/50: WIN | Score: 904.0 | Steps: 146\n",
            "Test 45/50: LOSE | Score: -103.0 | Steps: 123\n",
            "Test 46/50: WIN | Score: 900.0 | Steps: 150\n",
            "Test 47/50: WIN | Score: 938.0 | Steps: 112\n",
            "Test 48/50: WIN | Score: 953.0 | Steps: 97\n",
            "Test 49/50: WIN | Score: 905.0 | Steps: 145\n",
            "Test 50/50: WIN | Score: 938.0 | Steps: 112\n",
            "\n",
            "============================================================\n",
            "TEST RESULTS\n",
            "  Win rate: 92.00% (46/50)\n",
            "  Avg score: 840.4\n",
            "============================================================\n",
            "\n",
            "Seed 456: 92.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "testiamo il nostro agente su di un layout diverso (solo con il random).\n",
        "<br> vedremo:\n",
        "* utilizziamo lo stesso agente --> testiamo la capacità di generalizzazione, quindi facciamo un test sulla robustesta"
      ],
      "metadata": {
        "id": "j-_Te_4mi6Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- stesso agente ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST: 1 random ghost, medium_classic\")\n",
        "print(\"=\"*70)\n",
        "test2_wr, test2_score = test_agent(agent,\n",
        "                                    num_episodes=50,\n",
        "                                    n_ghosts=1,\n",
        "                                    ghost_cls=DirectionalGhost,\n",
        "                                    layout_name=\"medium_classic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrNrn6Yki575",
        "outputId": "03cf77ef-d2b6-4719-955a-dd5472fdcce4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST: 1 random ghost, medium_classic\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "TESTING AGENT (epsilon=0, greedy policy)\n",
            "============================================================\n",
            "\n",
            "Test 1/50: WIN | Score: 1289.0 | Steps: 181\n",
            "Test 2/50: WIN | Score: 781.0 | Steps: 689\n",
            "Test 3/50: LOSE | Score: 173.0 | Steps: 197\n",
            "Test 4/50: WIN | Score: 1300.0 | Steps: 170\n",
            "Test 5/50: WIN | Score: 1213.0 | Steps: 257\n",
            "Test 6/50: WIN | Score: 1262.0 | Steps: 208\n",
            "Test 7/50: WIN | Score: 1019.0 | Steps: 451\n",
            "Test 8/50: LOSE | Score: -115.0 | Steps: 145\n",
            "Test 9/50: WIN | Score: 1239.0 | Steps: 231\n",
            "Test 10/50: WIN | Score: 1041.0 | Steps: 429\n",
            "Test 11/50: WIN | Score: 1223.0 | Steps: 247\n",
            "Test 12/50: WIN | Score: 1110.0 | Steps: 360\n",
            "Test 13/50: WIN | Score: 1227.0 | Steps: 243\n",
            "Test 14/50: WIN | Score: 1240.0 | Steps: 230\n",
            "Test 15/50: WIN | Score: 811.0 | Steps: 659\n",
            "Test 16/50: WIN | Score: 1131.0 | Steps: 339\n",
            "Test 17/50: WIN | Score: 1035.0 | Steps: 435\n",
            "Test 18/50: WIN | Score: 840.0 | Steps: 630\n",
            "Test 19/50: WIN | Score: 1233.0 | Steps: 237\n",
            "Test 20/50: WIN | Score: 1291.0 | Steps: 179\n",
            "Test 21/50: WIN | Score: 1152.0 | Steps: 318\n",
            "Test 22/50: WIN | Score: 1225.0 | Steps: 245\n",
            "Test 23/50: WIN | Score: 1205.0 | Steps: 265\n",
            "Test 24/50: WIN | Score: 1157.0 | Steps: 313\n",
            "Test 25/50: WIN | Score: 1236.0 | Steps: 234\n",
            "Test 26/50: WIN | Score: 1183.0 | Steps: 287\n",
            "Test 27/50: WIN | Score: 1261.0 | Steps: 209\n",
            "Test 28/50: WIN | Score: 1294.0 | Steps: 176\n",
            "Test 29/50: WIN | Score: 1226.0 | Steps: 244\n",
            "Test 30/50: WIN | Score: 1258.0 | Steps: 212\n",
            "Test 31/50: WIN | Score: 1273.0 | Steps: 197\n",
            "Test 32/50: WIN | Score: 1173.0 | Steps: 297\n",
            "Test 33/50: WIN | Score: 1217.0 | Steps: 253\n",
            "Test 34/50: WIN | Score: 1123.0 | Steps: 347\n",
            "Test 35/50: WIN | Score: 1279.0 | Steps: 191\n",
            "Test 36/50: WIN | Score: 1136.0 | Steps: 334\n",
            "Test 37/50: WIN | Score: 1119.0 | Steps: 351\n",
            "Test 38/50: LOSE | Score: -71.0 | Steps: 111\n",
            "Test 39/50: WIN | Score: 1199.0 | Steps: 271\n",
            "Test 40/50: WIN | Score: 1292.0 | Steps: 178\n",
            "Test 41/50: WIN | Score: 1183.0 | Steps: 287\n",
            "Test 42/50: WIN | Score: 1226.0 | Steps: 244\n",
            "Test 43/50: WIN | Score: 1147.0 | Steps: 323\n",
            "Test 44/50: WIN | Score: 1114.0 | Steps: 356\n",
            "Test 45/50: WIN | Score: 1193.0 | Steps: 277\n",
            "Test 46/50: WIN | Score: 1255.0 | Steps: 215\n",
            "Test 47/50: WIN | Score: 1274.0 | Steps: 196\n",
            "Test 48/50: LOSE | Score: -71.0 | Steps: 111\n",
            "Test 49/50: WIN | Score: 1192.0 | Steps: 278\n",
            "Test 50/50: WIN | Score: 1245.0 | Steps: 225\n",
            "\n",
            "============================================================\n",
            "TEST RESULTS\n",
            "  Win rate: 92.00% (46/50)\n",
            "  Avg score: 1080.8\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- stesso agente ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST: 1 random ghost, original_classic\")\n",
        "print(\"=\"*70)\n",
        "test3_wr, test3_score = test_agent(agent,\n",
        "                                    num_episodes=50,\n",
        "                                    n_ghosts=1,\n",
        "                                    ghost_cls=DirectionalGhost,\n",
        "                                    layout_name=\"original_classic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNF2IeWpkIiB",
        "outputId": "ce562115-ef80-4aa6-cf0d-246c1b932e2c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST: 1 random ghost, original_classic\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "TESTING AGENT (epsilon=0, greedy policy)\n",
            "============================================================\n",
            "\n",
            "Test 1/50: LOSE | Score: 1060.0 | Steps: 1000\n",
            "Test 2/50: WIN | Score: 1936.0 | Steps: 854\n",
            "Test 3/50: LOSE | Score: 650.0 | Steps: 1000\n",
            "Test 4/50: LOSE | Score: 1110.0 | Steps: 1000\n",
            "Test 5/50: LOSE | Score: 630.0 | Steps: 1000\n",
            "Test 6/50: LOSE | Score: 682.0 | Steps: 448\n",
            "Test 7/50: LOSE | Score: 790.0 | Steps: 480\n",
            "Test 8/50: LOSE | Score: 550.0 | Steps: 580\n",
            "Test 9/50: LOSE | Score: 1140.0 | Steps: 1000\n",
            "Test 10/50: LOSE | Score: 1030.0 | Steps: 1000\n",
            "Test 11/50: LOSE | Score: 630.0 | Steps: 1000\n",
            "Test 12/50: LOSE | Score: 373.0 | Steps: 297\n",
            "Test 13/50: LOSE | Score: 1100.0 | Steps: 1000\n",
            "Test 14/50: LOSE | Score: 1100.0 | Steps: 1000\n",
            "Test 15/50: LOSE | Score: 512.0 | Steps: 568\n",
            "Test 16/50: LOSE | Score: 630.0 | Steps: 1000\n",
            "Test 17/50: WIN | Score: 1962.0 | Steps: 828\n",
            "Test 18/50: LOSE | Score: 630.0 | Steps: 1000\n",
            "Test 19/50: WIN | Score: 2282.0 | Steps: 508\n",
            "Test 20/50: WIN | Score: 2239.0 | Steps: 551\n",
            "Test 21/50: WIN | Score: 2131.0 | Steps: 659\n",
            "Test 22/50: LOSE | Score: 1060.0 | Steps: 1000\n",
            "Test 23/50: WIN | Score: 2174.0 | Steps: 616\n",
            "Test 24/50: LOSE | Score: 780.0 | Steps: 1000\n",
            "Test 25/50: LOSE | Score: 58.0 | Steps: 342\n",
            "Test 26/50: WIN | Score: 2068.0 | Steps: 722\n",
            "Test 27/50: WIN | Score: 2016.0 | Steps: 774\n",
            "Test 28/50: WIN | Score: 1948.0 | Steps: 842\n",
            "Test 29/50: LOSE | Score: 1170.0 | Steps: 1000\n",
            "Test 30/50: LOSE | Score: 1230.0 | Steps: 1000\n",
            "Test 31/50: LOSE | Score: 401.0 | Steps: 269\n",
            "Test 32/50: LOSE | Score: 860.0 | Steps: 620\n",
            "Test 33/50: LOSE | Score: 630.0 | Steps: 1000\n",
            "Test 34/50: LOSE | Score: 1100.0 | Steps: 1000\n",
            "Test 35/50: WIN | Score: 2097.0 | Steps: 693\n",
            "Test 36/50: LOSE | Score: 578.0 | Steps: 462\n",
            "Test 37/50: WIN | Score: 1903.0 | Steps: 887\n",
            "Test 38/50: WIN | Score: 1824.0 | Steps: 966\n",
            "Test 39/50: LOSE | Score: 391.0 | Steps: 479\n",
            "Test 40/50: WIN | Score: 1935.0 | Steps: 855\n",
            "Test 41/50: LOSE | Score: 740.0 | Steps: 390\n",
            "Test 42/50: WIN | Score: 2028.0 | Steps: 762\n",
            "Test 43/50: WIN | Score: 2197.0 | Steps: 593\n",
            "Test 44/50: LOSE | Score: 975.0 | Steps: 705\n",
            "Test 45/50: LOSE | Score: 1130.0 | Steps: 1000\n",
            "Test 46/50: LOSE | Score: 766.0 | Steps: 504\n",
            "Test 47/50: LOSE | Score: 630.0 | Steps: 1000\n",
            "Test 48/50: WIN | Score: 1863.0 | Steps: 927\n",
            "Test 49/50: LOSE | Score: 586.0 | Steps: 544\n",
            "Test 50/50: LOSE | Score: 630.0 | Steps: 1000\n",
            "\n",
            "============================================================\n",
            "TEST RESULTS\n",
            "  Win rate: 32.00% (16/50)\n",
            "  Avg score: 1178.7\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "questo training sotto (e relativo test) è solo per provare come sarebbe il modello se venissero introdotti due fatasmi nella partita"
      ],
      "metadata": {
        "id": "GlEfm0pCmLsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFASE 3: Training con 2 DirectionalGhost\")\n",
        "results_2D = train_with_progress1(agent,\n",
        "                     num_episodes=500,\n",
        "                     n_ghosts=2,\n",
        "                     ghost_cls=DirectionalGhost,\n",
        "                     layout_name=\"small_classic\",\n",
        "                     print_every=50)\n",
        "\n",
        "# Test con seed diversi\n",
        "for seed in [42, 128, 456]:\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    wr, _ = test_agent(agent, num_episodes=50,\n",
        "                       n_ghosts=2, ghost_cls=DirectionalGhost)\n",
        "    print(f\"Seed {seed}: {wr:.1%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuhvRMj4O0T3",
        "outputId": "582a6a65-ae48-49f6-d1f5-70aaa707fc08",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FASE 3: Training con 2 DirectionalGhost\n",
            "\n",
            "============================================================\n",
            "Episode 50/500\n",
            "  Win rate (ultimi 50): 16.00%\n",
            "  Win rate (globale): 16.00%\n",
            "  Avg score: -93.8 | Avg steps: 59.6\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 763\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 100/500\n",
            "  Win rate (ultimi 50): 6.00%\n",
            "  Win rate (globale): 11.00%\n",
            "  Avg score: -218.3 | Avg steps: 46.3\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 825\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 150/500\n",
            "  Win rate (ultimi 50): 12.00%\n",
            "  Win rate (globale): 11.33%\n",
            "  Avg score: -124.7 | Avg steps: 60.3\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 880\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 200/500\n",
            "  Win rate (ultimi 50): 6.00%\n",
            "  Win rate (globale): 10.00%\n",
            "  Avg score: -237.5 | Avg steps: 47.9\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 904\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 250/500\n",
            "  Win rate (ultimi 50): 4.00%\n",
            "  Win rate (globale): 8.80%\n",
            "  Avg score: -249.5 | Avg steps: 50.3\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 914\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 300/500\n",
            "  Win rate (ultimi 50): 10.00%\n",
            "  Win rate (globale): 9.00%\n",
            "  Avg score: -176.8 | Avg steps: 49.4\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 925\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 350/500\n",
            "  Win rate (ultimi 50): 2.00%\n",
            "  Win rate (globale): 8.00%\n",
            "  Avg score: -237.1 | Avg steps: 47.9\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 943\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 400/500\n",
            "  Win rate (ultimi 50): 4.00%\n",
            "  Win rate (globale): 7.50%\n",
            "  Avg score: -270.4 | Avg steps: 44.2\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 945\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 450/500\n",
            "  Win rate (ultimi 50): 0.00%\n",
            "  Win rate (globale): 6.67%\n",
            "  Avg score: -314.1 | Avg steps: 44.9\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 949\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Episode 500/500\n",
            "  Win rate (ultimi 50): 2.00%\n",
            "  Win rate (globale): 6.20%\n",
            "  Avg score: -295.0 | Avg steps: 47.0\n",
            "  Epsilon: 0.050\n",
            "  Q-values appresi: 956\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETATO\n",
            "============================================================\n",
            "Episodi totali: 500\n",
            "Vittorie: 31 (6.20%)\n",
            "Sconfitte: 469 (93.80%)\n",
            "Q-values finali: 956\n",
            "Epsilon finale: 0.050\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "TESTING AGENT (epsilon=0, greedy policy)\n",
            "============================================================\n",
            "\n",
            "Test 1/50: LOSE | Score: -368.0 | Steps: 18\n",
            "Test 2/50: LOSE | Score: -368.0 | Steps: 18\n",
            "Test 3/50: LOSE | Score: -377.0 | Steps: 17\n",
            "Test 4/50: LOSE | Score: -95.0 | Steps: 95\n",
            "Test 5/50: LOSE | Score: -368.0 | Steps: 18\n",
            "Test 6/50: LOSE | Score: -378.0 | Steps: 18\n",
            "Test 7/50: LOSE | Score: -377.0 | Steps: 17\n",
            "Test 8/50: LOSE | Score: -78.0 | Steps: 108\n",
            "Test 9/50: LOSE | Score: -69.0 | Steps: 69\n",
            "Test 10/50: LOSE | Score: -327.0 | Steps: 27\n",
            "Test 11/50: LOSE | Score: -316.0 | Steps: 56\n",
            "Test 12/50: LOSE | Score: -368.0 | Steps: 18\n",
            "Test 13/50: LOSE | Score: -355.0 | Steps: 35\n",
            "Test 14/50: LOSE | Score: -368.0 | Steps: 18\n",
            "Test 15/50: LOSE | Score: -360.0 | Steps: 40\n",
            "Test 16/50: LOSE | Score: -377.0 | Steps: 17\n",
            "Test 17/50: LOSE | Score: -353.0 | Steps: 33\n",
            "Test 18/50: LOSE | Score: -395.0 | Steps: 15\n",
            "Test 19/50: WIN | Score: 952.0 | Steps: 98\n",
            "Test 20/50: LOSE | Score: -369.0 | Steps: 19\n",
            "Test 21/50: LOSE | Score: -379.0 | Steps: 29\n",
            "Test 22/50: LOSE | Score: -145.0 | Steps: 45\n",
            "Test 23/50: LOSE | Score: -399.0 | Steps: 19\n",
            "Test 24/50: LOSE | Score: -326.0 | Steps: 46\n",
            "Test 25/50: LOSE | Score: -228.0 | Steps: 88\n",
            "Test 26/50: LOSE | Score: -312.0 | Steps: 42\n",
            "Test 27/50: LOSE | Score: -58.0 | Steps: 98\n",
            "Test 28/50: LOSE | Score: -339.0 | Steps: 29\n",
            "Test 29/50: LOSE | Score: -338.0 | Steps: 38\n",
            "Test 30/50: LOSE | Score: -369.0 | Steps: 19\n",
            "Test 31/50: LOSE | Score: -384.0 | Steps: 24\n",
            "Test 32/50: LOSE | Score: -377.0 | Steps: 17\n",
            "Test 33/50: LOSE | Score: -343.0 | Steps: 43\n",
            "Test 34/50: LOSE | Score: -230.0 | Steps: 60\n",
            "Test 35/50: LOSE | Score: -377.0 | Steps: 17\n",
            "Test 36/50: LOSE | Score: -321.0 | Steps: 41\n",
            "Test 37/50: LOSE | Score: -357.0 | Steps: 37\n",
            "Test 38/50: LOSE | Score: -357.0 | Steps: 37\n",
            "Test 39/50: LOSE | Score: -312.0 | Steps: 52\n",
            "Test 40/50: LOSE | Score: -252.0 | Steps: 42\n",
            "Test 41/50: LOSE | Score: -369.0 | Steps: 19\n",
            "Test 42/50: LOSE | Score: -253.0 | Steps: 43\n",
            "Test 43/50: LOSE | Score: -368.0 | Steps: 18\n",
            "Test 44/50: LOSE | Score: -361.0 | Steps: 41\n",
            "Test 45/50: LOSE | Score: -298.0 | Steps: 28\n",
            "Test 46/50: LOSE | Score: -377.0 | Steps: 17\n",
            "Test 47/50: LOSE | Score: -360.0 | Steps: 40\n",
            "Test 48/50: LOSE | Score: -253.0 | Steps: 43\n",
            "Test 49/50: LOSE | Score: -324.0 | Steps: 44\n",
            "Test 50/50: LOSE | Score: -370.0 | Steps: 20\n",
            "\n",
            "============================================================\n",
            "TEST RESULTS\n",
            "  Win rate: 2.00% (1/50)\n",
            "  Avg score: -293.0\n",
            "============================================================\n",
            "\n",
            "Seed 42: 2.0%\n",
            "\n",
            "============================================================\n",
            "TESTING AGENT (epsilon=0, greedy policy)\n",
            "============================================================\n",
            "\n",
            "Test 1/50: LOSE | Score: -326.0 | Steps: 46\n",
            "Test 2/50: LOSE | Score: -368.0 | Steps: 18\n",
            "Test 3/50: LOSE | Score: -334.0 | Steps: 24\n",
            "Test 4/50: LOSE | Score: -356.0 | Steps: 36\n",
            "Test 5/50: LOSE | Score: -304.0 | Steps: 44\n",
            "Test 6/50: LOSE | Score: -356.0 | Steps: 36\n",
            "Test 7/50: LOSE | Score: -399.0 | Steps: 19\n",
            "Test 8/50: LOSE | Score: -287.0 | Steps: 47\n",
            "Test 9/50: WIN | Score: 941.0 | Steps: 109\n",
            "Test 10/50: LOSE | Score: -225.0 | Steps: 55\n",
            "Test 11/50: LOSE | Score: -377.0 | Steps: 17\n",
            "Test 12/50: LOSE | Score: -255.0 | Steps: 85\n",
            "Test 13/50: LOSE | Score: -153.0 | Steps: 43\n",
            "Test 14/50: LOSE | Score: -94.0 | Steps: 114\n",
            "Test 15/50: LOSE | Score: -368.0 | Steps: 18\n",
            "Test 16/50: LOSE | Score: -380.0 | Steps: 20\n",
            "Test 17/50: LOSE | Score: -377.0 | Steps: 17\n",
            "Test 18/50: LOSE | Score: -383.0 | Steps: 23\n",
            "Test 19/50: LOSE | Score: -377.0 | Steps: 17\n",
            "Test 20/50: LOSE | Score: -348.0 | Steps: 28\n",
            "Test 21/50: LOSE | Score: -374.0 | Steps: 24\n",
            "Test 22/50: WIN | Score: 942.0 | Steps: 108\n",
            "Test 23/50: LOSE | Score: -377.0 | Steps: 17\n",
            "Test 24/50: LOSE | Score: -387.0 | Steps: 17\n",
            "Test 25/50: LOSE | Score: -262.0 | Steps: 82\n",
            "Test 26/50: LOSE | Score: -255.0 | Steps: 75\n",
            "Test 27/50: LOSE | Score: -368.0 | Steps: 18\n",
            "Test 28/50: LOSE | Score: -368.0 | Steps: 18\n",
            "Test 29/50: LOSE | Score: -375.0 | Steps: 25\n",
            "Test 30/50: LOSE | Score: -254.0 | Steps: 74\n",
            "Test 31/50: LOSE | Score: -92.0 | Steps: 52\n",
            "Test 32/50: LOSE | Score: -336.0 | Steps: 26\n",
            "Test 33/50: LOSE | Score: -369.0 | Steps: 19\n",
            "Test 34/50: LOSE | Score: -319.0 | Steps: 49\n",
            "Test 35/50: LOSE | Score: -327.0 | Steps: 27\n",
            "Test 36/50: LOSE | Score: -368.0 | Steps: 18\n",
            "Test 37/50: LOSE | Score: -326.0 | Steps: 46\n",
            "Test 38/50: LOSE | Score: -252.0 | Steps: 42\n",
            "Test 39/50: LOSE | Score: -378.0 | Steps: 18\n",
            "Test 40/50: LOSE | Score: -333.0 | Steps: 63\n",
            "Test 41/50: LOSE | Score: -267.0 | Steps: 117\n",
            "Test 42/50: WIN | Score: 938.0 | Steps: 112\n",
            "Test 43/50: LOSE | Score: -386.0 | Steps: 16\n",
            "Test 44/50: LOSE | Score: -335.0 | Steps: 35\n",
            "Test 45/50: LOSE | Score: -242.0 | Steps: 62\n",
            "Test 46/50: LOSE | Score: -328.0 | Steps: 48\n",
            "Test 47/50: LOSE | Score: -368.0 | Steps: 18\n",
            "Test 48/50: LOSE | Score: -336.0 | Steps: 36\n",
            "Test 49/50: LOSE | Score: -329.0 | Steps: 59\n",
            "Test 50/50: LOSE | Score: -130.0 | Steps: 90\n",
            "\n",
            "============================================================\n",
            "TEST RESULTS\n",
            "  Win rate: 6.00% (3/50)\n",
            "  Avg score: -241.7\n",
            "============================================================\n",
            "\n",
            "Seed 128: 6.0%\n",
            "\n",
            "============================================================\n",
            "TESTING AGENT (epsilon=0, greedy policy)\n",
            "============================================================\n",
            "\n",
            "Test 1/50: LOSE | Score: -377.0 | Steps: 17\n",
            "Test 2/50: LOSE | Score: -326.0 | Steps: 46\n",
            "Test 3/50: LOSE | Score: -111.0 | Steps: 101\n",
            "Test 4/50: LOSE | Score: -357.0 | Steps: 37\n",
            "Test 5/50: LOSE | Score: -358.0 | Steps: 38\n",
            "Test 6/50: LOSE | Score: -380.0 | Steps: 20\n",
            "Test 7/50: LOSE | Score: -365.0 | Steps: 25\n",
            "Test 8/50: LOSE | Score: -330.0 | Steps: 40\n",
            "Test 9/50: LOSE | Score: -138.0 | Steps: 48\n",
            "Test 10/50: LOSE | Score: -73.0 | Steps: 73\n",
            "Test 11/50: LOSE | Score: -81.0 | Steps: 111\n",
            "Test 12/50: LOSE | Score: -386.0 | Steps: 16\n",
            "Test 13/50: LOSE | Score: -94.0 | Steps: 54\n",
            "Test 14/50: LOSE | Score: -252.0 | Steps: 42\n",
            "Test 15/50: LOSE | Score: -252.0 | Steps: 42\n",
            "Test 16/50: LOSE | Score: -312.0 | Steps: 42\n",
            "Test 17/50: LOSE | Score: -338.0 | Steps: 128\n",
            "Test 18/50: LOSE | Score: -340.0 | Steps: 30\n",
            "Test 19/50: LOSE | Score: -325.0 | Steps: 55\n",
            "Test 20/50: LOSE | Score: -313.0 | Steps: 53\n",
            "Test 21/50: LOSE | Score: -377.0 | Steps: 17\n",
            "Test 22/50: LOSE | Score: -389.0 | Steps: 19\n",
            "Test 23/50: LOSE | Score: -358.0 | Steps: 38\n",
            "Test 24/50: LOSE | Score: -229.0 | Steps: 39\n",
            "Test 25/50: LOSE | Score: -359.0 | Steps: 39\n",
            "Test 26/50: LOSE | Score: -351.0 | Steps: 31\n",
            "Test 27/50: LOSE | Score: -78.0 | Steps: 88\n",
            "Test 28/50: LOSE | Score: -398.0 | Steps: 18\n",
            "Test 29/50: LOSE | Score: -287.0 | Steps: 57\n",
            "Test 30/50: LOSE | Score: -73.0 | Steps: 103\n",
            "Test 31/50: LOSE | Score: -186.0 | Steps: 86\n",
            "Test 32/50: LOSE | Score: -257.0 | Steps: 87\n",
            "Test 33/50: LOSE | Score: -369.0 | Steps: 49\n",
            "Test 34/50: LOSE | Score: -369.0 | Steps: 19\n",
            "Test 35/50: LOSE | Score: -326.0 | Steps: 46\n",
            "Test 36/50: LOSE | Score: -382.0 | Steps: 22\n",
            "Test 37/50: LOSE | Score: -368.0 | Steps: 18\n",
            "Test 38/50: LOSE | Score: -377.0 | Steps: 17\n",
            "Test 39/50: LOSE | Score: -312.0 | Steps: 52\n",
            "Test 40/50: LOSE | Score: -369.0 | Steps: 19\n",
            "Test 41/50: LOSE | Score: -335.0 | Steps: 35\n",
            "Test 42/50: LOSE | Score: -332.0 | Steps: 22\n",
            "Test 43/50: LOSE | Score: -384.0 | Steps: 24\n",
            "Test 44/50: LOSE | Score: -368.0 | Steps: 18\n",
            "Test 45/50: LOSE | Score: -331.0 | Steps: 71\n",
            "Test 46/50: LOSE | Score: -356.0 | Steps: 36\n",
            "Test 47/50: LOSE | Score: -383.0 | Steps: 23\n",
            "Test 48/50: LOSE | Score: -377.0 | Steps: 17\n",
            "Test 49/50: WIN | Score: 922.0 | Steps: 128\n",
            "Test 50/50: LOSE | Score: -368.0 | Steps: 18\n",
            "\n",
            "============================================================\n",
            "TEST RESULTS\n",
            "  Win rate: 2.00% (1/50)\n",
            "  Avg score: -280.7\n",
            "============================================================\n",
            "\n",
            "Seed 456: 2.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***FASE 4:*** PROGETTAZIONE AGENTE Q-LEARNING MIGLIORATO -> **DQN**"
      ],
      "metadata": {
        "id": "BlNvlMgzu6FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- LIBRERIE NECESSARIE ----\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque, namedtuple"
      ],
      "metadata": {
        "id": "mcDv76OBYfVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apj5N14oKh_M"
      },
      "outputs": [],
      "source": [
        "# ---- DEFINIZIONE DELLA DQN PER PACMAN ----\n",
        "# struttura MLP profonda con 3 hidden layer: 128 --> 128 --> 64 -> 32 -> 4 e layernorm dopo ogni layer\n",
        "# ho scelto layernorm invece di batchnorm perché è più stabile con piccoli batch\n",
        "# inizializzazione con xavier che di solito viene usata insieme alla relu\n",
        "# di default ho messo che lo state è un vettore di 33 dimensioni\n",
        "# è la rete neurale vera e propria, prende in input una rappresentazione numerica dello stato di pacman del labirinto\n",
        "# restituisce i Q-values per ciascuna delle azioni possibili\n",
        "# q-values = vettore di 4 elementi uno per ogni azioni e rappresenta quanto è \"buono\" prendere quella direzione (nord, sud, est, ovest)\n",
        "class PacmanDQN(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size=33, action_size=4, hidden_size=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size) # 33 -> 128\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size) # 128 -> 128\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size // 2 ) # 128 ->64\n",
        "        self.fc4 = nn.Linear(hidden_size//2, hidden_size // 4 ) # 64 -> 32\n",
        "        self.fc5 = nn.Linear(hidden_size // 4, action_size) # 32 -> 4\n",
        "\n",
        "        # BatchNorm invece di Dropout (più stabile per DQN)\n",
        "        self.bn1 = nn.LayerNorm(hidden_size)\n",
        "        self.bn2 = nn.LayerNorm(hidden_size)\n",
        "        self.bn3 = nn.LayerNorm(hidden_size // 2)\n",
        "        self.bn4 = nn.LayerNorm(hidden_size // 4)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "# inizializzazione Xavier per convergenza più veloce\n",
        "    def _initialize_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "\n",
        "        # Auto-reshape per singolo stato\n",
        "        if state.dim() == 1:\n",
        "            state = state.unsqueeze(0)\n",
        "            squeeze = True\n",
        "        else:\n",
        "            squeeze = False\n",
        "\n",
        "        # Forward pass\n",
        "        x = self.bn1(F.relu(self.fc1(state)))\n",
        "        x = self.bn2(F.relu(self.fc2(x)))\n",
        "        x = self.bn3(F.relu(self.fc3(x)))\n",
        "        x = self.bn4(F.relu(self.fc4(x)))\n",
        "        q_values = self.fc5(x)\n",
        "\n",
        "        # Rimuoviamo batch dim se era singolo stato\n",
        "        if squeeze:\n",
        "            q_values = q_values.squeeze(0)\n",
        "\n",
        "        return q_values\n",
        "\n",
        "\n",
        "\n",
        "# ---- REPLAY BUFFER ----\n",
        "\n",
        "# è importante che done_batch vada a gestire correttamente i futuri reward altrimenti il modello potrebbe apprendere stati che non ci sono più\n",
        "# la capacità del buffer messa per default a 10k, si potrebbe pensare di ampliarla a 50k dipende anche dagli step medi che fa\n",
        "# è una struttura di memoria, serve a salvare e riutilizzare esperienze passate, ovvero le transizioni che pacman ha vissuto durante il gioco\n",
        "# una transition è una tupla che ci dice \"ero nello stato s, ho fatto l’azione a, sono finito nello stato s', ho ricevuto il reward r, e magari il gioco è finito (done=True)\"\n",
        "# ci serve per far sì che pacman non usi esperienze molto correlate tra loro (magari facenti parte dello stesso ep) perché causerebbe instabilità e inefficienza per vari motivi\n",
        "# con sample(batch) si prendono esperienze da momenti differenti\n",
        "# la grandezza del batch è un iperparamentro che influenza la stabilità e la velocità di apprendimento. il batch_size è il numero di transizioni che vengono prelevate dal replay buffer per ogni aggiornamento della rete\n",
        "# ogni batch è un mini-dataset diciamo, dove viene calcolata la loss e vengono aggiornati i pesi\n",
        "# valori tipici 32, 64, 128 (in questo caso la scelta è fra i primi due)\n",
        "# diciamo che possiamo vederlo come la \"memoria dell'agente\"\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self._capacity = capacity\n",
        "\n",
        "\n",
        "    def capacity(self):\n",
        "        return self._capacity\n",
        "\n",
        "    def push(self, state, action, next_state, reward, done):\n",
        "        self.buffer.append(Transition(state, action, next_state, reward, done))\n",
        "# preparazione dei dati per aggiornare la rete neurale\n",
        "    def sample(self, batch_size):\n",
        "      # selezioniamo batch_size transizioni a caso dal buffer\n",
        "        transitions = random.sample(self.buffer, batch_size)\n",
        "        # definizione di un oggetto transition\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        # Stack in tensori\n",
        "        state_batch = torch.stack(batch.state)\n",
        "        action_batch = torch.tensor(batch.action, dtype=torch.long)\n",
        "        next_state_batch = torch.stack(batch.next_state)\n",
        "        reward_batch = torch.tensor(batch.reward, dtype=torch.float32)\n",
        "        done_batch = torch.tensor(batch.done, dtype=torch.float32)\n",
        "\n",
        "        return state_batch, action_batch, next_state_batch, reward_batch, done_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- AGENTE Q-LEARNING DQN ----\n",
        "class DQNAgent(Agent):\n",
        "\n",
        "# costruttore, inizializza i paramentri di apprendimento, crea la rete neurale (policy + target), inizializza il replay buffer, parametri ausiliari etc\n",
        "# viene usato adam come ottimizzatore, classico nelle DQN\n",
        "# distinzione tra policy_net e target_net permette l'uso di una double dqn\n",
        "# loss scelta: SmoothL1Loss (rispetto ad mse)\n",
        "# valori proposti:\n",
        "      # learning rate: sotto 0.0003, anche se per ora 0.0005 è una soluzione ragionevole --> quanto velocemente la rete modifica i propri pesi dopo ogni aggiornamento\n",
        "      # tau al posto del target update con valori da provare 0.005 (è quello standard e consigliato), 0.001 molta stabilità ma molto lento\n",
        "      # epsilon decay per ora ha decadimento esponenziale, si può pensare di cambiarlo in modo lineare --> quanto velocemente passa da esplorazione casuale a sfruttamento\n",
        "      # si consiglia 0.99 o 0.985 e decadimento lineare\n",
        "    def __init__(self, state_size=33, action_size=4,\n",
        "                 alpha=0.0005,  # learning_rate\n",
        "                 gamma=0.99,\n",
        "                 epsilon=1.0,\n",
        "                 epsilon_end=0.01,\n",
        "                 epsilon_decay=0.995,\n",
        "                 buffer_size=10000,\n",
        "                 batch_size=64,\n",
        "                 tau = 0.005,\n",
        "                 device=None,\n",
        "                 verbose=False\n",
        "                ):\n",
        "\n",
        "        # Parametri base\n",
        "        self.gamma = float(gamma)\n",
        "        self.epsilon_end = float(epsilon_end)\n",
        "        self.epsilon_decay = float(epsilon_decay)\n",
        "        self.epsilon = float(epsilon)\n",
        "        self.verbose = verbose\n",
        "        self.episode_count = 0\n",
        "\n",
        "\n",
        "        # Parametri DQN\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.batch_size = batch_size\n",
        "        self.tau = tau\n",
        "        self.learning_rate = alpha\n",
        "\n",
        "        # Device\n",
        "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Reti neurali\n",
        "        self.policy_net = PacmanDQN(state_size, action_size).to(self.device)\n",
        "        self.target_net = PacmanDQN(state_size, action_size).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval() # la target_net non deve mai essere allenata, quindi è corretto tenerla sempre in .eval().\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
        "        self.criterion = nn.SmoothL1Loss() # CAMBIATA LA LOSS PRIMA ERA  nn.MSELoss()\n",
        "\n",
        "        # Replay buffer\n",
        "        self.memory = ReplayBuffer(buffer_size)\n",
        "\n",
        "        # Contatori\n",
        "        self.steps_done = 0\n",
        "\n",
        "        # altro che serve\n",
        "        self.last_scared_timer = None\n",
        "        self.ate_in_this_scared = False\n",
        "        self.power_streak = 0\n",
        "\n",
        "        # Mapping azioni (Berkeley framework)\n",
        "        self.action_to_idx = {'North': 0, 'East': 1, 'South': 2, 'West': 3}\n",
        "        self.idx_to_action = {v: k for k, v in self.action_to_idx.items()}\n",
        "\n",
        "        # Info layout (per normalizzazione)\n",
        "        self.initial_food_count = None\n",
        "        self.layout_width = None\n",
        "        self.layout_height = None\n",
        "\n",
        "    # ---- METODI COPIATI DA Q-LEARNING TABELLARE ----\n",
        "# codifica la direzione verso un target (food, capsule) in 4 valori binari (N, E, S, W)\n",
        "# otteniamo un'info orientata sul labirinto\n",
        "    def _get_direction_vector(self, from_pos, to_pos):\n",
        "        dx = to_pos[0] - from_pos[0]\n",
        "        dy = to_pos[1] - from_pos[1]\n",
        "\n",
        "        north = 1 if dy > 0 else 0\n",
        "        south = 1 if dy < 0 else 0\n",
        "        east = 1 if dx > 0 else 0\n",
        "        west = 1 if dx < 0 else 0\n",
        "\n",
        "        return (north, east, south, west)\n",
        "\n",
        "# evitare mosse suicide filtrando quelle troppo vicine ai fantasmi, è utile perché stabilizza l'apprendimento\n",
        "\n",
        "    def _get_safe_actions(self, state, legal_actions):\n",
        "        ghost_states = state.get_ghost_states()\n",
        "        safe = []\n",
        "\n",
        "        for action in legal_actions:\n",
        "            successor = state.generate_successor(0, action)\n",
        "            new_pos = successor.get_pacman_position()\n",
        "\n",
        "            is_safe = True\n",
        "            for ghost_state in ghost_states:\n",
        "                ghost_pos = ghost_state.configuration.pos  # estraiamo posizione\n",
        "\n",
        "                # Fantasmi spaventati non sono pericolosi\n",
        "                if ghost_state.scared_timer > 0:\n",
        "                    continue\n",
        "\n",
        "                # Evitiamo celle adiacenti a un fantasma attivo\n",
        "                if manhattan_distance(new_pos, ghost_pos) <= 1:\n",
        "                    is_safe = False\n",
        "                    break\n",
        "\n",
        "            if is_safe:\n",
        "                safe.append(action)\n",
        "\n",
        "        return safe\n",
        "\n",
        "    # ---- METODI DQN SPECIFICI ----\n",
        "# info sulla mappa, viene chiamato una volta per episodio\n",
        "# riporta informazioni statiche sul labirinto\n",
        "    def _init_layout_info(self, state):\n",
        "        if self.initial_food_count is None:\n",
        "            self.initial_food_count = state.get_food().count()\n",
        "\n",
        "            self.initial_capsule_count = len(state.get_capsules()) # capsule iniziali che il fantasma può magiare\n",
        "\n",
        "            food_list = state.get_food().as_list()\n",
        "            if food_list:\n",
        "                self.layout_width = max(f[0] for f in food_list) + 1\n",
        "                self.layout_height = max(f[1] for f in food_list) + 1\n",
        "            else:\n",
        "                self.layout_width = 20\n",
        "                self.layout_height = 20\n",
        "\n",
        "# quando non ci sono azioni sicure, entro un raggio di pericolo di fantasmi attivi\n",
        "    def _fallback_maximize_distance(self, state, legal_actions, active_ghosts):\n",
        "        if not active_ghosts:\n",
        "            return legal_actions\n",
        "# posizione corrente per ogni fantasma attivo\n",
        "        ghost_positions = [gs.configuration.pos for gs in active_ghosts]\n",
        "        best = []\n",
        "        best_dist = -1\n",
        "\n",
        "        for action in legal_actions:\n",
        "            successor = state.generate_successor(0, action)\n",
        "            new_pos = successor.get_pacman_position()\n",
        "\n",
        "            dmin = min(manhattan_distance(new_pos, gp) for gp in ghost_positions)\n",
        "\n",
        "            if dmin > best_dist:\n",
        "                best_dist = dmin\n",
        "                best = [action]\n",
        "            elif dmin == best_dist:\n",
        "                best.append(action)\n",
        "\n",
        "        return best\n",
        "\n",
        "    def get_action(self, state):\n",
        "        legal = state.get_legal_actions(0)\n",
        "        if not legal:\n",
        "            return None\n",
        "\n",
        "        safe_actions = self._get_safe_actions(state, legal)\n",
        "        # fallback: nessuna azione sicura\n",
        "        if not safe_actions:\n",
        "            safe_actions = legal\n",
        "\n",
        "         # 1) Epsilon-greedy\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(safe_actions)\n",
        "\n",
        "        # 2) Exploitation\n",
        "        # estraiamo il vettore feature\n",
        "        state_features = self.extract_features(state)\n",
        "        with torch.no_grad():\n",
        "            self.policy_net.eval()\n",
        "            state_tensor = torch.FloatTensor(state_features).unsqueeze(0).to(self.device)\n",
        "            q_values = self.policy_net(state_tensor).cpu().numpy()[0]\n",
        "            self.policy_net.train()\n",
        "\n",
        "        # 3) Scegliamo tra le sole safe actions il Q massimo\n",
        "        best_value = -1e9\n",
        "        best_actions = []\n",
        "        for action in safe_actions:\n",
        "            idx = self.action_to_idx.get(action, 0)\n",
        "            if idx < len(q_values):\n",
        "                val = q_values[idx]\n",
        "                if val > best_value:\n",
        "                    best_value = val\n",
        "                    best_actions = [action]\n",
        "                elif val == best_value:\n",
        "                    best_actions.append(action)\n",
        " # fallback extra: se per qualche motivo non c’è best\n",
        "        if best_actions:\n",
        "            return random.choice(best_actions)\n",
        "\n",
        "        return random.choice(safe_actions)\n",
        "\n",
        "# estrae le feature, le converte in tensori e le salva nel buffer\n",
        "    def store_transition(self, state, action, next_state, reward, done):\n",
        "      # entrambi gli stati vengono trasformati in vettori numerici\n",
        "      # il buffer conterrà dati già pre-processati\n",
        "        state_features = self.extract_features(state)\n",
        "        next_state_features = self.extract_features(next_state)\n",
        "        # l'azione viene convertita in un indice 0-3\n",
        "        action_idx = self.action_to_idx.get(action, 0)\n",
        "\n",
        "        # Clip reward per stabilità\n",
        "        reward = np.clip(reward, -100, 100)\n",
        "\n",
        "        # Convertiamo in tensori torch\n",
        "        state_tensor = torch.FloatTensor(state_features)\n",
        "        next_state_tensor = torch.FloatTensor(next_state_features)\n",
        "        # salviamo tutto nel buffer\n",
        "        self.memory.push(state_tensor, action_idx, next_state_tensor, reward, done)\n",
        "\n",
        "# semplicemente ci serve per l'ambiente di lavoro e lancia il trainstep()\n",
        "    def observeTransition(self, state, action, nextState, deltaReward):\n",
        "        done = nextState.is_win() or nextState.is_lose()\n",
        "        self.store_transition(state, action, nextState, deltaReward, done)\n",
        "\n",
        "        # Training step\n",
        "        return self.train_step()\n",
        "\n",
        "# estrae un batch dal replay buffer e applica il training step\n",
        "    def train_step(self):\n",
        "        # Il buffer deve essere già pien\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return None\n",
        "        # 1) Estraiamo un batch\n",
        "        state_batch, action_batch, next_state_batch, reward_batch, done_batch = self.memory.sample(self.batch_size)\n",
        "        # 2) Spostiamo tutto sul device (tutti tensori pytorch)\n",
        "        state_batch = state_batch.to(self.device)                   # stati correnti [B, state_size]\n",
        "        next_state_batch = next_state_batch.to(self.device)         # stati successivi [B, state_size]\n",
        "        action_batch = action_batch.to(self.device).unsqueeze(1)    # azioni fatte [B, 1]\n",
        "        reward_batch = reward_batch.to(self.device)                 # reward [B]\n",
        "        done_batch = done_batch.to(self.device)                     # segnala il fine episodio [B]\n",
        "\n",
        "        # 3) Q(s,a) corrente\n",
        "        # shape: [B, 4] -> gather -> [B,1]\n",
        "        current_q = self.policy_net(state_batch).gather(1, action_batch)\n",
        "        # 4) Double DQN: Q_next = r + gamma * Q_target(s', argmax_a Q_policy(s'))\n",
        "        with torch.no_grad():\n",
        "            # azione migliore secondo policy_net\n",
        "            next_q_policy = self.policy_net(next_state_batch)\n",
        "            best_actions = next_q_policy.argmax(1, keepdim=True)  # shape: [batch_size, 1]\n",
        "\n",
        "            # valutiamo quelle azioni con target_net\n",
        "            next_q_target = self.target_net(next_state_batch).gather(1, best_actions).squeeze(1)\n",
        "\n",
        "            # gamma * Q_target * (1 - done), (secondo bellman)\n",
        "            target_q = reward_batch + (1 - done_batch) * self.gamma * next_q_target\n",
        "\n",
        "\n",
        "        # 5) Loss Huber (SmoothL1)\n",
        "        loss = self.criterion(current_q.view(-1), target_q)\n",
        "\n",
        "        # 6) Backprop\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        #  clipping gradiente per stabilità\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # 7) Soft update della target_net\n",
        "        self.soft_update(tau=0.005)\n",
        "        self.steps_done += 1\n",
        "\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "# copia i pesi della policy nella rete target\n",
        "    def soft_update(self, tau=0.005):\n",
        "        for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
        "            target_param.data.copy_(tau * policy_param.data + (1.0 - tau) * target_param.data)\n",
        "# Aggiorniamo epsilon durante il training\n",
        "    def set_epsilon(self, eps):\n",
        "        self.epsilon = float(eps)\n",
        "\n",
        "# semplicemente per il tracciamento interno\n",
        "# chiamata di inizio episodio\n",
        "    def startEpisode(self):\n",
        "        self.episode_count += 1\n",
        "        self.last_scared_timer = None\n",
        "        self.ate_in_this_scared = False\n",
        "        self.power_streak = 0\n",
        "        if self.verbose and self.episode_count % 10 == 0:\n",
        "            print(f\"\\n=== EPISODE {self.episode_count} (DQN) ===\")\n",
        "# chiamata di fine episodio\n",
        "    def stopEpisode(self):\n",
        "        self.ate_in_this_scared = False\n",
        "        self.power_streak = 0\n",
        "        self.last_scared_timer = None\n",
        "        # il decadimento dell'epsilon qui non c'è perché è nel training\n",
        "        if self.verbose and self.episode_count % 10 == 0:\n",
        "            print(f\"Episode {self.episode_count} ended | \"\n",
        "                  f\"Buffer: {len(self.memory)} | \"\n",
        "                  f\"Epsilon: {self.epsilon:.3f}\")\n",
        "\n",
        "# salvano e ripristinano lo stato del modello\n",
        "    def save(self, filepath):\n",
        "        torch.save({\n",
        "            'policy_net': self.policy_net.state_dict(),\n",
        "            'target_net': self.target_net.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'epsilon': self.epsilon,\n",
        "            'episode_count': self.episode_count,\n",
        "            'steps_done': self.steps_done\n",
        "        }, filepath)\n",
        "\n",
        "    def load(self, filepath):\n",
        "        checkpoint = torch.load(filepath, map_location=self.device)\n",
        "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
        "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        self.epsilon = checkpoint['epsilon']\n",
        "        self.episode_count = checkpoint['episode_count']\n",
        "        self.steps_done = checkpoint['steps_done']"
      ],
      "metadata": {
        "id": "3x5VfDMpKlg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "di seguito la classe che ci permette di estrarre le feature -> trasforma lo stato di pacman (posizioni, fantasmi, cibo, capsule, muri) in un vettore numerico continuo di dimensione fissa (33) che la rete neurale può usare.\n",
        "<br> stiamo descrivendo ciò che è \"decision-relevant\" per pacman e ogni feature cerca di rappresentare la risposta alla domanda \"quale informazione aiuta a scegliere la prossima azione?\"\n",
        "\n",
        "all'inizio vengono inizializzate le dimensioni del layout e i conteggi iniziali (una sola volta). Subito dopo vengono estratti tutti i “pezzi grezzi” dello stato: posizione di Pacman, lista del cibo, stati dei fantasmi, azioni legali, capsule e muri, così da non dover richiamare continuamente lo state\n",
        "<br> le varie feature sono estratte in \"blocchi\"\n",
        "* **food (4 feature)** --> dice alla rete dove andare per fare progresso, le quattro feature direzionali (N, E, S, W) non indicano se puoi andare lì, ma se andare in quella direzione ti avvicina al cibo più vicino, mentre la quinta feature è la distanza normalizzata dal cibo più vicino.\n",
        "* **ghost primary threat (10 feature)**--> solo le feature dedicate alla presenza del fantasma nel labirinto, servono per mettere in atto una strategia difensiva; viene costruita una lista, ordinata per distanza, arricchita con distanza, posizione, stato scared. le prime 8 feature ci dicono per ogni direzione legale “se vado lì, mi avvicino a un fantasma?”. Inoltre vengono separati i fantasmi normali e scared, cosa cruciale per non confondere fuga e caccia. le ultime due feature descrivono situazioni del tipo \"sono vicino a un fantasma non scared e andando a est peggioro la situazione\".\n",
        "* **dead-end detection (2 feature)** --> riguarda la struttura del labirinto, non dinamica. insegna alla rete che determinate posizioni a prescindere dai fantasmi sono \"pericolose\".\n",
        "* **capsule info (5 feature)** --> la rete impara dove sono le capsule, quanto sono lontane e in che direzioni si trovano. È importante che capsule e food abbiano feature simili ma semantica diversa, perché poi il reward e il contesto decidono quando una capsula è desiderabile.\n",
        "* **power-up state (5 feature)** -->  queste feature descrivono non dove pacman si trova, ma in che fase del gioco si trova. stessa posizione + stessa geometria ≠ stessa decisione, se cambia lo stato dei fantasmi.\n",
        "    * scared_ratio --> quanto il gioco è \"sbilanciato\" a favore di pacman, vale 0 se nessun fantasma è scared, 1 se lo sono tutti (può assumere anche valori intermedi). serve per far capire alla rete che le regole del gioco sono cambiate e quindi lo sono anche le priorità\n",
        "    * scared_time_norm --> quanto durerà ancora il vantaggio, se restano molti step allora si può pensare ad una strategia offensiva altrimenti di difesa; viene normalizzato su 40 step\n",
        "    * hunt_opportunity --> c'è un fantasma scared e ci dice quanto è conveniente attarccarlo in quel momento e combina la distanza, stato scared e prossimità reale\n",
        "    * capsule_urgency --> \"prendere subito una capsula o rimandare?\" unendo la minaccia di un fantasma vicino non scarede e la raggiungibiità della capsula\n",
        "    * capsules_eaten_ratio --> memoria globale dello stato delle risorse\n",
        "* **global contex (6 feature)** --> abbiamo 6 feature che insieme, permettono alla rete di distinguere stati che geometricamente sembrano simili ma strategicamente sono opposti.\n",
        "    * food_eaten_ratio --> misura il progresso reale verso la vittoria serve a distinguere inizio partita da fine partita\n",
        "    * score_norm --> Il punteggio incorpora molte informazioni: capsule prese, fantasmi mangiati, tempo. Normalizzandolo, diamo alla rete una traccia di “come sto andando” senza doverle ricostruire tutto. È utile anche per stabilizzare le policy: evita che la rete faccia mosse suicide quando è già in netto vantaggio\n",
        "    * urgency --> quanti fantasmi attivi sono molto vicini; A differenza del ghost danger direzionale, qui non stiamo dicendo da dove arriva il pericolo, ma quanto è concentrato.\n",
        "    * safety --> quanto \"spazio di manovra\" ha pacman;\n",
        "    * pressure_norm --> quanti fantasmi sono entro un raggio di 5, normalizzato, è una misura di pressione ambientale più ampia rispetto a urgency (utile soprattutto quando si hanno più fantasmi)\n",
        "    * escape_difficulty --> è legata al numero di azioni legali disponibili. Un corridoio con una sola uscita è fragile anche se in questo istante non c’è pericolo. È una feature strutturale che migliora moltissimo la stabilità del comportamento.\n",
        "\n",
        "\n",
        "\n",
        "l'assert len(features) == 33 --> è un controllo logico: serve per evitare (nel caso peggiore in assoluto) di allenare un modello con feature sbagliate e senza accorgersene"
      ],
      "metadata": {
        "id": "634wFOt6k_ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ScalableFeatureExtractor:\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "\n",
        "    def extract_features(self, state):\n",
        "\n",
        "        self.agent._init_layout_info(state)\n",
        "        # prendiamo tutto quello che ci serve\n",
        "        pacman_pos = state.get_pacman_position()\n",
        "        food = state.get_food().as_list()\n",
        "        ghost_states = state.get_ghost_states()\n",
        "        ghosts = [g.get_position() for g in ghost_states]\n",
        "        legal = state.get_legal_actions(0)\n",
        "        capsules = state.get_capsules()\n",
        "        walls = state.get_walls()\n",
        "\n",
        "        features = [] # le feature verranno inserite in ordine fisso\n",
        "\n",
        "        # ---- FOOD FEATURES (5) ----\n",
        "        if food:\n",
        "          # prendere il cibo più vicino\n",
        "            closest_food = min(food, key=lambda f: manhattan_distance(pacman_pos, f))\n",
        "            food_dist = manhattan_distance(pacman_pos, closest_food)\n",
        "\n",
        "          #  (4) valori (N,E,S,W) che codificano la direzione verso il target.\n",
        "            features.extend(self.agent._get_direction_vector(pacman_pos, closest_food))\n",
        "\n",
        "            # Distanza normalizzata, clippata a 1 (1)\n",
        "            food_dist_norm = min(food_dist / (self.agent.layout_width + self.agent.layout_height), 1.0)\n",
        "            features.append(food_dist_norm)\n",
        "        else:\n",
        "            features.extend([0, 0, 0, 0, 1.0])\n",
        "\n",
        "        # ---- GHOST PRIMARY THREAT (10) ----\n",
        "\n",
        "        # Creiamo una lista ordinata di fantasmi per distanza\n",
        "        ghost_distances = []\n",
        "        for i, ghost_pos in enumerate(ghosts):\n",
        "          # distanza pacman-fantasma\n",
        "            dist = manhattan_distance(pacman_pos, ghost_pos)\n",
        "            # flag\n",
        "            is_scared = ghost_states[i].scared_timer > 0 if i < len(ghost_states) else False\n",
        "            ghost_distances.append((dist, ghost_pos, is_scared, i))\n",
        "        # ghost_distances[0] è il fantasma più vicino.\n",
        "        ghost_distances.sort(key=lambda x: x[0])  # Ordina per distanza\n",
        "\n",
        "        # inizializziamo danger vectors\n",
        "        ghost_danger_normal = [0.0, 0.0, 0.0, 0.0]  # N, E, S, W\n",
        "        ghost_danger_scared = [0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "        # Analizziamo PRIMARY ghost (più vicino)\n",
        "        if ghost_distances:\n",
        "            closest_dist, closest_pos, closest_scared, _ = ghost_distances[0]\n",
        "\n",
        "            # Calcoliamo danger per direzione (solo se vicino)\n",
        "            if closest_dist <= 4:\n",
        "                directions = ['North', 'East', 'South', 'West']\n",
        "                for j, dir_name in enumerate(directions):\n",
        "                    if dir_name not in legal:\n",
        "                        continue\n",
        "\n",
        "                    successor = state.generate_successor(0, dir_name)\n",
        "                    new_pos = successor.get_pacman_position()\n",
        "                    new_dist = manhattan_distance(new_pos, closest_pos)\n",
        "\n",
        "                    if new_dist < closest_dist:\n",
        "                        danger_score = 1.0 - (new_dist / 5.0) # cresce se pacman è vicino\n",
        "                        if closest_scared:\n",
        "                            ghost_danger_scared[j] = danger_score\n",
        "                        else:\n",
        "                            ghost_danger_normal[j] = danger_score\n",
        "\n",
        "            features.extend(ghost_danger_normal)  # (4)\n",
        "            features.extend(ghost_danger_scared)  # (4)\n",
        "\n",
        "            # Distanza normalizzata (1)\n",
        "            ghost_dist_norm = min(closest_dist / 10.0, 1.0)\n",
        "            features.append(ghost_dist_norm)\n",
        "\n",
        "            # Flag scared (1)\n",
        "            features.append(1.0 if closest_scared else 0.0)\n",
        "        else:\n",
        "            # Nessun fantasma (caso improbabile, non dovrebbe accadere)\n",
        "            features.extend([0.0] * 10)\n",
        "\n",
        "        # ---- DEAD-END DETECTION (2) ----\n",
        "        num_legal_moves = len([a for a in legal if a != 'Stop']) # quante mosse reali si hanno senza stop\n",
        "        is_dead_end = 1.0 if num_legal_moves == 1 else 0.0 # corridoio chiuso (1)\n",
        "        is_junction = 1.0 if num_legal_moves >= 3 else 0.0 # incrocio dove si hanno almeno tre scelte (1)\n",
        "        features.extend([is_dead_end, is_junction])\n",
        "\n",
        "        # ---- CAPSULE INFO (5) ----\n",
        "        if capsules:\n",
        "            closest_capsule = min(capsules, key=lambda c: manhattan_distance(pacman_pos, c))\n",
        "            capsule_dist = manhattan_distance(pacman_pos, closest_capsule)\n",
        "\n",
        "            # Direzione capsule (4)\n",
        "            features.extend(self.agent._get_direction_vector(pacman_pos, closest_capsule))\n",
        "\n",
        "            # Distanza normalizzata (1)\n",
        "            capsule_dist_norm = min(capsule_dist / (self.agent.layout_width + self.agent.layout_height), 1.0)\n",
        "            features.append(capsule_dist_norm)\n",
        "        else:\n",
        "            features.extend([0, 0, 0, 0, 1.0])\n",
        "\n",
        "        # ---- POWER-UP STATE (5) ----\n",
        "        # Quanti fantasmi sono scared?\n",
        "        num_scared = sum(1 for gs in ghost_states if gs.scared_timer > 0)\n",
        "        scared_ratio = num_scared / max(len(ghost_states), 1) # (1)\n",
        "        features.append(scared_ratio)\n",
        "\n",
        "        # Tempo rimanente (max tra tutti i fantasmi)\n",
        "        max_scared_timer = max((gs.scared_timer for gs in ghost_states), default=0)\n",
        "        scared_time_norm = max_scared_timer / 40.0 # (1)\n",
        "        features.append(scared_time_norm)\n",
        "\n",
        "        # \"Opportunità di caccia\" - scared ghost vicino?\n",
        "        hunt_opportunity = 0.0\n",
        "        for i, gs in enumerate(ghost_states):\n",
        "            if gs.scared_timer > 0:\n",
        "                ghost_pos = ghosts[i]\n",
        "                dist = manhattan_distance(pacman_pos, ghost_pos)\n",
        "                if dist <= 5:\n",
        "                    hunt_opportunity = max(hunt_opportunity, 1.0 - (dist / 5.0)) # (1)\n",
        "        features.append(hunt_opportunity)\n",
        "\n",
        "        # \"Urgenza capsule\" - fantasma vicino E capsule disponibile?\n",
        "        capsule_urgency = 0.0\n",
        "        if capsules and ghost_distances:\n",
        "            # Usiamo la distanza del fantasma normale più vicino\n",
        "            closest_normal = next((g for g in ghost_distances if not g[2]), None)\n",
        "            if closest_normal:\n",
        "                closest_dist_normal = closest_normal[0]\n",
        "                if closest_dist_normal <= 3:\n",
        "                    closest_capsule = min(capsules, key=lambda c: manhattan_distance(pacman_pos, c))\n",
        "                    capsule_dist = manhattan_distance(pacman_pos, closest_capsule)\n",
        "\n",
        "                    ghost_urgency = 1.0 - (closest_dist_normal / 3.0)\n",
        "                    capsule_reachability = 1.0 - min(capsule_dist / 10.0, 1.0)\n",
        "\n",
        "                    capsule_urgency = ghost_urgency * capsule_reachability # (1)\n",
        "        features.append(capsule_urgency)\n",
        "\n",
        "        # progresso capsule (quante sono già state consumate)\n",
        "        capsules_eaten_ratio = 1.0 - (len(capsules) / max(self.agent.initial_capsule_count, 1)) # (1)\n",
        "        features.append(capsules_eaten_ratio)\n",
        "\n",
        "        # ---- GLOBAL CONTEXT (6) ----\n",
        "        # progresso cibo (quanto cibo è stato mangiato)\n",
        "        food_eaten_ratio = 1.0 - (len(food) / max(self.agent.initial_food_count, 1)) # (1)\n",
        "        features.append(food_eaten_ratio)\n",
        "\n",
        "        # Score normalizzato\n",
        "        score_norm = np.clip(state.get_score() / 1000.0, -1.0, 1.0) # (1)\n",
        "        features.append(score_norm)\n",
        "\n",
        "        # Urgency (numero fantasmi attivi vicini)\n",
        "        ghosts_nearby = sum(1 for g in ghost_distances if g[0] <= 2 and not g[2])\n",
        "        urgency = min(ghosts_nearby / max(len(ghosts), 1), 1.0) # (1)\n",
        "        features.append(urgency)\n",
        "\n",
        "        # Safety (spazio libero da fantasmi attivi)\n",
        "        safe_neighbors = 0\n",
        "        # guardiamo le 4 celle adiacenti\n",
        "        for dx, dy in [(0,1), (1,0), (0,-1), (-1,0)]:\n",
        "            nx, ny = pacman_pos[0] + dx, pacman_pos[1] + dy\n",
        "            if 0 <= nx < self.agent.layout_width and 0 <= ny < self.agent.layout_height:\n",
        "                if not walls[nx][ny]:\n",
        "                    safe_from_all = True\n",
        "                    for dist, g_pos, is_scared, _ in ghost_distances:\n",
        "                        if not is_scared:  # Solo fantasmi attivi\n",
        "                            if manhattan_distance((nx, ny), g_pos) <= 2:\n",
        "                                safe_from_all = False\n",
        "                                break\n",
        "                    if safe_from_all:\n",
        "                        safe_neighbors += 1\n",
        "        safety = safe_neighbors / 4.0 # (1)\n",
        "        features.append(safety)\n",
        "\n",
        "        # quanti fantasmi entro raggio 5?\n",
        "        pressure = sum(1 for g in ghost_distances if g[0] <= 5)\n",
        "        pressure_norm = min(pressure / max(len(ghosts), 1), 1.0) # (1)\n",
        "        features.append(pressure_norm)\n",
        "\n",
        "        # poche mosse disponibili di \"fuga\"\n",
        "        escape_routes = num_legal_moves\n",
        "        escape_difficulty = 1.0 - (escape_routes / 4.0) # (1)\n",
        "        features.append(escape_difficulty)\n",
        "\n",
        "        assert len(features) == 33, f\"Feature count mismatch: {len(features)} != 33\"\n",
        "        return np.array(features, dtype=np.float32)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# INTEGRAZIONE CON DQNAgent\n",
        "# ==========================================\n",
        "class DQNAgentScalable(DQNAgent):\n",
        "\n",
        "    def __init__(self, state_size=33, **kwargs):\n",
        "        super().__init__(state_size=state_size, **kwargs)\n",
        "        self.feature_extractor = ScalableFeatureExtractor(self)\n",
        "\n",
        "    def extract_features(self, state):\n",
        "        return self.feature_extractor.extract_features(state)"
      ],
      "metadata": {
        "id": "feIF4Jn-KxxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nota da ricordarsi\n",
        "if getattr(self.agent, \"use_33\", False)"
      ],
      "metadata": {
        "id": "vpBeA4TFqOMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- FUNZIONE DI TRAINING ----\n",
        "# decay_episodes = numero di episodi in cui il valore di epsilon passa da epsilon start a end quindi man mano che si avvicina al quel numero, esplora sempre meno\n",
        "# di solito viene scelto così decay_episodes ≈ ⅓ – ½ di num_episodes\n",
        "\n",
        "def train_with_progress2(\n",
        "    agent,\n",
        "    num_episodes=100,\n",
        "    layout_name=\"small_classic\",\n",
        "    n_ghosts=1,\n",
        "    ghost_cls=DirectionalGhost,\n",
        "    epsilon_schedule=None,\n",
        "    seed=None,\n",
        "    print_every=10,\n",
        "    max_steps=1000,\n",
        "    use_powerup_rewards=False,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.05,\n",
        "    decay_episodes=300\n",
        "):\n",
        "\n",
        "    if seed is not None: # per la riproducibilità dei vari training\n",
        "        # Python standard\n",
        "        random.seed(seed)\n",
        "        # NumPy\n",
        "        np.random.seed(seed)\n",
        "        # Torch CPU\n",
        "        torch.manual_seed(seed)\n",
        "        # Torch GPU\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "    # set up ambiente\n",
        "    layout = get_layout(layout_name)\n",
        "    rules = ClassicGameRules(timeout=0)\n",
        "    # definizione delle variabili\n",
        "    wins = 0\n",
        "    deaths = 0\n",
        "    timeouts = 0\n",
        "\n",
        "    results = []\n",
        "    recent_wins = []\n",
        "    recent_scores = []\n",
        "    recent_steps = []\n",
        "    losses = deque(maxlen=1000) # teniamo solo le ultime loss\n",
        "# per ogni episodio\n",
        "    for ep in range(1, num_episodes + 1):\n",
        "        agent.startEpisode()\n",
        "        # reset interno dell'agente\n",
        "        agent.ate_in_this_scared = False\n",
        "        agent.power_streak = 0\n",
        "        ghosts_eaten_total = 0\n",
        "\n",
        "        if epsilon_schedule is not None:\n",
        "            agent.set_epsilon(epsilon_schedule(ep)) #strategia esterna\n",
        "        else: # decadimento lineare\n",
        "            if ep <= decay_episodes:\n",
        "                new_eps = epsilon_start - (epsilon_start - epsilon_end) * (ep - 1) / (decay_episodes - 1)\n",
        "            else:\n",
        "                new_eps = epsilon_end\n",
        "            agent.set_epsilon(new_eps)\n",
        "        # inizializziamo i fantasmi, un nuovo gioco berkeley e lo stato iniziale\n",
        "        ghosts = [ghost_cls(i+1) for i in range(n_ghosts)]\n",
        "        game = rules.new_game(layout, agent, ghosts, display=None, quiet=True, catch_exceptions=False)\n",
        "        state = game.state\n",
        "        steps = 0\n",
        "        capsules_wasted = 0\n",
        "\n",
        "        # loop temporale dell'episodio\n",
        "        while not (state.is_win() or state.is_lose()) and steps < max_steps:\n",
        "            steps += 1\n",
        "\n",
        "            # ---- Turno Pacman ----\n",
        "            # ci servono dopo per il calcolo del reward\n",
        "            old_score = state.get_score()\n",
        "            old_food_count = state.get_food().count()\n",
        "            old_caps = len(state.get_capsules())\n",
        "            old_ghost_states = state.get_ghost_states()\n",
        "            old_scared_count = sum(1 for gs in old_ghost_states if gs.scared_timer > 0)\n",
        "            old_scared_active = old_scared_count > 0\n",
        "\n",
        "            #azione di pacman\n",
        "            action = agent.get_action(state)\n",
        "            next_state = state.generate_successor(0, action)\n",
        "\n",
        "\n",
        "            # aggiornamento post azione\n",
        "            new_score = next_state.get_score()\n",
        "            new_food_count = next_state.get_food().count()\n",
        "\n",
        "            new_caps = len(next_state.get_capsules())\n",
        "            new_ghost_states = next_state.get_ghost_states()\n",
        "            new_scared_count = sum(1 for gs in new_ghost_states if gs.scared_timer > 0)\n",
        "            new_scared_active = new_scared_count > 0\n",
        "            # calcolo reward\n",
        "            reward = 0.0\n",
        "\n",
        "            # Base reward (delta score normalizzato)\n",
        "            score_delta = new_score - old_score\n",
        "            reward += score_delta / 10.0\n",
        "\n",
        "            if next_state.is_win():\n",
        "                reward += 25.0\n",
        "            elif next_state.is_lose():\n",
        "                reward -= 18.0\n",
        "            elif new_food_count < old_food_count:\n",
        "                reward += 2.0\n",
        "            elif steps > 500:\n",
        "                penalty_factor = 1 + (steps - 500) / 200.0\n",
        "                reward -= 0.005 * penalty_factor  # Cresce gradualmente, efficienza, non dove vagare a vuoto\n",
        "            else:\n",
        "\n",
        "                reward -= 0.01\n",
        "\n",
        "            if use_powerup_rewards: # se abbiamo i power up attivi\n",
        "                pac_pos = next_state.get_pacman_position()\n",
        "                ghosts_pos = [g.get_position() for g in new_ghost_states]\n",
        "                # Coefficienti\n",
        "                R_capsule_base  = +3.0\n",
        "                R_capsule_waste = -1.0\n",
        "                r_scared_step   = +0.20\n",
        "                R_ghost_base    = +8.0\n",
        "                R_waste_scared  = -2.0\n",
        "\n",
        "                # Capsule prese\n",
        "                if new_caps < old_caps:\n",
        "                    # Calcola urgenza\n",
        "                    non_scared_pos = [g.get_position() for g in old_ghost_states if g.scared_timer == 0]\n",
        "                    if not non_scared_pos:\n",
        "                        non_scared_pos = ghosts_pos  # Fallback\n",
        "\n",
        "                    if non_scared_pos:\n",
        "                        dmin = min(manhattan_distance(pac_pos, g) for g in non_scared_pos)\n",
        "                        urgenza = max(0.0, 1.0 - dmin / 4.0)\n",
        "                    else:\n",
        "                        urgenza = 0.0\n",
        "\n",
        "                    reward += R_capsule_base + 0.6 * urgenza  # Normalizzato\n",
        "\n",
        "                    # pacman viene penalizzato solo quando quell'azione di prendere la capsula non vede vicino di almeno 8 celle il fantasma\n",
        "                    max_threat_radius = 5 if layout_name == \"small_classic\" else 6 # perché small_classic è 7x7\n",
        "                    any_ghost_nearby = any(manhattan_distance(pac_pos, g) <= max_threat_radius for g in ghosts_pos)\n",
        "\n",
        "                    if not new_scared_active and not any_ghost_nearby:\n",
        "                        reward += R_capsule_waste * 0.5\n",
        "                        capsules_wasted += 1\n",
        "\n",
        "                # Piccolo bonus per ogni step con scared attivo\n",
        "                if new_scared_active:\n",
        "                    reward += r_scared_step\n",
        "\n",
        "                # Ghost eating\n",
        "                ghosts_eaten = old_scared_count - new_scared_count\n",
        "\n",
        "                # Fallback: se score salta molto ma conteggio non cambia\n",
        "                if ghosts_eaten == 0 and score_delta >= 190:\n",
        "                    if score_delta >= 390:\n",
        "                        ghosts_eaten = 2\n",
        "                    else:\n",
        "                        ghosts_eaten = 1\n",
        "\n",
        "                if ghosts_eaten > 0:\n",
        "                    agent.ate_in_this_scared = True\n",
        "                    ghosts_eaten_total += ghosts_eaten\n",
        "                    for _ in range(ghosts_eaten):\n",
        "                        combo_mult = 2 ** agent.power_streak\n",
        "                        reward += R_ghost_base * combo_mult #per aver mangiato i fantasmi\n",
        "                        agent.power_streak += 1\n",
        "\n",
        "            if next_state.is_win() or next_state.is_lose(): # vince o muore prima che i fantasmi si muovano\n",
        "                reward = np.clip(reward, -20.0, 20.0)\n",
        "                loss = agent.observeTransition(state, action, next_state, reward)\n",
        "                if loss is not None:\n",
        "                    losses.append(loss)\n",
        "                state = next_state\n",
        "                break\n",
        "\n",
        "            # ---- turno fantasmi ----\n",
        "            ghost_turn_state = next_state\n",
        "            for g_idx, ghost in enumerate(ghosts, start=1):\n",
        "                g_action = ghost.get_action(ghost_turn_state)\n",
        "                ghost_turn_state = ghost_turn_state.generate_successor(g_idx, g_action)\n",
        "                if ghost_turn_state.is_win() or ghost_turn_state.is_lose():\n",
        "                    break #se uccide pacman\n",
        "\n",
        "            # score dopo che  i fantasmi si sono mossi\n",
        "            post_score = ghost_turn_state.get_score()\n",
        "            reward += (post_score - new_score) / 10.0\n",
        "\n",
        "            # muore o vince a causa dei fantasmi\n",
        "            if ghost_turn_state.is_win():\n",
        "                reward += 25.0\n",
        "            elif ghost_turn_state.is_lose():\n",
        "                reward -= 18.0\n",
        "\n",
        "            # caso con i power up attivi\n",
        "            if use_powerup_rewards and not (ghost_turn_state.is_win() or ghost_turn_state.is_lose()):\n",
        "                post_ghost_states = ghost_turn_state.get_ghost_states()\n",
        "                now_scared_active = any(gs.scared_timer > 0 for gs in post_ghost_states) #c'è almeno un fantasma ancora scared?\n",
        "\n",
        "                if now_scared_active:\n",
        "                    agent.last_scared_timer = max(gs.scared_timer for gs in post_ghost_states)\n",
        "                else:\n",
        "                    if agent.last_scared_timer is not None and agent.last_scared_timer <= 2: # se si era in fase scared\n",
        "\n",
        "                        if not agent.ate_in_this_scared: # è stata \"sprecata\" l'occasione\n",
        "                            reward += (-2.0)\n",
        "                        # reset interno\n",
        "                        agent.ate_in_this_scared = False\n",
        "                        agent.power_streak = 0\n",
        "                    agent.last_scared_timer = None\n",
        "\n",
        "            # clip e salvataggio transizione\n",
        "            reward = np.clip(reward, -20.0, 20.0)\n",
        "            loss = agent.observeTransition(state, action, ghost_turn_state, reward)\n",
        "            if loss is not None:\n",
        "                losses.append(loss)\n",
        "\n",
        "            # prepariamo il prossimo ciclo\n",
        "            state = ghost_turn_state\n",
        "\n",
        "        # Statistiche finali episodio\n",
        "        final_score = state.get_score()\n",
        "        win = state.is_win()\n",
        "        lose = state.is_lose()\n",
        "\n",
        "\n",
        "        wins += 1 if win else 0\n",
        "        deaths += 1 if lose else 0\n",
        "        timeouts += 1 if (steps >= max_steps) and not (win or lose) else 0\n",
        "        win_rate = wins / ep\n",
        "        death_rate = deaths / ep\n",
        "\n",
        "        results.append({\n",
        "            \"episode\": ep,\n",
        "            \"score\": final_score,\n",
        "            \"win\": win,\n",
        "            \"lose\": lose,\n",
        "            \"steps\": steps,\n",
        "            \"epsilon\": agent.epsilon,\n",
        "            \"buffer_size\": len(agent.memory),\n",
        "            \"timeout\": timeouts,\n",
        "            \"ghosts_eaten\": ghosts_eaten_total,\n",
        "            \"capsules_wasted\": capsules_wasted,\n",
        "\n",
        "        })\n",
        "\n",
        "        recent_wins.append(1 if win else 0)\n",
        "        recent_scores.append(final_score)\n",
        "        recent_steps.append(steps)\n",
        "\n",
        "        # ultimi print_every risultati\n",
        "        if len(recent_wins) > print_every:\n",
        "            recent_wins.pop(0)\n",
        "            recent_scores.pop(0)\n",
        "            recent_steps.pop(0)\n",
        "\n",
        "        agent.stopEpisode()\n",
        "\n",
        "        # statistiche \"locali\"\n",
        "        if ep % print_every == 0:\n",
        "            local_win_rate = sum(recent_wins) / len(recent_wins)\n",
        "            avg_score = sum(recent_scores) / len(recent_scores)\n",
        "            avg_steps = sum(recent_steps) / len(recent_steps)\n",
        "\n",
        "            # Calcoliamo la media loss degli ultimi print_every episodi\n",
        "            losses_list = list(losses)\n",
        "            if len(losses_list) > 0:\n",
        "                recent_losses = losses_list[-print_every:] if len(losses_list) >= print_every else losses_list\n",
        "                avg_loss = sum(recent_losses) / len(recent_losses)\n",
        "            else:\n",
        "                avg_loss = 0.0\n",
        "\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"Episode {ep}/{num_episodes}\")\n",
        "            print(f\"  Win rate (ultimi {print_every}): {local_win_rate:.2%}\")\n",
        "            print(f\"  Win rate (globale): {win_rate:.2%}\")\n",
        "            print(f\"  Avg score: {avg_score:.1f} | Avg steps: {avg_steps:.1f}\")\n",
        "            print(f\"  Epsilon: {agent.epsilon:.3f}\")\n",
        "            print(f\"  avg loss: {avg_loss}\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "    # statistiche finali\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TRAINING COMPLETATO\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Episodi totali: {num_episodes}\")\n",
        "    print(f\"Vittorie: {wins} ({win_rate:.2%})\")\n",
        "    print(f\"Sconfitte: {deaths} ({death_rate:.2%})\")\n",
        "    print(f\"timeouts: {timeouts} \")\n",
        "    print(f\"Buffer finale: {len(agent.memory)}\")\n",
        "    print(f\"Epsilon finale: {agent.epsilon:.3f}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "rBgUEp0aK3Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_dqn_agent(agent, num_episodes=10, layout_name=\"small_classic\", n_ghosts=1,\n",
        "                   track_powerups=False, ghost_cls=RandomGhost):\n",
        "\n",
        "    # Salviamo e impostiamo epsilon a 0\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.set_epsilon(0.0) # niente esplorazione\n",
        "    agent.policy_net.eval() # no dropout etc\n",
        "\n",
        "    layout = get_layout(layout_name)\n",
        "    rules = ClassicGameRules(timeout=0)\n",
        "\n",
        "    wins = 0\n",
        "    total_score = 0\n",
        "    total_steps = 0\n",
        "    total_capsules_eaten = 0\n",
        "    total_ghosts_eaten = 0\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TESTING DQN AGENT (epsilon=0, greedy policy)\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    for ep in range(1, num_episodes + 1):\n",
        "      # inizializzazione episodio\n",
        "        ghosts = [ghost_cls(i + 1) for i in range(n_ghosts)]\n",
        "        game = rules.new_game(layout, agent, ghosts, display=None, quiet=True, catch_exceptions=False)\n",
        "        state = game.state\n",
        "\n",
        "        steps = 0\n",
        "        prev_capsules = len(state.get_capsules())\n",
        "        prev_score = state.get_score()\n",
        "        prev_scared_active = any(gs.scared_timer > 0 for gs in state.get_ghost_states())\n",
        "        ghosts_eaten_in_ep = 0\n",
        "\n",
        "        while not (state.is_win() or state.is_lose()) and steps < 1000:\n",
        "            steps += 1\n",
        "\n",
        "            # turno pacman\n",
        "            action = agent.get_action(state)\n",
        "            state = state.generate_successor(0, action)\n",
        "\n",
        "            if track_powerups:\n",
        "                curr_capsules = len(state.get_capsules())\n",
        "                if curr_capsules < prev_capsules:\n",
        "                    total_capsules_eaten += 1\n",
        "                prev_capsules = curr_capsules\n",
        "\n",
        "                new_score = state.get_score()\n",
        "                delta = new_score - prev_score\n",
        "                curr_scared_active = any(gs.scared_timer > 0 for gs in state.get_ghost_states())\n",
        "\n",
        "                # Contiamo i fantasmi mangiati\n",
        "                if delta >= 190:\n",
        "                    ghosts_in_step = 0\n",
        "                    if delta >= 390:\n",
        "                        ghosts_in_step = 2\n",
        "                    elif delta >= 190:\n",
        "                        ghosts_in_step = 1\n",
        "\n",
        "                    if ghosts_in_step > 0 and (curr_scared_active or prev_scared_active):\n",
        "                        ghosts_eaten_in_ep += ghosts_in_step\n",
        "                        total_ghosts_eaten += ghosts_in_step\n",
        "\n",
        "                prev_score = new_score\n",
        "                prev_scared_active = curr_scared_active\n",
        "\n",
        "            if state.is_win() or state.is_lose():\n",
        "                break\n",
        "\n",
        "            # turno fantasmi\n",
        "            for g_idx, ghost in enumerate(ghosts, start=1):\n",
        "                g_action = ghost.get_action(state)\n",
        "                state = state.generate_successor(g_idx, g_action)\n",
        "                if state.is_win() or state.is_lose():\n",
        "                    break\n",
        "\n",
        "        # Fine episodio\n",
        "        win = state.is_win()\n",
        "        score = state.get_score()\n",
        "        if win:\n",
        "            wins += 1\n",
        "        total_score += score\n",
        "        total_steps += steps\n",
        "\n",
        "        result = \"WIN\" if win else \"LOSE\"\n",
        "        print(f\"Test {ep}/{num_episodes}: {result} | Score: {score:.1f} | Steps: {steps} | \"\n",
        "              f\"Ghosts eaten: {ghosts_eaten_in_ep}\")\n",
        "\n",
        "    # statistiche varie\n",
        "    win_rate = wins / num_episodes\n",
        "    avg_score = total_score / num_episodes\n",
        "    avg_steps = total_steps / num_episodes\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TEST RESULTS\")\n",
        "    print(f\"  Win rate: {win_rate:.2%} ({wins}/{num_episodes})\")\n",
        "    print(f\"  Avg score: {avg_score:.1f}\")\n",
        "    print(f\"  Avg steps: {avg_steps:.1f}\")\n",
        "    if track_powerups:\n",
        "        print(f\"  Avg capsules eaten: {total_capsules_eaten / num_episodes:.2f}\")\n",
        "        print(f\"  Avg ghosts eaten (scared): {total_ghosts_eaten / num_episodes:.2f}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Ripristino stato agente\n",
        "    agent.set_epsilon(original_epsilon)\n",
        "    agent.policy_net.train()\n",
        "\n",
        "    if track_powerups:\n",
        "        return win_rate, avg_score, avg_steps, total_capsules_eaten / num_episodes, total_ghosts_eaten / num_episodes\n",
        "    else:\n",
        "        return win_rate, avg_score, avg_steps\n"
      ],
      "metadata": {
        "id": "Ww4ARjZR7WOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAINING E TEST: 1 random ghost, small_classic (no power up)**"
      ],
      "metadata": {
        "id": "BKQVzmCZwBez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FASE 1: impara i \"fondamentali\" (senza power-up)\n",
        "\n",
        "agent = DQNAgentScalable(\n",
        "    state_size=33,\n",
        "    action_size=4,\n",
        "    alpha=4e-5,\n",
        "    gamma=0.99,\n",
        "    epsilon=1.0,\n",
        "    epsilon_end=0.09,\n",
        "    buffer_size=30000,\n",
        "    batch_size=64,\n",
        "    tau=0.002,\n",
        "    verbose=True\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\nFASE 1: Training con 1 RandomGhost\")\n",
        "results_A = train_with_progress2(\n",
        "    agent,\n",
        "    num_episodes=90,\n",
        "    layout_name=\"small_classic\",\n",
        "    n_ghosts=1,\n",
        "    ghost_cls=RandomGhost,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.5,\n",
        "    decay_episodes=60,\n",
        "    use_powerup_rewards=False,\n",
        "    print_every=50,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"\\nFASE 1B: Training con 1 RandomGhost\")\n",
        "agent.epsilon = 0.9\n",
        "results_B = train_with_progress2(\n",
        "    agent,\n",
        "    num_episodes=150,\n",
        "    layout_name=\"small_classic\",\n",
        "    n_ghosts=1,\n",
        "    ghost_cls=RandomGhost,\n",
        "    epsilon_start=0.9,\n",
        "    epsilon_end=0.09,\n",
        "    decay_episodes=100,\n",
        "    use_powerup_rewards=False,\n",
        "    print_every=50,\n",
        "    seed=42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bm1oOL7ZvkPs",
        "outputId": "2a027c71-86a6-43a6-94a9-d740f5e35781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FASE 1: Training con 1 RandomGhost\n",
            "\n",
            "=== EPISODE 10 (DQN) ===\n",
            "Episode 10 ended | Buffer: 5255 | Epsilon: 0.924\n",
            "\n",
            "=== EPISODE 20 (DQN) ===\n",
            "Episode 20 ended | Buffer: 9966 | Epsilon: 0.839\n",
            "\n",
            "=== EPISODE 30 (DQN) ===\n",
            "Episode 30 ended | Buffer: 13408 | Epsilon: 0.754\n",
            "\n",
            "=== EPISODE 40 (DQN) ===\n",
            "Episode 40 ended | Buffer: 16357 | Epsilon: 0.669\n",
            "\n",
            "=== EPISODE 50 (DQN) ===\n",
            "Episode 50 ended | Buffer: 18895 | Epsilon: 0.585\n",
            "\n",
            "============================================================\n",
            "Episode 50/90\n",
            "  Win rate (ultimi 50): 66.00%\n",
            "  Win rate (globale): 66.00%\n",
            "  Avg score: 403.3 | Avg steps: 377.9\n",
            "  Epsilon: 0.585\n",
            "  avg loss: 0.6051684737205505\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 60 (DQN) ===\n",
            "Episode 60 ended | Buffer: 20764 | Epsilon: 0.500\n",
            "\n",
            "=== EPISODE 70 (DQN) ===\n",
            "Episode 70 ended | Buffer: 23341 | Epsilon: 0.500\n",
            "\n",
            "=== EPISODE 80 (DQN) ===\n",
            "Episode 80 ended | Buffer: 25102 | Epsilon: 0.500\n",
            "\n",
            "=== EPISODE 90 (DQN) ===\n",
            "Episode 90 ended | Buffer: 27473 | Epsilon: 0.500\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETATO\n",
            "============================================================\n",
            "Episodi totali: 90\n",
            "Vittorie: 66 (73.33%)\n",
            "Sconfitte: 24 (26.67%)\n",
            "timeouts: 0 \n",
            "Buffer finale: 27473\n",
            "Epsilon finale: 0.500\n",
            "============================================================\n",
            "\n",
            "\n",
            "FASE 1B: Training con 1 RandomGhost\n",
            "\n",
            "=== EPISODE 100 (DQN) ===\n",
            "Episode 100 ended | Buffer: 30000 | Epsilon: 0.826\n",
            "\n",
            "=== EPISODE 110 (DQN) ===\n",
            "Episode 110 ended | Buffer: 30000 | Epsilon: 0.745\n",
            "\n",
            "=== EPISODE 120 (DQN) ===\n",
            "Episode 120 ended | Buffer: 30000 | Epsilon: 0.663\n",
            "\n",
            "=== EPISODE 130 (DQN) ===\n",
            "Episode 130 ended | Buffer: 30000 | Epsilon: 0.581\n",
            "\n",
            "=== EPISODE 140 (DQN) ===\n",
            "Episode 140 ended | Buffer: 30000 | Epsilon: 0.499\n",
            "\n",
            "============================================================\n",
            "Episode 50/150\n",
            "  Win rate (ultimi 50): 76.00%\n",
            "  Win rate (globale): 76.00%\n",
            "  Avg score: 541.2 | Avg steps: 326.8\n",
            "  Epsilon: 0.499\n",
            "  avg loss: 1.5053089475631714\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 150 (DQN) ===\n",
            "Episode 150 ended | Buffer: 30000 | Epsilon: 0.417\n",
            "\n",
            "=== EPISODE 160 (DQN) ===\n",
            "Episode 160 ended | Buffer: 30000 | Epsilon: 0.335\n",
            "\n",
            "=== EPISODE 170 (DQN) ===\n",
            "Episode 170 ended | Buffer: 30000 | Epsilon: 0.254\n",
            "\n",
            "=== EPISODE 180 (DQN) ===\n",
            "Episode 180 ended | Buffer: 30000 | Epsilon: 0.172\n",
            "\n",
            "=== EPISODE 190 (DQN) ===\n",
            "Episode 190 ended | Buffer: 30000 | Epsilon: 0.090\n",
            "\n",
            "============================================================\n",
            "Episode 100/150\n",
            "  Win rate (ultimi 50): 84.00%\n",
            "  Win rate (globale): 80.00%\n",
            "  Avg score: 698.3 | Avg steps: 214.1\n",
            "  Epsilon: 0.090\n",
            "  avg loss: 2.099910979270935\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 200 (DQN) ===\n",
            "Episode 200 ended | Buffer: 30000 | Epsilon: 0.090\n",
            "\n",
            "=== EPISODE 210 (DQN) ===\n",
            "Episode 210 ended | Buffer: 30000 | Epsilon: 0.090\n",
            "\n",
            "=== EPISODE 220 (DQN) ===\n",
            "Episode 220 ended | Buffer: 30000 | Epsilon: 0.090\n",
            "\n",
            "=== EPISODE 230 (DQN) ===\n",
            "Episode 230 ended | Buffer: 30000 | Epsilon: 0.090\n",
            "\n",
            "=== EPISODE 240 (DQN) ===\n",
            "Episode 240 ended | Buffer: 30000 | Epsilon: 0.090\n",
            "\n",
            "============================================================\n",
            "Episode 150/150\n",
            "  Win rate (ultimi 50): 92.00%\n",
            "  Win rate (globale): 84.00%\n",
            "  Avg score: 840.8 | Avg steps: 157.4\n",
            "  Epsilon: 0.090\n",
            "  avg loss: 2.173208684921265\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETATO\n",
            "============================================================\n",
            "Episodi totali: 150\n",
            "Vittorie: 126 (84.00%)\n",
            "Sconfitte: 23 (15.33%)\n",
            "timeouts: 1 \n",
            "Buffer finale: 30000\n",
            "Epsilon finale: 0.090\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dqn_agent(agent, num_episodes=30, layout_name=\"small_classic\", n_ghosts=1, ghost_cls=RandomGhost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSm84Y-3MH0G",
        "outputId": "a0a803a7-b51a-4c51-c3a6-35b77386f8a5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING DQN AGENT (epsilon=0, greedy policy)\n",
            "============================================================\n",
            "\n",
            "Test 1/30: WIN | Score: 1312.0 | Steps: 138 | Ghosts eaten: 0\n",
            "Test 2/30: WIN | Score: 954.0 | Steps: 96 | Ghosts eaten: 0\n",
            "Test 3/30: WIN | Score: 910.0 | Steps: 140 | Ghosts eaten: 0\n",
            "Test 4/30: WIN | Score: 972.0 | Steps: 78 | Ghosts eaten: 0\n",
            "Test 5/30: WIN | Score: 1077.0 | Steps: 173 | Ghosts eaten: 0\n",
            "Test 6/30: WIN | Score: 953.0 | Steps: 97 | Ghosts eaten: 0\n",
            "Test 7/30: LOSE | Score: -355.0 | Steps: 275 | Ghosts eaten: 0\n",
            "Test 8/30: WIN | Score: 963.0 | Steps: 87 | Ghosts eaten: 0\n",
            "Test 9/30: WIN | Score: 1061.0 | Steps: 189 | Ghosts eaten: 0\n",
            "Test 10/30: WIN | Score: 868.0 | Steps: 182 | Ghosts eaten: 0\n",
            "Test 11/30: WIN | Score: 964.0 | Steps: 86 | Ghosts eaten: 0\n",
            "Test 12/30: WIN | Score: 653.0 | Steps: 397 | Ghosts eaten: 0\n",
            "Test 13/30: WIN | Score: 936.0 | Steps: 114 | Ghosts eaten: 0\n",
            "Test 14/30: WIN | Score: 963.0 | Steps: 87 | Ghosts eaten: 0\n",
            "Test 15/30: WIN | Score: 967.0 | Steps: 83 | Ghosts eaten: 0\n",
            "Test 16/30: WIN | Score: 1152.0 | Steps: 98 | Ghosts eaten: 0\n",
            "Test 17/30: WIN | Score: 934.0 | Steps: 116 | Ghosts eaten: 0\n",
            "Test 18/30: WIN | Score: 1077.0 | Steps: 173 | Ghosts eaten: 0\n",
            "Test 19/30: WIN | Score: 914.0 | Steps: 336 | Ghosts eaten: 0\n",
            "Test 20/30: WIN | Score: 769.0 | Steps: 481 | Ghosts eaten: 0\n",
            "Test 21/30: WIN | Score: 922.0 | Steps: 128 | Ghosts eaten: 0\n",
            "Test 22/30: WIN | Score: 955.0 | Steps: 95 | Ghosts eaten: 0\n",
            "Test 23/30: WIN | Score: 874.0 | Steps: 176 | Ghosts eaten: 0\n",
            "Test 24/30: WIN | Score: 953.0 | Steps: 97 | Ghosts eaten: 0\n",
            "Test 25/30: WIN | Score: 913.0 | Steps: 137 | Ghosts eaten: 0\n",
            "Test 26/30: WIN | Score: 970.0 | Steps: 280 | Ghosts eaten: 0\n",
            "Test 27/30: WIN | Score: 954.0 | Steps: 96 | Ghosts eaten: 0\n",
            "Test 28/30: WIN | Score: 817.0 | Steps: 233 | Ghosts eaten: 0\n",
            "Test 29/30: LOSE | Score: -557.0 | Steps: 277 | Ghosts eaten: 0\n",
            "Test 30/30: WIN | Score: 958.0 | Steps: 92 | Ghosts eaten: 0\n",
            "\n",
            "============================================================\n",
            "TEST RESULTS\n",
            "  Win rate: 93.33% (28/30)\n",
            "  Avg score: 860.1\n",
            "  Avg steps: 167.9\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9333333333333333, 860.1, 167.9)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAINING E TEST: 1 directional ghost, small_classic (no power up)**"
      ],
      "metadata": {
        "id": "nVbyZrrQwOaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFASE 2A: Training con 1 DirectionalGhost\")\n",
        "# manteniamo una % del buffer\n",
        "n_keep = int(len(agent.memory.buffer) * 0.2)\n",
        "agent.memory.buffer = deque(list(agent.memory.buffer)[-n_keep:], maxlen=50000)\n",
        "# reset epsilon + tau leggermente più alto\n",
        "agent.epsilon = 1.0\n",
        "agent.tau = 0.004  # più aggressivo per DirectionalGhost\n",
        "# Riduciamo il  learning rate\n",
        "agent.learning_rate = 3e-5\n",
        "agent.optimizer = optim.Adam(agent.policy_net.parameters(), lr=3e-5)\n",
        "agent.episode_count = 0\n",
        "\n",
        "results_A = train_with_progress2(\n",
        "    agent,\n",
        "    num_episodes=150,\n",
        "    layout_name=\"small_classic\",\n",
        "    n_ghosts=1,\n",
        "    ghost_cls=DirectionalGhost,\n",
        "    epsilon_start=1.0,           #\n",
        "    epsilon_end=0.5,\n",
        "    decay_episodes=90,\n",
        "    use_powerup_rewards=False,\n",
        "    print_every=50,\n",
        "    max_steps=950,\n",
        "    seed = 42\n",
        ")\n",
        "\n",
        "agent.tau = 0.004\n",
        "agent.epsilon = 0.7\n",
        "results_B = train_with_progress2(\n",
        "    agent,\n",
        "    num_episodes=300,\n",
        "    layout_name=\"small_classic\",\n",
        "    n_ghosts=1,\n",
        "    ghost_cls=DirectionalGhost,\n",
        "    epsilon_start=0.7,\n",
        "    epsilon_end=0.05,\n",
        "    decay_episodes=300,\n",
        "    use_powerup_rewards=False,\n",
        "    print_every=50,\n",
        "    max_steps=950,\n",
        "    seed = 42\n",
        ")\n",
        "\n",
        "agent.tau = 0.003\n",
        "agent.epsilon = 0.3\n",
        "results_C = train_with_progress2(\n",
        "    agent,\n",
        "    num_episodes=250,\n",
        "    layout_name=\"small_classic\",\n",
        "    n_ghosts=1,\n",
        "    ghost_cls=DirectionalGhost,\n",
        "    epsilon_start=0.3,\n",
        "    epsilon_end=0.02,\n",
        "    decay_episodes=150,\n",
        "    use_powerup_rewards=False,\n",
        "    print_every=50,\n",
        "    seed = 42\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l4XU101byt8",
        "outputId": "1cf35279-9a6e-42e1-ba11-5984c78eb6c3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FASE 2A: Training con 1 DirectionalGhost\n",
            "\n",
            "=== EPISODE 10 (DQN) ===\n",
            "Episode 10 ended | Buffer: 7482 | Epsilon: 0.949\n",
            "\n",
            "=== EPISODE 20 (DQN) ===\n",
            "Episode 20 ended | Buffer: 8898 | Epsilon: 0.893\n",
            "\n",
            "=== EPISODE 30 (DQN) ===\n",
            "Episode 30 ended | Buffer: 10369 | Epsilon: 0.837\n",
            "\n",
            "=== EPISODE 40 (DQN) ===\n",
            "Episode 40 ended | Buffer: 11982 | Epsilon: 0.781\n",
            "\n",
            "=== EPISODE 50 (DQN) ===\n",
            "Episode 50 ended | Buffer: 13335 | Epsilon: 0.725\n",
            "\n",
            "============================================================\n",
            "Episode 50/150\n",
            "  Win rate (ultimi 50): 12.00%\n",
            "  Win rate (globale): 12.00%\n",
            "  Avg score: -59.5 | Avg steps: 146.7\n",
            "  Epsilon: 0.725\n",
            "  avg loss: 2.4036529588699342\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 60 (DQN) ===\n",
            "Episode 60 ended | Buffer: 14417 | Epsilon: 0.669\n",
            "\n",
            "=== EPISODE 70 (DQN) ===\n",
            "Episode 70 ended | Buffer: 15658 | Epsilon: 0.612\n",
            "\n",
            "=== EPISODE 80 (DQN) ===\n",
            "Episode 80 ended | Buffer: 17097 | Epsilon: 0.556\n",
            "\n",
            "=== EPISODE 90 (DQN) ===\n",
            "Episode 90 ended | Buffer: 18349 | Epsilon: 0.500\n",
            "\n",
            "=== EPISODE 100 (DQN) ===\n",
            "Episode 100 ended | Buffer: 19627 | Epsilon: 0.500\n",
            "\n",
            "============================================================\n",
            "Episode 100/150\n",
            "  Win rate (ultimi 50): 36.00%\n",
            "  Win rate (globale): 24.00%\n",
            "  Avg score: 280.4 | Avg steps: 125.8\n",
            "  Epsilon: 0.500\n",
            "  avg loss: 2.620048611164093\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 110 (DQN) ===\n",
            "Episode 110 ended | Buffer: 20880 | Epsilon: 0.500\n",
            "\n",
            "=== EPISODE 120 (DQN) ===\n",
            "Episode 120 ended | Buffer: 22158 | Epsilon: 0.500\n",
            "\n",
            "=== EPISODE 130 (DQN) ===\n",
            "Episode 130 ended | Buffer: 23298 | Epsilon: 0.500\n",
            "\n",
            "=== EPISODE 140 (DQN) ===\n",
            "Episode 140 ended | Buffer: 24714 | Epsilon: 0.500\n",
            "\n",
            "=== EPISODE 150 (DQN) ===\n",
            "Episode 150 ended | Buffer: 26144 | Epsilon: 0.500\n",
            "\n",
            "============================================================\n",
            "Episode 150/150\n",
            "  Win rate (ultimi 50): 50.00%\n",
            "  Win rate (globale): 32.67%\n",
            "  Avg score: 436.1 | Avg steps: 130.3\n",
            "  Epsilon: 0.500\n",
            "  avg loss: 2.957657949924469\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETATO\n",
            "============================================================\n",
            "Episodi totali: 150\n",
            "Vittorie: 49 (32.67%)\n",
            "Sconfitte: 101 (67.33%)\n",
            "timeouts: 0 \n",
            "Buffer finale: 26144\n",
            "Epsilon finale: 0.500\n",
            "============================================================\n",
            "\n",
            "\n",
            "=== EPISODE 160 (DQN) ===\n",
            "Episode 160 ended | Buffer: 27787 | Epsilon: 0.680\n",
            "\n",
            "=== EPISODE 170 (DQN) ===\n",
            "Episode 170 ended | Buffer: 29227 | Epsilon: 0.659\n",
            "\n",
            "=== EPISODE 180 (DQN) ===\n",
            "Episode 180 ended | Buffer: 30578 | Epsilon: 0.637\n",
            "\n",
            "=== EPISODE 190 (DQN) ===\n",
            "Episode 190 ended | Buffer: 31919 | Epsilon: 0.615\n",
            "\n",
            "=== EPISODE 200 (DQN) ===\n",
            "Episode 200 ended | Buffer: 33403 | Epsilon: 0.593\n",
            "\n",
            "============================================================\n",
            "Episode 50/300\n",
            "  Win rate (ultimi 50): 30.00%\n",
            "  Win rate (globale): 30.00%\n",
            "  Avg score: 206.8 | Avg steps: 145.2\n",
            "  Epsilon: 0.593\n",
            "  avg loss: 2.5546925401687623\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 210 (DQN) ===\n",
            "Episode 210 ended | Buffer: 34834 | Epsilon: 0.572\n",
            "\n",
            "=== EPISODE 220 (DQN) ===\n",
            "Episode 220 ended | Buffer: 36727 | Epsilon: 0.550\n",
            "\n",
            "=== EPISODE 230 (DQN) ===\n",
            "Episode 230 ended | Buffer: 37980 | Epsilon: 0.528\n",
            "\n",
            "=== EPISODE 240 (DQN) ===\n",
            "Episode 240 ended | Buffer: 39451 | Epsilon: 0.507\n",
            "\n",
            "=== EPISODE 250 (DQN) ===\n",
            "Episode 250 ended | Buffer: 40866 | Epsilon: 0.485\n",
            "\n",
            "============================================================\n",
            "Episode 100/300\n",
            "  Win rate (ultimi 50): 44.00%\n",
            "  Win rate (globale): 37.00%\n",
            "  Avg score: 385.7 | Avg steps: 149.3\n",
            "  Epsilon: 0.485\n",
            "  avg loss: 2.96026154756546\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 260 (DQN) ===\n",
            "Episode 260 ended | Buffer: 42344 | Epsilon: 0.463\n",
            "\n",
            "=== EPISODE 270 (DQN) ===\n",
            "Episode 270 ended | Buffer: 43905 | Epsilon: 0.441\n",
            "\n",
            "=== EPISODE 280 (DQN) ===\n",
            "Episode 280 ended | Buffer: 45107 | Epsilon: 0.420\n",
            "\n",
            "=== EPISODE 290 (DQN) ===\n",
            "Episode 290 ended | Buffer: 46321 | Epsilon: 0.398\n",
            "\n",
            "=== EPISODE 300 (DQN) ===\n",
            "Episode 300 ended | Buffer: 47745 | Epsilon: 0.376\n",
            "\n",
            "============================================================\n",
            "Episode 150/300\n",
            "  Win rate (ultimi 50): 38.00%\n",
            "  Win rate (globale): 37.33%\n",
            "  Avg score: 361.8 | Avg steps: 137.6\n",
            "  Epsilon: 0.376\n",
            "  avg loss: 2.4415461087226866\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 310 (DQN) ===\n",
            "Episode 310 ended | Buffer: 49158 | Epsilon: 0.354\n",
            "\n",
            "=== EPISODE 320 (DQN) ===\n",
            "Episode 320 ended | Buffer: 50000 | Epsilon: 0.333\n",
            "\n",
            "=== EPISODE 330 (DQN) ===\n",
            "Episode 330 ended | Buffer: 50000 | Epsilon: 0.311\n",
            "\n",
            "=== EPISODE 340 (DQN) ===\n",
            "Episode 340 ended | Buffer: 50000 | Epsilon: 0.289\n",
            "\n",
            "=== EPISODE 350 (DQN) ===\n",
            "Episode 350 ended | Buffer: 50000 | Epsilon: 0.267\n",
            "\n",
            "============================================================\n",
            "Episode 200/300\n",
            "  Win rate (ultimi 50): 42.00%\n",
            "  Win rate (globale): 38.50%\n",
            "  Avg score: 441.8 | Avg steps: 136.8\n",
            "  Epsilon: 0.267\n",
            "  avg loss: 2.074168210029602\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 360 (DQN) ===\n",
            "Episode 360 ended | Buffer: 50000 | Epsilon: 0.246\n",
            "\n",
            "=== EPISODE 370 (DQN) ===\n",
            "Episode 370 ended | Buffer: 50000 | Epsilon: 0.224\n",
            "\n",
            "=== EPISODE 380 (DQN) ===\n",
            "Episode 380 ended | Buffer: 50000 | Epsilon: 0.202\n",
            "\n",
            "=== EPISODE 390 (DQN) ===\n",
            "Episode 390 ended | Buffer: 50000 | Epsilon: 0.180\n",
            "\n",
            "=== EPISODE 400 (DQN) ===\n",
            "Episode 400 ended | Buffer: 50000 | Epsilon: 0.159\n",
            "\n",
            "============================================================\n",
            "Episode 250/300\n",
            "  Win rate (ultimi 50): 48.00%\n",
            "  Win rate (globale): 40.40%\n",
            "  Avg score: 477.3 | Avg steps: 126.9\n",
            "  Epsilon: 0.159\n",
            "  avg loss: 1.7603315269947053\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 410 (DQN) ===\n",
            "Episode 410 ended | Buffer: 50000 | Epsilon: 0.137\n",
            "\n",
            "=== EPISODE 420 (DQN) ===\n",
            "Episode 420 ended | Buffer: 50000 | Epsilon: 0.115\n",
            "\n",
            "=== EPISODE 430 (DQN) ===\n",
            "Episode 430 ended | Buffer: 50000 | Epsilon: 0.093\n",
            "\n",
            "=== EPISODE 440 (DQN) ===\n",
            "Episode 440 ended | Buffer: 50000 | Epsilon: 0.072\n",
            "\n",
            "=== EPISODE 450 (DQN) ===\n",
            "Episode 450 ended | Buffer: 50000 | Epsilon: 0.050\n",
            "\n",
            "============================================================\n",
            "Episode 300/300\n",
            "  Win rate (ultimi 50): 58.00%\n",
            "  Win rate (globale): 43.33%\n",
            "  Avg score: 664.9 | Avg steps: 137.7\n",
            "  Epsilon: 0.050\n",
            "  avg loss: 1.8139008235931398\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETATO\n",
            "============================================================\n",
            "Episodi totali: 300\n",
            "Vittorie: 130 (43.33%)\n",
            "Sconfitte: 170 (56.67%)\n",
            "timeouts: 0 \n",
            "Buffer finale: 50000\n",
            "Epsilon finale: 0.050\n",
            "============================================================\n",
            "\n",
            "\n",
            "=== EPISODE 460 (DQN) ===\n",
            "Episode 460 ended | Buffer: 50000 | Epsilon: 0.283\n",
            "\n",
            "=== EPISODE 470 (DQN) ===\n",
            "Episode 470 ended | Buffer: 50000 | Epsilon: 0.264\n",
            "\n",
            "=== EPISODE 480 (DQN) ===\n",
            "Episode 480 ended | Buffer: 50000 | Epsilon: 0.246\n",
            "\n",
            "=== EPISODE 490 (DQN) ===\n",
            "Episode 490 ended | Buffer: 50000 | Epsilon: 0.227\n",
            "\n",
            "=== EPISODE 500 (DQN) ===\n",
            "Episode 500 ended | Buffer: 50000 | Epsilon: 0.208\n",
            "\n",
            "============================================================\n",
            "Episode 50/250\n",
            "  Win rate (ultimi 50): 70.00%\n",
            "  Win rate (globale): 70.00%\n",
            "  Avg score: 751.8 | Avg steps: 137.4\n",
            "  Epsilon: 0.208\n",
            "  avg loss: 1.9923781168460846\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 510 (DQN) ===\n",
            "Episode 510 ended | Buffer: 50000 | Epsilon: 0.189\n",
            "\n",
            "=== EPISODE 520 (DQN) ===\n",
            "Episode 520 ended | Buffer: 50000 | Epsilon: 0.170\n",
            "\n",
            "=== EPISODE 530 (DQN) ===\n",
            "Episode 530 ended | Buffer: 50000 | Epsilon: 0.152\n",
            "\n",
            "=== EPISODE 540 (DQN) ===\n",
            "Episode 540 ended | Buffer: 50000 | Epsilon: 0.133\n",
            "\n",
            "=== EPISODE 550 (DQN) ===\n",
            "Episode 550 ended | Buffer: 50000 | Epsilon: 0.114\n",
            "\n",
            "============================================================\n",
            "Episode 100/250\n",
            "  Win rate (ultimi 50): 76.00%\n",
            "  Win rate (globale): 73.00%\n",
            "  Avg score: 837.1 | Avg steps: 129.1\n",
            "  Epsilon: 0.114\n",
            "  avg loss: 1.7226436984539033\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 560 (DQN) ===\n",
            "Episode 560 ended | Buffer: 50000 | Epsilon: 0.095\n",
            "\n",
            "=== EPISODE 570 (DQN) ===\n",
            "Episode 570 ended | Buffer: 50000 | Epsilon: 0.076\n",
            "\n",
            "=== EPISODE 580 (DQN) ===\n",
            "Episode 580 ended | Buffer: 50000 | Epsilon: 0.058\n",
            "\n",
            "=== EPISODE 590 (DQN) ===\n",
            "Episode 590 ended | Buffer: 50000 | Epsilon: 0.039\n",
            "\n",
            "=== EPISODE 600 (DQN) ===\n",
            "Episode 600 ended | Buffer: 50000 | Epsilon: 0.020\n",
            "\n",
            "============================================================\n",
            "Episode 150/250\n",
            "  Win rate (ultimi 50): 90.00%\n",
            "  Win rate (globale): 78.67%\n",
            "  Avg score: 977.2 | Avg steps: 120.8\n",
            "  Epsilon: 0.020\n",
            "  avg loss: 1.7620291900634766\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 610 (DQN) ===\n",
            "Episode 610 ended | Buffer: 50000 | Epsilon: 0.020\n",
            "\n",
            "=== EPISODE 620 (DQN) ===\n",
            "Episode 620 ended | Buffer: 50000 | Epsilon: 0.020\n",
            "\n",
            "=== EPISODE 630 (DQN) ===\n",
            "Episode 630 ended | Buffer: 50000 | Epsilon: 0.020\n",
            "\n",
            "=== EPISODE 640 (DQN) ===\n",
            "Episode 640 ended | Buffer: 50000 | Epsilon: 0.020\n",
            "\n",
            "=== EPISODE 650 (DQN) ===\n",
            "Episode 650 ended | Buffer: 50000 | Epsilon: 0.020\n",
            "\n",
            "============================================================\n",
            "Episode 200/250\n",
            "  Win rate (ultimi 50): 94.00%\n",
            "  Win rate (globale): 82.50%\n",
            "  Avg score: 1011.2 | Avg steps: 125.2\n",
            "  Epsilon: 0.020\n",
            "  avg loss: 1.5527626061439515\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 660 (DQN) ===\n",
            "Episode 660 ended | Buffer: 50000 | Epsilon: 0.020\n",
            "\n",
            "=== EPISODE 670 (DQN) ===\n",
            "Episode 670 ended | Buffer: 50000 | Epsilon: 0.020\n",
            "\n",
            "=== EPISODE 680 (DQN) ===\n",
            "Episode 680 ended | Buffer: 50000 | Epsilon: 0.020\n",
            "\n",
            "=== EPISODE 690 (DQN) ===\n",
            "Episode 690 ended | Buffer: 50000 | Epsilon: 0.020\n",
            "\n",
            "=== EPISODE 700 (DQN) ===\n",
            "Episode 700 ended | Buffer: 50000 | Epsilon: 0.020\n",
            "\n",
            "============================================================\n",
            "Episode 250/250\n",
            "  Win rate (ultimi 50): 94.00%\n",
            "  Win rate (globale): 84.80%\n",
            "  Avg score: 1042.4 | Avg steps: 131.2\n",
            "  Epsilon: 0.020\n",
            "  avg loss: 1.4920242273807525\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETATO\n",
            "============================================================\n",
            "Episodi totali: 250\n",
            "Vittorie: 212 (84.80%)\n",
            "Sconfitte: 38 (15.20%)\n",
            "timeouts: 0 \n",
            "Buffer finale: 50000\n",
            "Epsilon finale: 0.020\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dqn_agent(agent, num_episodes=50, layout_name=\"small_classic\", n_ghosts=1, ghost_cls=DirectionalGhost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwo6_EJbeu4e",
        "outputId": "e3993e3d-8116-48fc-e5ae-266b7d84f87d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING DQN AGENT (epsilon=0, greedy policy)\n",
            "============================================================\n",
            "\n",
            "Test 1/50: LOSE | Score: -39.0 | Steps: 159 | Ghosts eaten: 0\n",
            "Test 2/50: WIN | Score: 1173.0 | Steps: 77 | Ghosts eaten: 0\n",
            "Test 3/50: LOSE | Score: 19.0 | Steps: 61 | Ghosts eaten: 0\n",
            "Test 4/50: WIN | Score: 1365.0 | Steps: 85 | Ghosts eaten: 0\n",
            "Test 5/50: WIN | Score: 1296.0 | Steps: 154 | Ghosts eaten: 0\n",
            "Test 6/50: WIN | Score: 789.0 | Steps: 261 | Ghosts eaten: 0\n",
            "Test 7/50: WIN | Score: 1337.0 | Steps: 113 | Ghosts eaten: 0\n",
            "Test 8/50: WIN | Score: 1053.0 | Steps: 197 | Ghosts eaten: 0\n",
            "Test 9/50: WIN | Score: 1173.0 | Steps: 77 | Ghosts eaten: 0\n",
            "Test 10/50: WIN | Score: 1327.0 | Steps: 123 | Ghosts eaten: 0\n",
            "Test 11/50: WIN | Score: 937.0 | Steps: 113 | Ghosts eaten: 0\n",
            "Test 12/50: WIN | Score: 1142.0 | Steps: 108 | Ghosts eaten: 0\n",
            "Test 13/50: WIN | Score: 915.0 | Steps: 135 | Ghosts eaten: 0\n",
            "Test 14/50: WIN | Score: 931.0 | Steps: 119 | Ghosts eaten: 0\n",
            "Test 15/50: WIN | Score: 1070.0 | Steps: 180 | Ghosts eaten: 0\n",
            "Test 16/50: WIN | Score: 863.0 | Steps: 387 | Ghosts eaten: 0\n",
            "Test 17/50: WIN | Score: 1377.0 | Steps: 73 | Ghosts eaten: 0\n",
            "Test 18/50: WIN | Score: 1037.0 | Steps: 213 | Ghosts eaten: 0\n",
            "Test 19/50: WIN | Score: 965.0 | Steps: 85 | Ghosts eaten: 0\n",
            "Test 20/50: WIN | Score: 1099.0 | Steps: 151 | Ghosts eaten: 0\n",
            "Test 21/50: WIN | Score: 1165.0 | Steps: 85 | Ghosts eaten: 0\n",
            "Test 22/50: WIN | Score: 1165.0 | Steps: 85 | Ghosts eaten: 0\n",
            "Test 23/50: WIN | Score: 1150.0 | Steps: 100 | Ghosts eaten: 0\n",
            "Test 24/50: WIN | Score: 910.0 | Steps: 140 | Ghosts eaten: 0\n",
            "Test 25/50: WIN | Score: 1341.0 | Steps: 109 | Ghosts eaten: 0\n",
            "Test 26/50: WIN | Score: 1147.0 | Steps: 103 | Ghosts eaten: 0\n",
            "Test 27/50: LOSE | Score: -5.0 | Steps: 85 | Ghosts eaten: 0\n",
            "Test 28/50: WIN | Score: 1115.0 | Steps: 135 | Ghosts eaten: 0\n",
            "Test 29/50: WIN | Score: 1310.0 | Steps: 140 | Ghosts eaten: 0\n",
            "Test 30/50: WIN | Score: 1135.0 | Steps: 115 | Ghosts eaten: 0\n",
            "Test 31/50: WIN | Score: 1121.0 | Steps: 129 | Ghosts eaten: 0\n",
            "Test 32/50: WIN | Score: 1289.0 | Steps: 161 | Ghosts eaten: 0\n",
            "Test 33/50: WIN | Score: 1309.0 | Steps: 141 | Ghosts eaten: 0\n",
            "Test 34/50: WIN | Score: 1157.0 | Steps: 93 | Ghosts eaten: 0\n",
            "Test 35/50: WIN | Score: 1142.0 | Steps: 108 | Ghosts eaten: 0\n",
            "Test 36/50: WIN | Score: 943.0 | Steps: 107 | Ghosts eaten: 0\n",
            "Test 37/50: WIN | Score: 1345.0 | Steps: 105 | Ghosts eaten: 0\n",
            "Test 38/50: WIN | Score: 957.0 | Steps: 93 | Ghosts eaten: 0\n",
            "Test 39/50: WIN | Score: 929.0 | Steps: 121 | Ghosts eaten: 0\n",
            "Test 40/50: WIN | Score: 1071.0 | Steps: 179 | Ghosts eaten: 0\n",
            "Test 41/50: WIN | Score: 1052.0 | Steps: 198 | Ghosts eaten: 0\n",
            "Test 42/50: WIN | Score: 1112.0 | Steps: 138 | Ghosts eaten: 0\n",
            "Test 43/50: WIN | Score: 1158.0 | Steps: 92 | Ghosts eaten: 0\n",
            "Test 44/50: WIN | Score: 1304.0 | Steps: 146 | Ghosts eaten: 0\n",
            "Test 45/50: WIN | Score: 1096.0 | Steps: 154 | Ghosts eaten: 0\n",
            "Test 46/50: WIN | Score: 1349.0 | Steps: 101 | Ghosts eaten: 0\n",
            "Test 47/50: LOSE | Score: 62.0 | Steps: 178 | Ghosts eaten: 0\n",
            "Test 48/50: LOSE | Score: 103.0 | Steps: 117 | Ghosts eaten: 0\n",
            "Test 49/50: WIN | Score: 964.0 | Steps: 86 | Ghosts eaten: 0\n",
            "Test 50/50: WIN | Score: 1096.0 | Steps: 154 | Ghosts eaten: 0\n",
            "\n",
            "============================================================\n",
            "TEST RESULTS\n",
            "  Win rate: 90.00% (45/50)\n",
            "  Avg score: 1016.4\n",
            "  Avg steps: 131.4\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9, 1016.42, 131.38)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FASE 5:** USO DEI POWER UP"
      ],
      "metadata": {
        "id": "DNCT-Pvdvvss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAINING E TEST: 1 directional ghost, small_classic**"
      ],
      "metadata": {
        "id": "honIYlYFxqtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent3 = DQNAgentScalable(\n",
        "    state_size=33,\n",
        "    action_size=4,\n",
        "    alpha=2e-5,\n",
        "    gamma=0.99,\n",
        "    epsilon=1.0,\n",
        "    epsilon_end=0.7,\n",
        "    buffer_size=20000,\n",
        "    batch_size=64,\n",
        "    tau=0.003,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"agente ricaricato con successo!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AClaRra6v0Ec",
        "outputId": "83684f92-7e63-467b-9580-fa1d37787a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "agente ricaricato con successo!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"\\nFASE 3A: Training con 1 DirectionalGhost, con POWER UP\")\n",
        "\n",
        "print(\"\\n FASE 1 ESPLORATIVA\")\n",
        "results_S = train_with_progress2(\n",
        "    agent3,\n",
        "    num_episodes=250,\n",
        "    layout_name=\"small_classic\",\n",
        "    n_ghosts=1,\n",
        "    ghost_cls=DirectionalGhost,\n",
        "    epsilon_start=1.0,      # esplorazione totale\n",
        "    epsilon_end=0.7,        # ancora esplorativo\n",
        "    decay_episodes=200,\n",
        "    use_powerup_rewards=True,\n",
        "    print_every=50,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"\\n FASE 2 ESPLORATIVA\")\n",
        "# settiamo di nuovo i parametri che ci interessano\n",
        "agent3.epsilon = 0.8\n",
        "agent3.tau = 0.001\n",
        "agent3.gamma = 0.99\n",
        "agent3.epsilon_end = 0.2\n",
        "\n",
        "results_M = train_with_progress2(\n",
        "    agent3,\n",
        "    num_episodes=400,\n",
        "    layout_name=\"small_classic\",\n",
        "    n_ghosts=1,\n",
        "    ghost_cls=DirectionalGhost,\n",
        "    epsilon_start=0.8,\n",
        "    epsilon_end=0.2,\n",
        "    decay_episodes=320,\n",
        "    use_powerup_rewards=True,\n",
        "    print_every=50,\n",
        "    seed=42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJkZa99FyDc_",
        "outputId": "aaa64e00-cb70-4d31-8ee2-7325ce833167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FASE 3A: Training con 1 DirectionalGhost, con POWER UP\n",
            "\n",
            " FASE 1 ESPLORATIVA\n",
            "\n",
            "=== EPISODE 10 (DQN) ===\n",
            "Episode 10 ended | Buffer: 1506 | Epsilon: 0.986\n",
            "\n",
            "=== EPISODE 20 (DQN) ===\n",
            "Episode 20 ended | Buffer: 3000 | Epsilon: 0.971\n",
            "\n",
            "=== EPISODE 30 (DQN) ===\n",
            "Episode 30 ended | Buffer: 4788 | Epsilon: 0.956\n",
            "\n",
            "=== EPISODE 40 (DQN) ===\n",
            "Episode 40 ended | Buffer: 6155 | Epsilon: 0.941\n",
            "\n",
            "=== EPISODE 50 (DQN) ===\n",
            "Episode 50 ended | Buffer: 7103 | Epsilon: 0.926\n",
            "\n",
            "============================================================\n",
            "Episode 50/250\n",
            "  Win rate (ultimi 50): 16.00%\n",
            "  Win rate (globale): 16.00%\n",
            "  Avg score: -36.1 | Avg steps: 142.1\n",
            "  Epsilon: 0.926\n",
            "  avg loss: 0.9246874403953552\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 60 (DQN) ===\n",
            "Episode 60 ended | Buffer: 8716 | Epsilon: 0.911\n",
            "\n",
            "=== EPISODE 70 (DQN) ===\n",
            "Episode 70 ended | Buffer: 10174 | Epsilon: 0.896\n",
            "\n",
            "=== EPISODE 80 (DQN) ===\n",
            "Episode 80 ended | Buffer: 12139 | Epsilon: 0.881\n",
            "\n",
            "=== EPISODE 90 (DQN) ===\n",
            "Episode 90 ended | Buffer: 13866 | Epsilon: 0.866\n",
            "\n",
            "=== EPISODE 100 (DQN) ===\n",
            "Episode 100 ended | Buffer: 15603 | Epsilon: 0.851\n",
            "\n",
            "============================================================\n",
            "Episode 100/250\n",
            "  Win rate (ultimi 50): 22.00%\n",
            "  Win rate (globale): 19.00%\n",
            "  Avg score: 48.8 | Avg steps: 170.0\n",
            "  Epsilon: 0.851\n",
            "  avg loss: 0.7914252144098282\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 110 (DQN) ===\n",
            "Episode 110 ended | Buffer: 17231 | Epsilon: 0.836\n",
            "\n",
            "=== EPISODE 120 (DQN) ===\n",
            "Episode 120 ended | Buffer: 18376 | Epsilon: 0.821\n",
            "\n",
            "=== EPISODE 130 (DQN) ===\n",
            "Episode 130 ended | Buffer: 20000 | Epsilon: 0.806\n",
            "\n",
            "=== EPISODE 140 (DQN) ===\n",
            "Episode 140 ended | Buffer: 20000 | Epsilon: 0.790\n",
            "\n",
            "=== EPISODE 150 (DQN) ===\n",
            "Episode 150 ended | Buffer: 20000 | Epsilon: 0.775\n",
            "\n",
            "============================================================\n",
            "Episode 150/250\n",
            "  Win rate (ultimi 50): 22.00%\n",
            "  Win rate (globale): 20.00%\n",
            "  Avg score: 87.7 | Avg steps: 154.3\n",
            "  Epsilon: 0.775\n",
            "  avg loss: 0.9361753433942794\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 160 (DQN) ===\n",
            "Episode 160 ended | Buffer: 20000 | Epsilon: 0.760\n",
            "\n",
            "=== EPISODE 170 (DQN) ===\n",
            "Episode 170 ended | Buffer: 20000 | Epsilon: 0.745\n",
            "\n",
            "=== EPISODE 180 (DQN) ===\n",
            "Episode 180 ended | Buffer: 20000 | Epsilon: 0.730\n",
            "\n",
            "=== EPISODE 190 (DQN) ===\n",
            "Episode 190 ended | Buffer: 20000 | Epsilon: 0.715\n",
            "\n",
            "=== EPISODE 200 (DQN) ===\n",
            "Episode 200 ended | Buffer: 20000 | Epsilon: 0.700\n",
            "\n",
            "============================================================\n",
            "Episode 200/250\n",
            "  Win rate (ultimi 50): 40.00%\n",
            "  Win rate (globale): 25.00%\n",
            "  Avg score: 289.1 | Avg steps: 151.7\n",
            "  Epsilon: 0.700\n",
            "  avg loss: 1.0641808271408082\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 210 (DQN) ===\n",
            "Episode 210 ended | Buffer: 20000 | Epsilon: 0.700\n",
            "\n",
            "=== EPISODE 220 (DQN) ===\n",
            "Episode 220 ended | Buffer: 20000 | Epsilon: 0.700\n",
            "\n",
            "=== EPISODE 230 (DQN) ===\n",
            "Episode 230 ended | Buffer: 20000 | Epsilon: 0.700\n",
            "\n",
            "=== EPISODE 240 (DQN) ===\n",
            "Episode 240 ended | Buffer: 20000 | Epsilon: 0.700\n",
            "\n",
            "=== EPISODE 250 (DQN) ===\n",
            "Episode 250 ended | Buffer: 20000 | Epsilon: 0.700\n",
            "\n",
            "============================================================\n",
            "Episode 250/250\n",
            "  Win rate (ultimi 50): 38.00%\n",
            "  Win rate (globale): 27.60%\n",
            "  Avg score: 285.1 | Avg steps: 151.3\n",
            "  Epsilon: 0.700\n",
            "  avg loss: 1.2399679428339005\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETATO\n",
            "============================================================\n",
            "Episodi totali: 250\n",
            "Vittorie: 69 (27.60%)\n",
            "Sconfitte: 181 (72.40%)\n",
            "timeouts: 0 \n",
            "Buffer finale: 20000\n",
            "Epsilon finale: 0.700\n",
            "============================================================\n",
            "\n",
            "\n",
            " FASE 2 ESPLORATIVA\n",
            "\n",
            "=== EPISODE 260 (DQN) ===\n",
            "Episode 260 ended | Buffer: 20000 | Epsilon: 0.783\n",
            "\n",
            "=== EPISODE 270 (DQN) ===\n",
            "Episode 270 ended | Buffer: 20000 | Epsilon: 0.764\n",
            "\n",
            "=== EPISODE 280 (DQN) ===\n",
            "Episode 280 ended | Buffer: 20000 | Epsilon: 0.745\n",
            "\n",
            "=== EPISODE 290 (DQN) ===\n",
            "Episode 290 ended | Buffer: 20000 | Epsilon: 0.727\n",
            "\n",
            "=== EPISODE 300 (DQN) ===\n",
            "Episode 300 ended | Buffer: 20000 | Epsilon: 0.708\n",
            "\n",
            "============================================================\n",
            "Episode 50/400\n",
            "  Win rate (ultimi 50): 36.00%\n",
            "  Win rate (globale): 36.00%\n",
            "  Avg score: 271.0 | Avg steps: 143.4\n",
            "  Epsilon: 0.708\n",
            "  avg loss: 1.2153664636611938\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 310 (DQN) ===\n",
            "Episode 310 ended | Buffer: 20000 | Epsilon: 0.689\n",
            "\n",
            "=== EPISODE 320 (DQN) ===\n",
            "Episode 320 ended | Buffer: 20000 | Epsilon: 0.670\n",
            "\n",
            "=== EPISODE 330 (DQN) ===\n",
            "Episode 330 ended | Buffer: 20000 | Epsilon: 0.651\n",
            "\n",
            "=== EPISODE 340 (DQN) ===\n",
            "Episode 340 ended | Buffer: 20000 | Epsilon: 0.633\n",
            "\n",
            "=== EPISODE 350 (DQN) ===\n",
            "Episode 350 ended | Buffer: 20000 | Epsilon: 0.614\n",
            "\n",
            "============================================================\n",
            "Episode 100/400\n",
            "  Win rate (ultimi 50): 44.00%\n",
            "  Win rate (globale): 40.00%\n",
            "  Avg score: 346.8 | Avg steps: 136.6\n",
            "  Epsilon: 0.614\n",
            "  avg loss: 1.5790043032169343\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 360 (DQN) ===\n",
            "Episode 360 ended | Buffer: 20000 | Epsilon: 0.595\n",
            "\n",
            "=== EPISODE 370 (DQN) ===\n",
            "Episode 370 ended | Buffer: 20000 | Epsilon: 0.576\n",
            "\n",
            "=== EPISODE 380 (DQN) ===\n",
            "Episode 380 ended | Buffer: 20000 | Epsilon: 0.557\n",
            "\n",
            "=== EPISODE 390 (DQN) ===\n",
            "Episode 390 ended | Buffer: 20000 | Epsilon: 0.539\n",
            "\n",
            "=== EPISODE 400 (DQN) ===\n",
            "Episode 400 ended | Buffer: 20000 | Epsilon: 0.520\n",
            "\n",
            "============================================================\n",
            "Episode 150/400\n",
            "  Win rate (ultimi 50): 52.00%\n",
            "  Win rate (globale): 44.00%\n",
            "  Avg score: 464.1 | Avg steps: 128.5\n",
            "  Epsilon: 0.520\n",
            "  avg loss: 1.9090068101882935\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 410 (DQN) ===\n",
            "Episode 410 ended | Buffer: 20000 | Epsilon: 0.501\n",
            "\n",
            "=== EPISODE 420 (DQN) ===\n",
            "Episode 420 ended | Buffer: 20000 | Epsilon: 0.482\n",
            "\n",
            "=== EPISODE 430 (DQN) ===\n",
            "Episode 430 ended | Buffer: 20000 | Epsilon: 0.463\n",
            "\n",
            "=== EPISODE 440 (DQN) ===\n",
            "Episode 440 ended | Buffer: 20000 | Epsilon: 0.445\n",
            "\n",
            "=== EPISODE 450 (DQN) ===\n",
            "Episode 450 ended | Buffer: 20000 | Epsilon: 0.426\n",
            "\n",
            "============================================================\n",
            "Episode 200/400\n",
            "  Win rate (ultimi 50): 66.00%\n",
            "  Win rate (globale): 49.50%\n",
            "  Avg score: 709.6 | Avg steps: 127.8\n",
            "  Epsilon: 0.426\n",
            "  avg loss: 2.205847890377045\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 460 (DQN) ===\n",
            "Episode 460 ended | Buffer: 20000 | Epsilon: 0.407\n",
            "\n",
            "=== EPISODE 470 (DQN) ===\n",
            "Episode 470 ended | Buffer: 20000 | Epsilon: 0.388\n",
            "\n",
            "=== EPISODE 480 (DQN) ===\n",
            "Episode 480 ended | Buffer: 20000 | Epsilon: 0.369\n",
            "\n",
            "=== EPISODE 490 (DQN) ===\n",
            "Episode 490 ended | Buffer: 20000 | Epsilon: 0.350\n",
            "\n",
            "=== EPISODE 500 (DQN) ===\n",
            "Episode 500 ended | Buffer: 20000 | Epsilon: 0.332\n",
            "\n",
            "============================================================\n",
            "Episode 250/400\n",
            "  Win rate (ultimi 50): 78.00%\n",
            "  Win rate (globale): 55.20%\n",
            "  Avg score: 768.4 | Avg steps: 119.6\n",
            "  Epsilon: 0.332\n",
            "  avg loss: 2.0237287199497223\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 510 (DQN) ===\n",
            "Episode 510 ended | Buffer: 20000 | Epsilon: 0.313\n",
            "\n",
            "=== EPISODE 520 (DQN) ===\n",
            "Episode 520 ended | Buffer: 20000 | Epsilon: 0.294\n",
            "\n",
            "=== EPISODE 530 (DQN) ===\n",
            "Episode 530 ended | Buffer: 20000 | Epsilon: 0.275\n",
            "\n",
            "=== EPISODE 540 (DQN) ===\n",
            "Episode 540 ended | Buffer: 20000 | Epsilon: 0.256\n",
            "\n",
            "=== EPISODE 550 (DQN) ===\n",
            "Episode 550 ended | Buffer: 20000 | Epsilon: 0.238\n",
            "\n",
            "============================================================\n",
            "Episode 300/400\n",
            "  Win rate (ultimi 50): 80.00%\n",
            "  Win rate (globale): 59.33%\n",
            "  Avg score: 798.7 | Avg steps: 113.9\n",
            "  Epsilon: 0.238\n",
            "  avg loss: 1.9487192726135254\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 560 (DQN) ===\n",
            "Episode 560 ended | Buffer: 20000 | Epsilon: 0.219\n",
            "\n",
            "=== EPISODE 570 (DQN) ===\n",
            "Episode 570 ended | Buffer: 20000 | Epsilon: 0.200\n",
            "\n",
            "=== EPISODE 580 (DQN) ===\n",
            "Episode 580 ended | Buffer: 20000 | Epsilon: 0.200\n",
            "\n",
            "=== EPISODE 590 (DQN) ===\n",
            "Episode 590 ended | Buffer: 20000 | Epsilon: 0.200\n",
            "\n",
            "=== EPISODE 600 (DQN) ===\n",
            "Episode 600 ended | Buffer: 20000 | Epsilon: 0.200\n",
            "\n",
            "============================================================\n",
            "Episode 350/400\n",
            "  Win rate (ultimi 50): 84.00%\n",
            "  Win rate (globale): 62.86%\n",
            "  Avg score: 868.6 | Avg steps: 116.6\n",
            "  Epsilon: 0.200\n",
            "  avg loss: 2.482207775115967\n",
            "============================================================\n",
            "\n",
            "=== EPISODE 610 (DQN) ===\n",
            "Episode 610 ended | Buffer: 20000 | Epsilon: 0.200\n",
            "\n",
            "=== EPISODE 620 (DQN) ===\n",
            "Episode 620 ended | Buffer: 20000 | Epsilon: 0.200\n",
            "\n",
            "=== EPISODE 630 (DQN) ===\n",
            "Episode 630 ended | Buffer: 20000 | Epsilon: 0.200\n",
            "\n",
            "=== EPISODE 640 (DQN) ===\n",
            "Episode 640 ended | Buffer: 20000 | Epsilon: 0.200\n",
            "\n",
            "=== EPISODE 650 (DQN) ===\n",
            "Episode 650 ended | Buffer: 20000 | Epsilon: 0.200\n",
            "\n",
            "============================================================\n",
            "Episode 400/400\n",
            "  Win rate (ultimi 50): 86.00%\n",
            "  Win rate (globale): 65.75%\n",
            "  Avg score: 888.9 | Avg steps: 113.3\n",
            "  Epsilon: 0.200\n",
            "  avg loss: 2.8391734671592714\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETATO\n",
            "============================================================\n",
            "Episodi totali: 400\n",
            "Vittorie: 263 (65.75%)\n",
            "Sconfitte: 137 (34.25%)\n",
            "timeouts: 0 \n",
            "Buffer finale: 20000\n",
            "Epsilon finale: 0.200\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dqn_agent(agent3, num_episodes=50, layout_name=\"small_classic\", n_ghosts=1, ghost_cls=DirectionalGhost, track_powerups=True )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxnLHLdByX7m",
        "outputId": "950ef164-681e-4877-c16a-f575a2984194",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TESTING DQN AGENT (epsilon=0, greedy policy)\n",
            "============================================================\n",
            "\n",
            "Test 1/50: WIN | Score: 1171.0 | Steps: 79 | Ghosts eaten: 1\n",
            "Test 2/50: LOSE | Score: 27.0 | Steps: 213 | Ghosts eaten: 1\n",
            "Test 3/50: WIN | Score: 1116.0 | Steps: 134 | Ghosts eaten: 1\n",
            "Test 4/50: WIN | Score: 1115.0 | Steps: 135 | Ghosts eaten: 1\n",
            "Test 5/50: WIN | Score: 1171.0 | Steps: 79 | Ghosts eaten: 1\n",
            "Test 6/50: WIN | Score: 1375.0 | Steps: 75 | Ghosts eaten: 2\n",
            "Test 7/50: WIN | Score: 1136.0 | Steps: 114 | Ghosts eaten: 1\n",
            "Test 8/50: WIN | Score: 1155.0 | Steps: 95 | Ghosts eaten: 1\n",
            "Test 9/50: WIN | Score: 1155.0 | Steps: 95 | Ghosts eaten: 1\n",
            "Test 10/50: WIN | Score: 1165.0 | Steps: 85 | Ghosts eaten: 1\n",
            "Test 11/50: WIN | Score: 967.0 | Steps: 83 | Ghosts eaten: 0\n",
            "Test 12/50: WIN | Score: 1149.0 | Steps: 101 | Ghosts eaten: 1\n",
            "Test 13/50: WIN | Score: 1339.0 | Steps: 111 | Ghosts eaten: 2\n",
            "Test 14/50: WIN | Score: 1149.0 | Steps: 101 | Ghosts eaten: 3\n",
            "Test 15/50: WIN | Score: 914.0 | Steps: 136 | Ghosts eaten: 0\n",
            "Test 16/50: WIN | Score: 963.0 | Steps: 87 | Ghosts eaten: 0\n",
            "Test 17/50: WIN | Score: 1090.0 | Steps: 160 | Ghosts eaten: 1\n",
            "Test 18/50: WIN | Score: 971.0 | Steps: 79 | Ghosts eaten: 0\n",
            "Test 19/50: WIN | Score: 1310.0 | Steps: 140 | Ghosts eaten: 2\n",
            "Test 20/50: WIN | Score: 953.0 | Steps: 97 | Ghosts eaten: 0\n",
            "Test 21/50: WIN | Score: 945.0 | Steps: 105 | Ghosts eaten: 2\n",
            "Test 22/50: WIN | Score: 1160.0 | Steps: 90 | Ghosts eaten: 1\n",
            "Test 23/50: WIN | Score: 975.0 | Steps: 75 | Ghosts eaten: 0\n",
            "Test 24/50: WIN | Score: 1129.0 | Steps: 121 | Ghosts eaten: 1\n",
            "Test 25/50: LOSE | Score: -199.0 | Steps: 59 | Ghosts eaten: 0\n",
            "Test 26/50: WIN | Score: 1145.0 | Steps: 105 | Ghosts eaten: 1\n",
            "Test 27/50: WIN | Score: 1179.0 | Steps: 71 | Ghosts eaten: 1\n",
            "Test 28/50: WIN | Score: 1344.0 | Steps: 106 | Ghosts eaten: 2\n",
            "Test 29/50: WIN | Score: 1141.0 | Steps: 109 | Ghosts eaten: 1\n",
            "Test 30/50: WIN | Score: 1153.0 | Steps: 97 | Ghosts eaten: 1\n",
            "Test 31/50: WIN | Score: 900.0 | Steps: 150 | Ghosts eaten: 2\n",
            "Test 32/50: WIN | Score: 949.0 | Steps: 101 | Ghosts eaten: 2\n",
            "Test 33/50: WIN | Score: 1170.0 | Steps: 80 | Ghosts eaten: 1\n",
            "Test 34/50: WIN | Score: 1137.0 | Steps: 113 | Ghosts eaten: 1\n",
            "Test 35/50: WIN | Score: 932.0 | Steps: 118 | Ghosts eaten: 0\n",
            "Test 36/50: WIN | Score: 1324.0 | Steps: 126 | Ghosts eaten: 2\n",
            "Test 37/50: WIN | Score: 1177.0 | Steps: 73 | Ghosts eaten: 1\n",
            "Test 38/50: WIN | Score: 1358.0 | Steps: 92 | Ghosts eaten: 2\n",
            "Test 39/50: WIN | Score: 1143.0 | Steps: 107 | Ghosts eaten: 1\n",
            "Test 40/50: WIN | Score: 952.0 | Steps: 98 | Ghosts eaten: 2\n",
            "Test 41/50: WIN | Score: 969.0 | Steps: 81 | Ghosts eaten: 0\n",
            "Test 42/50: LOSE | Score: -205.0 | Steps: 65 | Ghosts eaten: 0\n",
            "Test 43/50: WIN | Score: 1333.0 | Steps: 117 | Ghosts eaten: 2\n",
            "Test 44/50: WIN | Score: 1148.0 | Steps: 102 | Ghosts eaten: 1\n",
            "Test 45/50: WIN | Score: 985.0 | Steps: 65 | Ghosts eaten: 0\n",
            "Test 46/50: WIN | Score: 1107.0 | Steps: 143 | Ghosts eaten: 1\n",
            "Test 47/50: WIN | Score: 981.0 | Steps: 69 | Ghosts eaten: 0\n",
            "Test 48/50: WIN | Score: 1160.0 | Steps: 90 | Ghosts eaten: 1\n",
            "Test 49/50: WIN | Score: 1132.0 | Steps: 118 | Ghosts eaten: 1\n",
            "Test 50/50: WIN | Score: 903.0 | Steps: 147 | Ghosts eaten: 2\n",
            "\n",
            "============================================================\n",
            "TEST RESULTS\n",
            "  Win rate: 94.00% (47/50)\n",
            "  Avg score: 1038.4\n",
            "  Avg steps: 103.8\n",
            "  Avg capsules eaten: 1.08\n",
            "  Avg ghosts eaten (scared): 1.04\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.94, 1038.36, 103.84, 1.08, 1.04)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}